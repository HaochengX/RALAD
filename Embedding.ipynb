{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.document_loaders.parsers import LanguageParser\n",
    "from langchain_community.document_loaders.generic import GenericLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import Language\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = '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'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from git import Repo\n",
    "from langchain_community.document_loaders.generic import GenericLoader\n",
    "from langchain_community.document_loaders.parsers import LanguageParser\n",
    "from langchain_text_splitters import Language\n",
    "repo_path = \"/home/ubuntu/Home_User/HHT_Documents/Splitted_code\"\n",
    "\n",
    "loader = GenericLoader.from_filesystem(\n",
    "    repo_path,\n",
    "    glob=\"**/*\",\n",
    "    suffixes=[\".cpp\"],\n",
    "    # exclude=[\"**/non-utf8-encoding.py\"],\n",
    "    parser=LanguageParser(language=Language.CPP, parser_threshold=500),\n",
    ")\n",
    "documents = loader.load()\n",
    "len(documents)\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON, chunk_size=1500, chunk_overlap=200\n",
    ")\n",
    "texts = python_splitter.split_documents(documents)\n",
    "len(texts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m db1 \u001b[38;5;241m=\u001b[39m \u001b[43mFAISS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Home_User/HHT_Documents/fermihedral-main/venv/lib/python3.11/site-packages/langchain_core/vectorstores.py:635\u001b[0m, in \u001b[0;36mVectorStore.from_documents\u001b[0;34m(cls, documents, embedding, **kwargs)\u001b[0m\n\u001b[1;32m    633\u001b[0m texts \u001b[38;5;241m=\u001b[39m [d\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m    634\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [d\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m--> 635\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Home_User/HHT_Documents/fermihedral-main/venv/lib/python3.11/site-packages/langchain_community/vectorstores/faiss.py:930\u001b[0m, in \u001b[0;36mFAISS.from_texts\u001b[0;34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    904\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_texts\u001b[39m(\n\u001b[1;32m    905\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    911\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m FAISS:\n\u001b[1;32m    912\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Construct FAISS wrapper from raw documents.\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \n\u001b[1;32m    914\u001b[0m \u001b[38;5;124;03m    This is a user friendly interface that:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;124;03m            faiss = FAISS.from_texts(texts, embeddings)\u001b[39;00m\n\u001b[1;32m    929\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 930\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[43membedding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    931\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__from(\n\u001b[1;32m    932\u001b[0m         texts,\n\u001b[1;32m    933\u001b[0m         embeddings,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    937\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    938\u001b[0m     )\n",
      "File \u001b[0;32m~/Home_User/HHT_Documents/fermihedral-main/venv/lib/python3.11/site-packages/langchain_openai/embeddings/base.py:535\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.embed_documents\u001b[0;34m(self, texts, chunk_size)\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;66;03m# NOTE: to keep things simple, we assume the list may contain texts longer\u001b[39;00m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;66;03m#       than the maximum context and use length-safe embedding function.\u001b[39;00m\n\u001b[1;32m    534\u001b[0m engine \u001b[38;5;241m=\u001b[39m cast(\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeployment)\n\u001b[0;32m--> 535\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_len_safe_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Home_User/HHT_Documents/fermihedral-main/venv/lib/python3.11/site-packages/langchain_openai/embeddings/base.py:430\u001b[0m, in \u001b[0;36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[0;34m(self, texts, engine, chunk_size)\u001b[0m\n\u001b[1;32m    428\u001b[0m batched_embeddings: List[List[\u001b[38;5;28mfloat\u001b[39m]] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m _iter:\n\u001b[0;32m--> 430\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m_chunk_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_invocation_params\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    434\u001b[0m         response \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mmodel_dump()\n",
      "File \u001b[0;32m~/Home_User/HHT_Documents/fermihedral-main/venv/lib/python3.11/site-packages/openai/resources/embeddings.py:114\u001b[0m, in \u001b[0;36mEmbeddings.create\u001b[0;34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    108\u001b[0m         embedding\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfrombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[1;32m    109\u001b[0m             base64\u001b[38;5;241m.\u001b[39mb64decode(data), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    110\u001b[0m         )\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/embeddings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEmbeddingCreateParams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCreateEmbeddingResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Home_User/HHT_Documents/fermihedral-main/venv/lib/python3.11/site-packages/openai/_base_client.py:1240\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1226\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1227\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1228\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1235\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1236\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1237\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1238\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1239\u001b[0m     )\n\u001b[0;32m-> 1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Home_User/HHT_Documents/fermihedral-main/venv/lib/python3.11/site-packages/openai/_base_client.py:921\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    913\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    914\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    919\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    920\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Home_User/HHT_Documents/fermihedral-main/venv/lib/python3.11/site-packages/openai/_base_client.py:952\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    949\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSending HTTP Request: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, request\u001b[38;5;241m.\u001b[39mmethod, request\u001b[38;5;241m.\u001b[39murl)\n\u001b[1;32m    951\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 952\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    958\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered httpx.TimeoutException\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Home_User/HHT_Documents/fermihedral-main/venv/lib/python3.11/site-packages/httpx/_client.py:914\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    906\u001b[0m follow_redirects \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    907\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfollow_redirects\n\u001b[1;32m    908\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(follow_redirects, UseClientDefault)\n\u001b[1;32m    909\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m follow_redirects\n\u001b[1;32m    910\u001b[0m )\n\u001b[1;32m    912\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 914\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m~/Home_User/HHT_Documents/fermihedral-main/venv/lib/python3.11/site-packages/httpx/_client.py:942\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    939\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 942\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    947\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    948\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Home_User/HHT_Documents/fermihedral-main/venv/lib/python3.11/site-packages/httpx/_client.py:979\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    977\u001b[0m     hook(request)\n\u001b[0;32m--> 979\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    981\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/Home_User/HHT_Documents/fermihedral-main/venv/lib/python3.11/site-packages/httpx/_client.py:1015\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1010\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1011\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1012\u001b[0m     )\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1015\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1019\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[0;32m~/Home_User/HHT_Documents/fermihedral-main/venv/lib/python3.11/site-packages/httpx/_transports/default.py:233\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    220\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    221\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    222\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    230\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    231\u001b[0m )\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 233\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    238\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    239\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    240\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[1;32m    241\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    242\u001b[0m )\n",
      "File \u001b[0;32m~/Home_User/HHT_Documents/fermihedral-main/venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:216\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    213\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, Iterable)\n",
      "File \u001b[0;32m~/Home_User/HHT_Documents/fermihedral-main/venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:196\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    192\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[0;32m~/Home_User/HHT_Documents/fermihedral-main/venv/lib/python3.11/site-packages/httpcore/_sync/connection.py:99\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 99\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mhandle_request(request)\n",
      "File \u001b[0;32m~/Home_User/HHT_Documents/fermihedral-main/venv/lib/python3.11/site-packages/httpcore/_sync/connection.py:76\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request_lock:\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m         stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m         ssl_object \u001b[38;5;241m=\u001b[39m stream\u001b[38;5;241m.\u001b[39mget_extra_info(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mssl_object\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     79\u001b[0m         http2_negotiated \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     80\u001b[0m             ssl_object \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     81\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m ssl_object\u001b[38;5;241m.\u001b[39mselected_alpn_protocol() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     82\u001b[0m         )\n",
      "File \u001b[0;32m~/Home_User/HHT_Documents/fermihedral-main/venv/lib/python3.11/site-packages/httpcore/_sync/connection.py:122\u001b[0m, in \u001b[0;36mHTTPConnection._connect\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    114\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhost\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_origin\u001b[38;5;241m.\u001b[39mhost\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mascii\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    116\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mport\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_origin\u001b[38;5;241m.\u001b[39mport,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msocket_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_socket_options,\n\u001b[1;32m    120\u001b[0m     }\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconnect_tcp\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[0;32m--> 122\u001b[0m         stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_tcp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m         trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m stream\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Home_User/HHT_Documents/fermihedral-main/venv/lib/python3.11/site-packages/httpcore/_backends/sync.py:206\u001b[0m, in \u001b[0;36mSyncBackend.connect_tcp\u001b[0;34m(self, host, port, timeout, local_address, socket_options)\u001b[0m\n\u001b[1;32m    200\u001b[0m exc_map: ExceptionMapping \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    201\u001b[0m     socket\u001b[38;5;241m.\u001b[39mtimeout: ConnectTimeout,\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;167;01mOSError\u001b[39;00m: ConnectError,\n\u001b[1;32m    203\u001b[0m }\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[0;32m--> 206\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m option \u001b[38;5;129;01min\u001b[39;00m socket_options:\n\u001b[1;32m    212\u001b[0m         sock\u001b[38;5;241m.\u001b[39msetsockopt(\u001b[38;5;241m*\u001b[39moption)  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/socket.py:836\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, all_errors)\u001b[0m\n\u001b[1;32m    834\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source_address:\n\u001b[1;32m    835\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[0;32m--> 836\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n\u001b[1;32m    838\u001b[0m exceptions\u001b[38;5;241m.\u001b[39mclear()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "db1 = FAISS.from_documents(texts[:200], embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Parallel Programming for FPGAs\\nRyan Kastner, Janarbek Matai, and Stephen Neuendorffer\\n2018-05-11\\n8102\\nyaM\\n9\\n]RA.sc[\\n1v84630.5081:viXraCopyright 2011-2018.\\nThis work is licensed under the Creative Commons Attribution 4.0 International License.\\nTo view a copy of this license, visit http://creativecommons.org/licenses/by/3.0/.\\nThe newest version of this book can be found at http://hlsbook.ucsd.edu. The authors welcome\\nyour feedback and suggestions.\\n2Contents\\nPreface 7\\nAcknowledgements 9\\n1 Introduction 11\\n1.1 High-level Synthesis (HLS) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\\n1.2 FPGA Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\\n1.3 FPGA Design Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\\n1.4 Design Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\\n1.4.1 Performance Characterization . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\\n1.4.2'),\n",
       " Document(page_content='19\\n1.4 Design Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\\n1.4.1 Performance Characterization . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\\n1.4.2 Area/Throughput Tradeoffs . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\\n1.4.3 Restrictions on Processing Rate . . . . . . . . . . . . . . . . . . . . . . . . . . 26\\n1.4.4 Coding Style . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\\n1.5 Restructured Code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\\n1.6 Book Organization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\\n2 Finite Impulse Response (FIR) Filters 31\\n2.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\\n2.2 Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\\n2.3 Base FIR Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .'),\n",
       " Document(page_content='. . . . . 31\\n2.2 Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\\n2.3 Base FIR Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\\n2.4 Calculating Performance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\\n2.5 Operation Chaining . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\\n2.6 Code Hoisting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\\n2.7 Loop Fission . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\\n2.8 Loop Unrolling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\\n2.9 Loop Pipelining . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43\\n2.10 Bitwidth Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\\n2.11 Complex FIR Filter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .'),\n",
       " Document(page_content='. . . . 43\\n2.10 Bitwidth Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\\n2.11 Complex FIR Filter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49\\n2.12 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52\\n3 CORDIC 55\\n3.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55\\n3.2 Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56\\n3.3 Calculating Sine and Cosine . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61\\n3.4 Cartesian to Polar Conversion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63\\n3.5 Number Representation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66\\n3.5.1 Binary and Hexadecimal Numbers . . . . . . . . . . . . . . . . . . . . . . . . 66\\n3.5.2 Negative numbers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .'),\n",
       " Document(page_content='. . . . . . . . . . 66\\n3.5.1 Binary and Hexadecimal Numbers . . . . . . . . . . . . . . . . . . . . . . . . 66\\n3.5.2 Negative numbers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69\\n3CONTENTS CONTENTS\\n3.5.3 Overflow, Underflow, and Rounding . . . . . . . . . . . . . . . . . . . . . . . 70\\n3.5.4 Binary arithmetic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72\\n3.5.5 Representing Arbitrary Precision Integers in C and C++ . . . . . . . . . . . 73\\n3.5.6 Floating Point . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75\\n3.6 Further Optimizations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75\\n3.7 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76\\n4 Discrete Fourier Transform 77\\n4.1 Fourier Series . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77\\n4.2 Discrete Fourier Transform (DFT) Background . . . . . . . . . . . . . . . . . .'),\n",
       " Document(page_content='Transform 77\\n4.1 Fourier Series . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77\\n4.2 Discrete Fourier Transform (DFT) Background . . . . . . . . . . . . . . . . . . . . . 79\\n4.3 Matrix-Vector Multiplication Optimizations . . . . . . . . . . . . . . . . . . . . . . . 82\\n4.4 Pipelining and Parallelism . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83\\n4.5 Storage Tradeoffs and Array Partitioning . . . . . . . . . . . . . . . . . . . . . . . . 87\\n4.6 Baseline Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92\\n4.7 DFT optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94\\n4.8 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97\\n5 Fast Fourier Transform 99\\n5.1 Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99\\n5.2 Baseline Implementation . . . . . . . . . . . . . . . . . . . . . . . .'),\n",
       " Document(page_content='Fourier Transform 99\\n5.1 Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99\\n5.2 Baseline Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108\\n5.3 Bit Reversal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111\\n5.4 Task Pipelining . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113\\n5.5 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116\\n6 Sparse Matrix Vector Multiplication 117\\n6.1 Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117\\n6.2 Baseline Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119\\n6.3 Testbench . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120\\n6.4 Specifying Loop Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122\\n6.5 C/RTL Cosimulation . . .'),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120\\n6.4 Specifying Loop Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122\\n6.5 C/RTL Cosimulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123\\n6.6 Loop Optimizations and Array Partitioning . . . . . . . . . . . . . . . . . . . . . . . 124\\n6.7 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128\\n7 Matrix Multiplication 129\\n7.1 Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129\\n7.2 Complete Matrix Multiplication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130\\n7.3 Block Matrix Multiplication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133\\n7.4 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144\\n8 Prefix Sum and Histogram 145\\n8.1 Prefix Sum . . . . . . . . . . . . . . . . . . . . . . . . . . . . .'),\n",
       " Document(page_content='Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144\\n8 Prefix Sum and Histogram 145\\n8.1 Prefix Sum . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145\\n8.2 Histogram . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148\\n8.3 Histogram Optimization and False Dependencies . . . . . . . . . . . . . . . . . . . . 149\\n8.4 Increasing Histogram Performance . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152\\n8.5 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156\\n4CONTENTS CONTENTS\\n9 Video Systems 157\\n9.1 Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157\\n9.1.1 Representing Video Pixels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 158\\n9.1.2 Digital Video Formats . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 158\\n9.1.3 Video Processing System Architectures . . .'),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . 158\\n9.1.2 Digital Video Formats . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 158\\n9.1.3 Video Processing System Architectures . . . . . . . . . . . . . . . . . . . . . 161\\n9.2 Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164\\n9.2.1 Line Buffers and Frame Buffers . . . . . . . . . . . . . . . . . . . . . . . . . . 164\\n9.2.2 Causal Filters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 166\\n9.2.3 Boundary Conditions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168\\n9.3 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171\\n10 Sorting Algorithms 173\\n10.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173\\n10.2 Insertion Sort . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 174\\n10.2.1 Basic Insertion Sort Implementation'),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . 173\\n10.2 Insertion Sort . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 174\\n10.2.1 Basic Insertion Sort Implementation . . . . . . . . . . . . . . . . . . . . . . . 175\\n10.2.2 Parallelising Insertion Sort . . . . . . . . . . . . . . . . . . . . . . . . . . . . 178\\n10.2.3 Explicit Systolic Array For Insertion Sort . . . . . . . . . . . . . . . . . . . . 180\\n10.3 Merge Sort . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 185\\n10.3.1 Basic Merge Sort . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 186\\n10.3.2 Restructured Merge Sort . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 189\\n10.4 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 192\\n11 Huffman Encoding 195\\n11.1 Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 195\\n11.2 Implementation . . . . . . . . . .'),\n",
       " Document(page_content='. . . . . . . . . . . . . . . 192\\n11 Huffman Encoding 195\\n11.1 Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 195\\n11.2 Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 199\\n11.2.1 Filter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 202\\n11.2.2 Sort . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203\\n11.2.3 Create Tree . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 207\\n11.2.4 Compute Bit Length . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 211\\n11.2.5 Truncate Tree . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 213\\n11.2.6 Canonize Tree . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215\\n11.2.7 Create Codeword . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 217\\n11.2.8 Testbench . . . . . . . . . . . . . . . . . . . . . . . .'),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . 215\\n11.2.7 Create Codeword . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 217\\n11.2.8 Testbench . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 220\\n11.3 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 223\\nBibliography 225\\nGlossary 231\\nAcronyms 235\\n5CONTENTS CONTENTS\\n6Preface\\n“When someone says, ’I want a programming language in which I need only say what I\\nwish done’, give him a lollipop.” -Alan Perlis\\nThis book focuses on the use of algorithmic high-level synthesis (HLS) to build application-specific\\nFPGA systems. Our goal is to give the reader an appreciation of the process of creating an\\noptimized hardware design using HLS. Although the details are, of necessity, different from parallel\\nprogramming for multicore processors or GPUs, many of the fundamental concepts are similar.\\nFor example, designers must understand memory hierarchy and bandwidth, spatial'),\n",
       " Document(page_content='different from parallel\\nprogramming for multicore processors or GPUs, many of the fundamental concepts are similar.\\nFor example, designers must understand memory hierarchy and bandwidth, spatial and temporal\\nlocality of reference, parallelism, and tradeoffs between computation and storage.\\nThis book is a practical guide for anyone interested in building FPGA systems. In a university\\nenvironment,itisappropriateforadvancedundergraduateandgraduatecourses. Atthesametime,\\nit is also useful for practicing system designers and embedded programmers. The book assumes\\nthe reader has a working knowledge of C/C++ and includes a significant amount of sample code.\\nIn addition, we assume familiarity with basic computer architecture concepts (pipelining, speedup,\\nAmdahl’s Law, etc.). A knowledge of the RTL-based FPGA design flow is helpful, although not\\nrequired.\\nThe book includes several features that make it particularly valuable in a classroom environ-\\nment. It includes questions within each'),\n",
       " Document(page_content='the RTL-based FPGA design flow is helpful, although not\\nrequired.\\nThe book includes several features that make it particularly valuable in a classroom environ-\\nment. It includes questions within each chapter that will challenge the reader to solidify their\\nunderstanding of the material. It provides specific projects in the Appendix. These were devel-\\noped and used in the HLS class taught at UCSD (CSE 237C). We will make the files for these\\nprojects available to instructors upon request. These projects teach concepts in HLS using exam-\\nples in the domain of digital signal processing with a focus on developing wireless communication\\nsystems. Each project is more or less associated with one chapter in the book. The projects\\nhave reference designs targeting FPGA boards distributed through the Xilinx University Program\\n(http://www.xilinx.com/support/university.html). The FPGA boards are available for com-\\nmercialpurchase.'),\n",
       " Document(page_content='reference designs targeting FPGA boards distributed through the Xilinx University Program\\n(http://www.xilinx.com/support/university.html). The FPGA boards are available for com-\\nmercialpurchase. AnyreaderofthebookisencouragedtorequestanevaluationlicenseofVivado(cid:13)R\\nHLS at http://www.xilinx.com.\\nThis book is not primarily about HLS algorithms. There are many excellent resources that\\nprovide details about the HLS process including algorithms for scheduling, resource allocation,\\nand binding [51, 29, 18, 26]. This book is valuable in a course that focuses on these concepts as\\nsupplementary material, giving students an idea of how the algorithms fit together in a coherent\\nform, and providing concrete use cases of applications developed in a HLS language. This book is\\nalso not primarily about the intricacies of FPGA architectures or RTL design techniques. However,\\nagain it may be valuable as supplementary material for those looking to understand more about\\nthe system-level context.\\nThis'),\n",
       " Document(page_content='the intricacies of FPGA architectures or RTL design techniques. However,\\nagain it may be valuable as supplementary material for those looking to understand more about\\nthe system-level context.\\nThis book focuses on using Xilinx tools for implementing designs, in particular Vivado(cid:13)R HLS\\n7CONTENTS CONTENTS\\nto perform the translation from C-like code to RTL. C programming examples are given that are\\nspecific to the syntax used in Vivado(cid:13)R HLS. In general, the book explains not only Vivado(cid:13)R HLS\\nspecifics, but also the underlying generic HLS concepts that are often found in other tools. We\\nencourage readers with access to other tools to understand how these concepts are interpreted in\\nany HLS tool they may be using.\\nGood luck and happy programming!\\n8Acknowledgements\\nMany people have contributed to making this book happen. Probably first and foremost are the\\nmany people who have done research in the area of High-Level Synthesis. Underlying each of the\\napplications in'),\n",
       " Document(page_content='people have contributed to making this book happen. Probably first and foremost are the\\nmany people who have done research in the area of High-Level Synthesis. Underlying each of the\\napplications in this book are many individual synthesis and mapping technologies which combine\\nto result in high-quality implementations.\\nManypeoplehaveworkeddirectlyontheVivado(cid:13)R HLStoolovertheyears. Fromthebeginning\\nin Jason Cong’s research group at UCLA as the AutoPilot tool, to making a commercial product\\nat AutoESL Inc., to widespread adoption at Xilinx, it’s taken many hours of engineering to build\\nan effective tool. To Zhiru Zhang, Yiping Fan, Bin Liu, Guoling Han, Kecheng Hao, Peichen Pan,\\nDevadas Varma, Chuck Song, and many others: your efforts are greatly appreciated.\\nTheideaforthisbookoriginallyaroseoutofahallwayconversationwithSalilRajeshortlyafter\\nXilinx acquired the AutoESL technology. Much thanks to Salil for his early encouragement and\\nfinancial support. Ivo Bolsens and Kees Vissers'),\n",
       " Document(page_content='acquired the AutoESL technology. Much thanks to Salil for his early encouragement and\\nfinancial support. Ivo Bolsens and Kees Vissers also had the trust that we could make something\\nworth the effort. Thanks to you all for having the patience to wait for good things to emerge.\\nThis book would not have been possible without substantial support from the UCSD Wireless\\nEmbedded Systems Program. This book grew out of the need to make a hardware design class that\\ncould be broadly applicable to students coming from a mix of software and hardware backgrounds.\\nThe program provided substantial resources in terms of lab instructors, teaching assistants, and\\nsupplies that were invaluable as we developed (and re-developed) the curriculum that eventually\\nmorphed into this book. Thanks to all the UCSD 237C students over the years for providing\\nfeedback on what made sense, what didn’t, and generally acting as guinea pigs over many revisions\\nof the class and this book. Your suggestions and feedback were'),\n",
       " Document(page_content='students over the years for providing\\nfeedback on what made sense, what didn’t, and generally acting as guinea pigs over many revisions\\nof the class and this book. Your suggestions and feedback were extremely helpful. A special thanks\\nto the TAs for these classes, notable Alireza Khodamoradi, Pingfan Meng, Dajung Lee, Quentin\\nGautier, and Armaiti Ardeshiricham; they certainly felt the growing pains a lot more than the\\ninstructor.\\nVariouscolleagues havebeen subjected to early drafts of the book, including Zhiru Zhang, Mike\\nWirthlin, Jonathan Corbett. We appreciate your feedback.\\n9CONTENTS CONTENTS\\n10Chapter 1\\nIntroduction\\n1.1 High-level Synthesis (HLS)\\nThe hardware design process has evolved significantly over the years. When the circuits were\\nsmall, hardware designers could more easily specify every transistor, how they were wired together,\\nand their physical layout. Everything was done manually. As our ability to manufacture more\\ntransistors increased, hardware designers began to rely'),\n",
       " Document(page_content='every transistor, how they were wired together,\\nand their physical layout. Everything was done manually. As our ability to manufacture more\\ntransistors increased, hardware designers began to rely on automated design tools to help them in\\ntheprocessofcreatingthecircuits. Thesetoolsgraduallybecomemoreandmoresophisticatedand\\nallowed hardware designers to work at higher levels of abstraction and thus become more efficient.\\nRather than specify the layout of every transistor, a hardware designer could instead specify digital\\ncircuits and have electronic design automation (EDA) tools automatically translate these more\\nabstract specifications into a physical layout.\\nThe Mead and Conway approach [50] of using a programming language (e.g., Verilog or VHDL)\\nthat compiles a design into physical chips took hold in the 1980s. Since that time, the hardware\\ncomplexity has continued to increase at an exponential rate, which forced hardware designers to\\nmove to even more abstract hardware programming'),\n",
       " Document(page_content='took hold in the 1980s. Since that time, the hardware\\ncomplexity has continued to increase at an exponential rate, which forced hardware designers to\\nmove to even more abstract hardware programming languages. register-transfer level (RTL) was\\none step in abstraction, enabling a designer to simply specify the registers and the operations\\nperformed on those registers, without considering how the registers and operations are eventually\\nimplementation. EDA tools can translate RTL specifications into a digital circuit model and then\\nsubsequently into the detailed specification for a device that implements the digital circuit. This\\nspecification might be the files necessary to manufacture a custom device or might be the files\\nnecessary to program an off-the-shelf device, such as an field-programmable gate array (FPGA).\\nUltimately,thecombinationoftheseabstractionsenablesdesignerstobuildextraordinarilycomplex\\nsystemswithoutgettinglostinthedetailsofhowtheyareimplemented.'),\n",
       " Document(page_content='such as an field-programmable gate array (FPGA).\\nUltimately,thecombinationoftheseabstractionsenablesdesignerstobuildextraordinarilycomplex\\nsystemswithoutgettinglostinthedetailsofhowtheyareimplemented. Anon-technicalperspective\\non the value of these abstractions can be found in [42].\\nhigh-level synthesis (HLS) is yet another step in abstraction that enables a designer to focus on\\nlargerarchitecturalquestionsratherthanindividualregistersandcycle-to-cycleoperations. Instead\\na designer captures behavior in a program that does not include specific registers or cycles and an\\nHLS tool creates the detailed RTL micro-architecture. One of the first tools to implement such\\na flow was based on behavioral Verilog and generated an RTL-level architecture also captured in\\nVerilog[35]. Many commercial tools now use C/C++ as the input language. For the most part\\nthe language is unimportant, assuming that you have a tool that accepts the program you want to\\nsynthesize!\\nFundamentally, algorithmic HLS does'),\n",
       " Document(page_content='now use C/C++ as the input language. For the most part\\nthe language is unimportant, assuming that you have a tool that accepts the program you want to\\nsynthesize!\\nFundamentally, algorithmic HLS does several things automatically that an RTL designer does\\nmanually:\\n11INTRODUCTION\\n• HLS analyzes and exploits the concurrency in an algorithm.\\n• HLSinsertsregistersasnecessarytolimitcriticalpathsandachieveadesiredclockfrequency.\\n• HLS generates control logic that directs the data path.\\n• HLS implements interfaces to connect to the rest of the system.\\n• HLS maps data onto storage elements to balance resource usage and bandwidth.\\n• HLS maps computation onto logic elements performing user specified and automatic opti-\\nmizations to achieve the most efficient implementation.\\nGenerally, the goal of HLS is to make these decisions automatically based upon user-provided\\ninput specification and design constraints. However, HLS tools greatly differ in their ability to do\\nthis effectively. Fortunately,'),\n",
       " Document(page_content='is to make these decisions automatically based upon user-provided\\ninput specification and design constraints. However, HLS tools greatly differ in their ability to do\\nthis effectively. Fortunately, there exist many mature HLS tools (e.g., Xilinx Vivado(cid:13)R HLS, LegUp\\n[13], and Mentor Catapult HLS) that can make these decisions automatically for a wide range\\nof applications. We will use Vivado(cid:13)R HLS as an exemplar for this book; however, the general\\ntechniques are broadly applicable to most HLS tools though likely with some changes in input\\nlanguage syntax/semantics.\\nIngeneral,thedesignerisexpectedtosupplytheHLStoolafunctionalspecification,describethe\\ninterface,provideatargetcomputationaldevice,andgiveoptimizationdirectives. Morespecifically,\\nVivado(cid:13)R HLS requires the following inputs:\\n• A function specified in C, C++, or SystemC\\n• A design testbench that calls the function and verifies its correctness by checking the results.\\n• A target FPGA device\\n• The desired'),\n",
       " Document(page_content='following inputs:\\n• A function specified in C, C++, or SystemC\\n• A design testbench that calls the function and verifies its correctness by checking the results.\\n• A target FPGA device\\n• The desired clock period\\n• Directives guiding the implementation process\\nIn general, HLS tools can not handle any arbitrary software code. Many concepts that are com-\\nmon in software programming are difficult to implement in hardware. Yet, a hardware description\\noffers much more flexibility in terms of how to implement the computation. It typically requires\\nadditional information to be added by the designers (suggestions or #pragmas) that provide hints\\nto the tool about how to create the most efficient design. Thus, HLS tools simultaneously limit\\nand enhance the expressiveness of the input language. For example, it is common to not be able to\\nhandle dynamic memory allocation. There is often limited support for standard libraries. System\\ncalls are typically avoided in hardware to reduce complexity. The'),\n",
       " Document(page_content='it is common to not be able to\\nhandle dynamic memory allocation. There is often limited support for standard libraries. System\\ncalls are typically avoided in hardware to reduce complexity. The ability to perform recursion\\nis often limited. On the other hand, HLS tools can deal with a variety of different interfaces\\n(direct memory access, streaming, on-chip memories). And these tools can perform advanced opti-\\nmizations (pipelining, memory partitioning, bitwidth optimization) to create an efficient hardware\\nimplementation.\\nWe make the following assumptions about the input function specification, which generally\\nadheres to the guidelines of the Vivado(cid:13)R HLS tool:\\n• No dynamic memory allocation (no operators like malloc(), free(), new, and delete())\\n• Limited use of pointers-to-pointers (e.g., may not appear at the interface)\\n12INTRODUCTION\\n• System calls are not supported (e.g., abort(), exit(), printf(), etc. They can be used in the\\ncode, e.g., in the testbench, but they are'),\n",
       " Document(page_content='(e.g., may not appear at the interface)\\n12INTRODUCTION\\n• System calls are not supported (e.g., abort(), exit(), printf(), etc. They can be used in the\\ncode, e.g., in the testbench, but they are ignored (removed) during synthesis.\\n• Limited use of other standard libraries (e.g., common math.h functions are supported, but\\nuncommon ones are not)\\n• Limited use of function pointers and virtual functions in C++ classes (function calls must be\\ncompile-time determined by the compiler).\\n• No recursive function calls.\\n• The interface must be precisely defined.\\nTheprimaryoutputofanHLStoolisaRTLhardwaredesignthatiscapableofbeingsynthesized\\nthrough the rest of the hardware design flow. Additionally, the tool may output testbenches to\\naid in the verification process. Finally, the tool will provide some estimates on resource usage and\\nperformance. Vivado(cid:13)R HLS generates the following outputs:\\n• Synthesizable Verilog and VHDL\\n• RTL simulations based on the design testbench\\n• Static analysis of'),\n",
       " Document(page_content='estimates on resource usage and\\nperformance. Vivado(cid:13)R HLS generates the following outputs:\\n• Synthesizable Verilog and VHDL\\n• RTL simulations based on the design testbench\\n• Static analysis of performance and resource usage\\n• Metadata at the boundaries of a design, making it easier to integrate into a system.\\nOnce an RTL-level design is available, other tools are usually used in a standard RTL design\\nflow. In the Xilinx Vivado(cid:13)RDesign Suite, logic synthesis is performed, translating the RTL-level\\ndesign into a netlist of primitive FPGA logical elements. The netlist (consisting of logical elements\\nand the connections between them) is then associated with specific resources in a target device,\\na process called place and route (PAR). The resulting configuration of the FPGA resources is\\ncaptured in a bitstream, which can be loaded onto the FPGA to program its functionality. The\\nbitstream contains a binary representation of the configuration of each FPGA resource,'),\n",
       " Document(page_content='FPGA resources is\\ncaptured in a bitstream, which can be loaded onto the FPGA to program its functionality. The\\nbitstream contains a binary representation of the configuration of each FPGA resource, including\\nlogicelements, wireconnections, andon-chipmemories. AlargeXilinxUltraScaleFPGAswillhave\\nover 1 billion configuration bits and even the “smaller” devices have hundreds of millions of bits\\n[64].\\n1.2 FPGA Architecture\\nIt is important to understand the modern FPGA architectures since many of the HLS optimiza-\\ntions specifically target these features. Over the decades, FPGAs have gone from small arrays of\\nprogrammable logic and interconnect to massive arrays of programmable logic and interconnect\\nwith on-chip memories, custom data paths, high speed I/O, and microprocessor cores all co-located\\non the same chip. In this section, we discuss the architectural features that are relevant to HLS.\\nIt is not our intention (nor do we think it is necessary) to provide substantial details of the'),\n",
       " Document(page_content='the same chip. In this section, we discuss the architectural features that are relevant to HLS.\\nIt is not our intention (nor do we think it is necessary) to provide substantial details of the FPGA\\narchitecture. Rather we aim to give the reader enough information to understand the HLS reports\\nandsuccessfullyuseandleveragetheHLSdirectives, manyofwhichveryspecificallytargetmodern\\nFPGA architectural features.\\nFPGAs are an array of programmable logic blocks and memory elements that are connected\\ntogether using programmable interconnect. Typically these logic blocks are implemented as a\\nlookup table (LUT) – a memory where the address signal are the inputs and the outputs are stored\\n13INTRODUCTION\\na)Lookup Table (LUT) b) c) Slice\\nin[1] in[0] out\\nConfiguration Bit0 0 0 0 LUT Memory\\n0 1 0\\nConfiguration Bit1\\nout 1 0 0 in\\nConfiguration Bit2 1 1 1 3\\nFF\\nConfiguration Bit3\\nout = in[1] & in[0] FF\\n2\\nSelect\\nin out\\nFigure 1.1: Part a) shows a 2 input LUT, i.e., a 2-LUT. Each of the four configuration'),\n",
       " Document(page_content='Bit1\\nout 1 0 0 in\\nConfiguration Bit2 1 1 1 3\\nFF\\nConfiguration Bit3\\nout = in[1] & in[0] FF\\n2\\nSelect\\nin out\\nFigure 1.1: Part a) shows a 2 input LUT, i.e., a 2-LUT. Each of the four configuration bits can\\nbe programmed to change the function of the 2-LUT making it a fully programmable 2 input logic\\ngate. Part b) provides a sample programming to implement an AND gate. The values in the “out”\\ncolumn from top to bottom correspond directly to configuration bits 0 through 3. Part c) shows a\\nsimple slice that contains a slightly more complex 3-LUT with the possibility of storing the output\\ninto a flip-flop (FF). Note that there are nine configuration bits: eight to program the 3-LUT and\\none to decide whether the output should be direct from the 3-LUT or the one stored in the FF.\\nMore generally, a slice is defined as a small number of LUTs and FFs combined with routing logic\\n(multiplexers) to move inputs, outputs, and internal values between the LUTs and FFs.\\ninthememoryentries.'),\n",
       " Document(page_content='generally, a slice is defined as a small number of LUTs and FFs combined with routing logic\\n(multiplexers) to move inputs, outputs, and internal values between the LUTs and FFs.\\ninthememoryentries. Ann-bitLUTcanbeprogrammedtocomputeanyn-inputBooleanfunction\\nby using the function’s truth table as the values of the LUT memory.\\nFigure1.1a)showsa2inputLUT.Ithas22 = 4configurationbits. Thesebitsaretheonesthat\\nare programmed to determine the functionality of the LUT. Figure 1.1 b) shows the truth table\\nfor a 2 input AND gate. By using the four values in the “out” column for configuration bits 0-3,\\nwe can program this 2 input LUT to be a 2 input AND gate. The functionality is easily changed\\nby reprogramming the configuration bits. This is a flexible and fairly efficient method for encoding\\nsmaller Boolean logic functions. Most FPGAs use a LUTs with 4-6 input bits as their base element\\nfor computation. Larger FPGAs can have millions of these programmable logic elements.\\nHow would you program'),\n",
       " Document(page_content='Boolean logic functions. Most FPGAs use a LUTs with 4-6 input bits as their base element\\nfor computation. Larger FPGAs can have millions of these programmable logic elements.\\nHow would you program the 2-LUT from Figure 1.1 to implement an XOR gate? An OR\\ngate? How many programming bits does an n input (n-LUT) require?\\nHowmanyuniquefunctionscana2-LUTbeprogrammedtoimplement? Howmanyunique\\nfunctions can a n input (n-LUT) implement?\\nThe FF is the basic memory element for the FPGA. They are typically co-located with a\\nLUTs. LUTs can be replicated and combined with FFs and other specialized functions (e.g., a\\nfull adder) to create a more complex logic element called a configurable logic block (CLB), logic\\narray block (LAB), or slice depending on the vendor or design tool. We use the term slice since it\\nis the resource reported by the Vivado(cid:13)R HLS tool. A slice is a small number of LUTs, FFs and\\nmultiplexers combined to make a more powerful programmable logic element. The exact'),\n",
       " Document(page_content='slice since it\\nis the resource reported by the Vivado(cid:13)R HLS tool. A slice is a small number of LUTs, FFs and\\nmultiplexers combined to make a more powerful programmable logic element. The exact number\\nand combination of LUTs, FFs and multiplexers varies by architecture, but generally a slice has\\nonly few of each of these elements. Figure 1.1 c) shows a very simple slice with one 3-LUT and\\n14INTRODUCTION\\n...\\nSlice\\n...\\nRouting Channel\\nlennahC\\ngnituoR\\nSwitch\\nRouting Track\\n...\\nSwitchbox\\nFigure 1.2: A slice contains a small number of LUTs and FF. We show a very simple slice with\\none LUT and one FF though generally these have two or more of each. Slices are connected to one\\nanother using a routing channel and switchbox. These two provide a programmable interconnect\\nthat provide the data movement between the programmable logic elements (slices). The switch-\\nbox has many switches (typically implemented as pass transistors) that allow for arbitrary wiring\\nconfigurations between the'),\n",
       " Document(page_content='data movement between the programmable logic elements (slices). The switch-\\nbox has many switches (typically implemented as pass transistors) that allow for arbitrary wiring\\nconfigurations between the different routing tracks in the routing tracks adjacent to the switchbox.\\none FF. A slice may also use some more complex logic functions. For example, it is common to\\nembedded a full adder into a slice. This is an example of “hardening” the FPGA; this full adder is\\nnot programmable logic – it can only be used as a full adder, but it is common to use full adders\\n(to make addition operations) and it is more efficient to use the custom full adder as opposed to\\nimplementing a full adder on the programmable logic (which is also an option). And thus, overall\\nit is beneficial to have a hard full adder in the slice.\\nProgrammable interconnect is the other key element of an FPGA. It provides a flexible network\\nof wires to create connections between the slices. The inputs and outputs of the slice'),\n",
       " Document(page_content='adder in the slice.\\nProgrammable interconnect is the other key element of an FPGA. It provides a flexible network\\nof wires to create connections between the slices. The inputs and outputs of the slice are connected\\nto a routing channel. The routing channel contains a set configuration bits can be programmed to\\nconnect or disconnect the inputs/outputs of the slice to the programmable interconnect. Routing\\nchannelsareconnectedtoswitchboxes. Aswitchboxisacollectionofswitchesthatareimplemented\\naspasstransistors. Theseprovidetheabilitytoconnecttheroutingtracksfromoneroutingchannels\\nto another.\\nFigure 1.2 provides an example of how a slice, routing channel, and switchbox are connected.\\nEachinput/outputtotheslicecanbeconnectedtooneofmanyroutingtracksthatexistinarouting\\nchannel. You can think of routing tracks as single bit wires. The physical connections between\\nthe slice and the routing tracks in the routing channel are configured using a pass transistor that\\nis programmed to perform a'),\n",
       " Document(page_content='of routing tracks as single bit wires. The physical connections between\\nthe slice and the routing tracks in the routing channel are configured using a pass transistor that\\nis programmed to perform a connect or disconnect from the input/output of the slice and the\\nprogrammable interconnect.\\nThe switchboxes provides a connection matrix between routing tracks in adjacent routing chan-\\nnels. Typically, an FPGA has a logical 2D representation. That is, the FPGA is designed in a\\nmanner that provides a 2D abstraction for computation. This is often called an “island-style” ar-\\nchitecture where the slices represent “logic islands” that are connected using the routing channels\\nand switchboxes. Here the switchbox connects to four routing channels to the north, east, south,\\n15INTRODUCTION\\nSwitchbox\\nRouting Channel\\nI/O Block Slice\\nFigure 1.3: The two-dimensional structure of an FPGA showing an island style architecture.'),\n",
       " Document(page_content='to four routing channels to the north, east, south,\\n15INTRODUCTION\\nSwitchbox\\nRouting Channel\\nI/O Block Slice\\nFigure 1.3: The two-dimensional structure of an FPGA showing an island style architecture. The\\nlogicandmemoryresourcesintheslicesareinterconnectedusingroutingchannelsandswitchboxes.\\nThe input/output (I/O) blocks provide an external interface, e.g., to a memory, microprocessor,\\nsensor,and/oractuator. OnsomeFPGAs,theI/Odirectlyconnectstothechippins. OtherFPGAs\\nuse the I/O to connect the programmable logic fabric to on-chip resources (e.g., a microprocessor\\nbus or cache).\\nand west directions. The exact programming of the switches in the routing channels and switch-\\nboxes determines how the inputs and outputs of the programmable logic blocks are connected. The\\nnumber of channels, the connectivity of the switchboxes, the structure of the slice, and other logic\\nandcircuitlevelFPGAarchitecturaltechniquesareverywellstudied; werefertheinterestedreader\\nto the following books and surveys'),\n",
       " Document(page_content='connectivity of the switchboxes, the structure of the slice, and other logic\\nandcircuitlevelFPGAarchitecturaltechniquesareverywellstudied; werefertheinterestedreader\\nto the following books and surveys on this topic for more information [12, 10, 30]. Generally, it\\nis not necessary to understand all of the nitty-gritty details of the FPGA architecture in order to\\nsuccessfully use the HLS tools, rather it is more important to have a general understanding of the\\nvarious FPGA resources and how the HLS optimizations effect the resource usage.\\nFigure 1.3 provides an even more abstract depiction of an FPGA programmable logic and\\ninterconnect. This provides a larger view of the 2D dimensional layout of the programmable logic\\n(e.g., slices), routing channels, and switchboxes. The FPGA programmable logic uses I/O blocks\\nto communicate with an external device. This may be a microcontroller (e.g., an on-chip ARM\\nprocessor using an AXI interface), memory (e.g., an on-chip cache or an off-chip DRAM'),\n",
       " Document(page_content='logic uses I/O blocks\\nto communicate with an external device. This may be a microcontroller (e.g., an on-chip ARM\\nprocessor using an AXI interface), memory (e.g., an on-chip cache or an off-chip DRAM memory\\ncontroller), a sensor (e.g., an antenna through an A/D interface), or a actuator (e.g., a motor\\nthrough an D/A interface). More recently, FPGAs have integrated custom on-chip I/O handlers,\\ne.g., memory controllers, transceivers, or analog-to-digital (and vice versa) controllers directly into\\n16INTRODUCTION\\nHigh\\nSpeed\\nInterfaces\\nMicroprocessor\\nDSP\\nBRAM\\nBlock\\nFigure 1.4: Modern FPGAs are becoming more heterogenous with a mix of programmable logic\\nelementsand“hardened”architecturalelementslikeregisterfiles,customdatapaths,andhighspeed\\ninterconnect. The FPGA is often paired with one or more microprocessors, e.g., ARM or x86 cores,\\nthat coordinates the control of the system.\\nthe fabric in order to increase performance.\\nAs the number of transistors on the FPGA has continued to increase,'),\n",
       " Document(page_content='microprocessors, e.g., ARM or x86 cores,\\nthat coordinates the control of the system.\\nthe fabric in order to increase performance.\\nAs the number of transistors on the FPGA has continued to increase, FPGA architectures\\nhave begun to incorporate more and more “hard” resources. These are custom resources designed\\nspecifically to perform a task. For example, many applications make heavy use of addition and\\nmultiplication operations. Thus, the FPGA fabric added custom resources targeted at these oper-\\nations. An example of this is the DSP48 custom datapaths, which efficiently implement a series\\nof arithmetic operations including multiplication, addition, multiply-accumulate, and word level\\nlogical operations. These DSP48 blocks have some programmability, but are not as flexible as the\\nprogrammable logic. Yet, implementing a multiply or MAC operation on these DSP48s is much\\nmoreefficientthanperformingthesameoperationontheprogrammablelogic. Thus, thereisafun-\\ndamental tradeoff of efficiency'),\n",
       " Document(page_content='logic. Yet, implementing a multiply or MAC operation on these DSP48s is much\\nmoreefficientthanperformingthesameoperationontheprogrammablelogic. Thus, thereisafun-\\ndamental tradeoff of efficiency versus flexibility. Modern FPGAs will have hundreds to thousands\\nof these DSP48 distributed throughout the logic fabric as shown in Figure 1.4.\\nComparetheperformanceofamultiplyaccumulateoperationusingtheprogrammablelogic\\nversus the DSP48s. What is the maximum frequency that one can obtain in both cases? How\\ndoes the FPGA resource usage change?\\nA block RAM (BRAM) is another example of a hardened resource. BRAMs are configurable\\nrandomaccessmemorymodulesthatsupportdifferentmemorylayoutsandinterfaces. Forexample,\\nthey can be changed to have byte, half-word, word, and double word transfers and connected to a\\nvariety of different interfaces including local on-chip buses (for talking to the programmable fabric)\\nandprocessorbuses(tocommunicatewithon-chipprocessors).'),\n",
       " Document(page_content='double word transfers and connected to a\\nvariety of different interfaces including local on-chip buses (for talking to the programmable fabric)\\nandprocessorbuses(tocommunicatewithon-chipprocessors). Generally,theseareusedtotransfer\\ndata between on-chip resources (e.g., the FPGA fabric and microprocessor) and store large data\\n17INTRODUCTION\\nExternal\\nMemory BRAM FFs\\ncount 1-4 thousands millions\\nsize GBytes KBytes Bits\\ntotal size GBytes MBytes 100s of KBytes\\nwidth 8-64 1-16 1\\ntotal bandwidth GBytes/sec TBytes/sec 100s of TBytes/sec\\nFigure 1.5: A comparison of three different on- and off-chip memory storage options. External\\nmemory provides the most density but has limited total bandwidth. Moving on-chip there are two\\noptions: FFs and BRAMs. FFs have the best total bandwidth but only a limited amount of total\\ndata storage capability. BRAMs provide an intermediate value between external memory and FFs.\\nsets on chip. We could choose to store the data set in the slices (using the FFs) but'),\n",
       " Document(page_content='amount of total\\ndata storage capability. BRAMs provide an intermediate value between external memory and FFs.\\nsets on chip. We could choose to store the data set in the slices (using the FFs) but this would\\nincur overheads in performance and resource usage.\\nAtypicalBRAMhasaround32Kbitofmemorystoragewhichcanbeconfiguredas32Kx1bit,\\n16K x 2 bits, 8K x 4 bits, etc. They can be cascaded together to create larger memories. All of this\\nconfiguration is done by the Vivado tools; this is a major advantage of Vivado(cid:13)R HLS: the designer\\ndoes not need to worry about these low level details. The BRAMs are typically co-located next\\nto the DSP48. For HLS, it may be beneficial to think of the BRAMs as configurable register files.\\nThese BRAMs can directly feed the custom datapaths (DSP48s), talk to on-chip microprocessors,\\nand transfer data to custom datapaths implemented on the programmable logic.\\nImplement a design that stores a large (e.g., thousand item) array in BRAMs and pro-\\ngrammable'),\n",
       " Document(page_content='to on-chip microprocessors,\\nand transfer data to custom datapaths implemented on the programmable logic.\\nImplement a design that stores a large (e.g., thousand item) array in BRAMs and pro-\\ngrammable logic. How does the performance change? What about the resource usage?\\nFigure 1.5 provides a comparison between different on-chip and off-chip memory resources.\\nThere are millions of FFs on-chip and these provide hundreds of Kbytes of bit level storage. These\\ncan be read to and written to on every cycle and thus provide a tremendous amount of total\\nbandwidth. Unfortunately, theydonotprovidethemostefficientstoragecapacity. BRAMsprovide\\na bit more storage density at the cost of limited total bandwidth. Only one or two entries of the\\nBRAMs can be accessed during every cycle which is the major limiting factor for the bandwidth.\\nGoing even further in this direction, we can use very high density off-chip external memory, but\\nthe bandwidth is even further reduced. The decision about where to'),\n",
       " Document(page_content='limiting factor for the bandwidth.\\nGoing even further in this direction, we can use very high density off-chip external memory, but\\nthe bandwidth is even further reduced. The decision about where to place your application’s data\\nis crucial and one that we will consider extensively throughout this book. The Vivado(cid:13)R HLS tool\\nhas many options that allow the designer to specify exactly where and how to store data.\\nAs on-chip transistors have become more plentiful, we have the ability to consider integrating\\nevenmorecomplexhardenedresources. On-chipmicroprocessorsareaprimeexampleofthis. High-\\nend modern FPGAs can include four or more on-chip microprocessors (e.g., ARM cores). While\\nonly the largest FPGAs include four microprocessors, it is very common to see one microprocessor\\nincluded in even the smaller FPGA devices. This provides the ability to run an operating system\\n(e.g., Linux) and quickly leverage all of its facilities, e.g., to communicate with devices through\\ndrivers, run'),\n",
       " Document(page_content='in even the smaller FPGA devices. This provides the ability to run an operating system\\n(e.g., Linux) and quickly leverage all of its facilities, e.g., to communicate with devices through\\ndrivers, run larger software packages like OpenCV, and use more common high level programming\\nlanguages like Python to get the system up and running quickly. The microprocessor is often the\\ncontroller for the system. It orchestrates data movement between off-chip memories, sensors, and\\nactuatorstoon-chipresourcesliketheBRAMs. Andthemicroprocessorcancoordinatebetweenthe\\ncustom IP cores developed with the HLS tools, third party IP cores, and the board level resources.\\n18INTRODUCTION\\nLow Performance\\nAccelerator 3 Interconnect\\nVideo Input\\nControl\\nConsole\\nAccelerator 1\\nInterface Processor\\nNetwork\\nInterface High Performance Interconnect Accelerator 2\\nStorage\\nExternal\\nInterface Memory Video Output\\nFigure 1.6: A block diagram showing a hypothetical embedded FPGA design, consisting of'),\n",
       " Document(page_content='High Performance Interconnect Accelerator 2\\nStorage\\nExternal\\nInterface Memory Video Output\\nFigure 1.6: A block diagram showing a hypothetical embedded FPGA design, consisting of I/O\\ninterfacecores(showninblue),standardcores(showningreen),andapplicationspecificaccelerator\\ncores (shown in purple). Note that accelerator cores might have streaming interfaces (Accelerator\\n2), memory-mapped interfaces (Accelerator 3), or both (Accelerator 1).\\n1.3 FPGA Design Process\\nGiven the complexity and size of modern FPGA devices, designers have looked to impose a higher-\\nlevel structure on building designs. As a result, FPGA designs are often composed of components\\nor IP cores, structured something like Figure 1.6. At the periphery of the design, close to the\\nI/O pins, is a relatively small amount of logic that implements timing-critical I/O functions or\\nprotocols, such as a memory controller block, video interface core or analog-to-digital converter.\\nThis logic, which we will refer to as an I/O'),\n",
       " Document(page_content='logic that implements timing-critical I/O functions or\\nprotocols, such as a memory controller block, video interface core or analog-to-digital converter.\\nThis logic, which we will refer to as an I/O interface core, is typically implemented as structural\\nRTL,oftenwithadditionaltimingconstraintsthatdescribethetimingrelationshipsbetweensignals\\nand the variability of these signals. These constraints must also take into account interference of\\nsignals propagating through traces in a circuit board and connectors outside of the FPGA. In\\norder to implement high speed interfaces, these cores typically make use of dedicated logic in the\\nFPGA architecture that is close to the I/O pins for serializing and deserializing data, recovering\\nand distributing clock signals, and delaying signals with picosecond accuracy in order to repeatably\\ncapture data in a register. The implementation of I/O interface cores is often highly customized for\\na particular FPGA architecture and hence are typically provided'),\n",
       " Document(page_content='accuracy in order to repeatably\\ncapture data in a register. The implementation of I/O interface cores is often highly customized for\\na particular FPGA architecture and hence are typically provided by FPGA vendors as reference\\ndesigns or off-the-shelf components, so for the purposes of this book, we can ignore the detailed\\nimplementation of these blocks.\\nAway from I/O pins, FPGA designs often contain standard cores, such as processor cores, on-\\nchip memories, and interconnect switches. Other standard cores include generic, fixed-function\\nprocessing components, such as filters, FFTs, and codecs. Although instances of these cores are\\noften parameterized and assembled in a wide variety of ways in a different designs, they are not\\ntypicallythedifferentiatingelementinacustomersdesign. Instead, theyrepresentcommodity, hor-\\nizontal technology that can be leveraged in designs in many different application areas. As a result,\\nthey are often provided by an FPGA vendor or component provider and'),\n",
       " Document(page_content='theyrepresentcommodity, hor-\\nizontal technology that can be leveraged in designs in many different application areas. As a result,\\nthey are often provided by an FPGA vendor or component provider and only rarely implemented\\nby a system designer. Unlike interface I/O cores, standard cores are primarily synchronous circuits\\nthat require few constraints other than basic timing constraints specifying the clock period. As a\\n19INTRODUCTION\\nresult, such cores are typically portable between FPGA families, although their circuit structure\\nmay still be highly optimized.\\nLastly, FPGA designs typically contain customized, application-specific accelerator cores. As\\nwith standard cores, accelerator cores are primarily synchronous circuits that can be characterized\\nby a clock period timing constraint. In contrast, however, they are almost inevitably constructed\\nfor a specific application by a system designer. These are often the “secret sauce” and used to\\ndifferentiate your system from others. Ideally'),\n",
       " Document(page_content='contrast, however, they are almost inevitably constructed\\nfor a specific application by a system designer. These are often the “secret sauce” and used to\\ndifferentiate your system from others. Ideally a designer can quickly and easily generate high-\\nperformance custom cores, perform a design space exploration about feasible designs, and integrate\\nthese into their systems in a short timeframe. This book will focus on the development of custom\\ncores using HLS in a fast, efficient and high performance manner.\\nWhen integrating a design as in Figure 1.6, there are two common design methodologies. One\\nmethodology is to treat HLS-generated accelerator cores just like any other cores. After creating\\nthe appropriate accelerator cores using HLS, the cores are composed together (for instance, in a\\ntool such as Vivado(cid:13)RIP Integrator) along with I/O interface cores and standard cores to form a\\ncomplete design. This core-based design methodology is similar to the way that FPGA designs'),\n",
       " Document(page_content='in a\\ntool such as Vivado(cid:13)RIP Integrator) along with I/O interface cores and standard cores to form a\\ncomplete design. This core-based design methodology is similar to the way that FPGA designs are\\ndeveloped without the use of HLS. A newer methodology focuses on standard design templates\\nor platforms, which combine a stable, verified composition of standard cores and I/O interface\\ncores targeting a particular board. This platform-based design methodology enables a high-level\\nprogrammer to quickly integrate different algorithms or roles within the interfaces provided by a\\nsingle platform or shell. It can also make it easier to port an accelerator from one platform to\\nanother as long as the shells support the same standardized interfaces.\\n1.4 Design Optimization\\n1.4.1 Performance Characterization\\nBefore we can talk about optimizing a design, we need to discuss the key criterion that are used to\\ncharacterize a design. The computation time is a particularly important metric for'),\n",
       " Document(page_content='Characterization\\nBefore we can talk about optimizing a design, we need to discuss the key criterion that are used to\\ncharacterize a design. The computation time is a particularly important metric for design quality.\\nWhen describing synchronous circuits, one often will use the number of clock cycles as a measure\\nof performance. However, this is not appropriate when comparing designs that have different\\nclock rates, which is typically the case in HLS. For example, the clock frequency is specified as\\nan input constraint to the Vivado(cid:13)R HLS, and it is feasible to generate different architectures for\\nthe same exact code by simply changing the target clock frequency. Thus, it is most appropriate\\nto use seconds, which allows an apples-to-apples comparison between any HLS architecture. The\\nVivado(cid:13)R HLS tool reports the number of cycles and the clock frequency. These can be used to\\ncalculate the exact amount of time that some piece of code requires to compute.\\nIt is possible to'),\n",
       " Document(page_content='HLS tool reports the number of cycles and the clock frequency. These can be used to\\ncalculate the exact amount of time that some piece of code requires to compute.\\nIt is possible to optimize the design by changing the clock frequency. The Vivado(cid:13)R HLS\\ntool takes as input a target clock frequency, and changing this frequency target will likely\\nresult in the tool generating different implementations. We discuss this throughout the book.\\nFor example, Chapter 2.4 describes the constraints the are imposed on the Vivado(cid:13)R HLS tool\\ndepending on the clock period. Chapter 2.5 discusses how increasing the clock period can\\nincrease the throughput by employing operation chaining.\\nWe use the term task to mean a fundamental atomic unit of behavior; this corresponds to a\\nfunction invocation in Vivado(cid:13)R HLS. The task latency is the time between when a task starts and\\nwhen it finishes. The task interval is the time between when one task starts and the next'),\n",
       " Document(page_content='to a\\nfunction invocation in Vivado(cid:13)R HLS. The task latency is the time between when a task starts and\\nwhen it finishes. The task interval is the time between when one task starts and the next starts\\n20INTRODUCTION\\nTask Interval = 1 Task Interval = 13\\nTask Latency = 10 Task Latency = 13\\nFigure 1.7: The task interval and task latency for two different designs. The left design is pipelined\\nwhile the right one uses a more sequential implementation.\\nor the difference between the start times of two consecutive tasks. All task input, output, and\\ncomputation is bounded by the task latency, but the start of a task may not coincide with the\\nreading of inputs and the end of a task may not coincide with writing of outputs, particularly if\\nthe task has state that is passed from one task to the next. In many designs, data rate is a key\\ndesign goal, and depends on both the task interval and the size of the arguments to the function.\\nFigure 1.7 shows two different designs of some hypothetical'),\n",
       " Document(page_content='next. In many designs, data rate is a key\\ndesign goal, and depends on both the task interval and the size of the arguments to the function.\\nFigure 1.7 shows two different designs of some hypothetical application. The horizontal axis\\nrepresentstime(increasingtotheright)andtheverticalaxisrepresentsdifferentfunctionalunitsin\\nthe design. Input operations are shaded red and and output operations are shaded orange. Active\\noperators are represented in dark blue and inactive operators are represented in light blue. Each\\nincoming arrow represents the start of a task and each outgoing arrow represents the completion\\nof a task. The diagram on the left represents four executions of an architecture that starts a new\\ntask every cycle. This corresponds to a ‘fully-pipelined’ implementation. On the right, there is\\ntask with a very different architecture, reading a block of four pieces of input data, processing it,\\nand then producing a block of four output samples after some time. This architecture has'),\n",
       " Document(page_content='there is\\ntask with a very different architecture, reading a block of four pieces of input data, processing it,\\nand then producing a block of four output samples after some time. This architecture has the\\nsame latency and interval (13 cycles) and can only process one task at a time. This behavior is in\\ncontrast to the behavior of the pipelined design on the left, which has multiple tasks executing at\\nany given instant of time. Pipelining in HLS is very similar to the concept of instruction pipelines\\nin a microprocessor. However, instead of using a simple 5-stage pipeline where the results of an\\noperation are usually written back to the register file before being executed on again, the Vivado(cid:13)R\\nHLS tool constructs a circuit that is customized to run a particular program on a particular FPGA\\ndevice. The tool optimizes the number of pipeline stages, the initiation interval (the time between\\nsuccessive data provided to the pipeline – similar to the task interval), the number and'),\n",
       " Document(page_content='FPGA\\ndevice. The tool optimizes the number of pipeline stages, the initiation interval (the time between\\nsuccessive data provided to the pipeline – similar to the task interval), the number and types of\\nfunctional units and their interconnection based on a particular program and the device that is\\nbeing targeted.\\nTheVivado(cid:13)R HLStoolcountscyclesbydeterminingthemaximumnumberofregistersbetween\\nanyinputandoutputofatask. Thus, itispossibleforadesigntohaveatasklatencyofzerocycles,\\ncorresponding to a combinational circuit which has no registers in any path from input to output.\\nAnother convention is to count the input and/or output as a register and then find the maximum\\n21INTRODUCTION\\nregisters on any path. This would result in a larger number of cycles. We use the Vivado(cid:13)R HLS\\nconvention throughout this book.\\nNote that many tools report the task interval as “throughput”. This terminology is some-\\nwhat counterintuitive since a longer task interval almost inevitably results in'),\n",
       " Document(page_content='throughout this book.\\nNote that many tools report the task interval as “throughput”. This terminology is some-\\nwhat counterintuitive since a longer task interval almost inevitably results in fewer tasks being\\ncompleted in a given time and thus lower data rates at the interfaces. Similarly, many tools\\nuse “latency” to describe a relationship between reading inputs and writing outputs. Unfor-\\ntunately, in designs with complex behavior and multiple interfaces, it is hard to characterize\\ntasks solely in terms of inputs and outputs, e.g., a task may require multiple reads or writes\\nfrom a memory interface.\\n1.4.2 Area/Throughput Tradeoffs\\nIn order to better discuss some of the challenges that one faces when using an HLS tool, let’s\\nconsider a simple yet common hardware function – the finite impulse response (FIR) filter. An\\nFIR performs a convolution on an input sequence with a fixed set of coefficients. An FIR is quite\\ngeneral – it can be used to perform different types of filter (high pass'),\n",
       " Document(page_content='response (FIR) filter. An\\nFIR performs a convolution on an input sequence with a fixed set of coefficients. An FIR is quite\\ngeneral – it can be used to perform different types of filter (high pass filter, low pass, band pass,\\netc.). Perhaps the most simple example of an FIR is a moving average filter. We will talk more\\nbackground on FIR in Chapter 2 and describe many specific optimizations that can be done using\\nHLS. But in the meantime just consider its implementation at a high level.\\nThe C code in Figure 1.8 provides a functional or task description for HLS; this can be directly\\nused as input to the Vivado(cid:13)R HLS tool, which will analyze it and produce a functionally equivalent\\nRTL circuit. This is a complex process, and we will not get into much detail about this now, but\\nthink of it as a compiler like gcc. Yet instead of outputting assembly code, the HLS “compiler”\\ncreates an RTL hardware description. In both cases, it is not necessary to understand exactly how\\nthe compiler'),\n",
       " Document(page_content='of it as a compiler like gcc. Yet instead of outputting assembly code, the HLS “compiler”\\ncreates an RTL hardware description. In both cases, it is not necessary to understand exactly how\\nthe compiler works. This is exactly why we have the compiler in the first place – to automate\\nthe design process and allow the programmer/designer to work at a higher level of abstraction.\\nYet at the same time, someone that knows more about how the compiler works will often be able\\nto write more efficient code. This is particularly important for writing HLS code since there are\\nmany options for synthesizing the design that are not typically obvious to one that only knows\\nthe “software” flow. For example, ideas like custom memory layout, pipelining, and different I/O\\ninterfaces are important for HLS, but not for a “software compiler”. These are the concepts that\\nwe focus on in this book.\\nA key question to understand is: “What circuit is generated from this code?”. Depending on\\nyour assumptions and the'),\n",
       " Document(page_content='not for a “software compiler”. These are the concepts that\\nwe focus on in this book.\\nA key question to understand is: “What circuit is generated from this code?”. Depending on\\nyour assumptions and the capabilities of a particular HLS tool, the answer can vary widely. There\\nare multiple ways that this could be synthesized by an HLS tool.\\nOnepossiblecircuitwouldexecutethecodesequentially,aswouldasimpleRISCmicroprocessor.\\nFigure 1.9 shows assembly code for the Xilinx Microblaze processor which implements the C code\\nin Figure 1.8. Although this code has been optimized, many instructions must still be executed to\\ncompute array index expressions and loop control operations. If we assume that a new instruction\\ncan be issued each clock cycle, then this code will take approximately 49 clock cycles to compute\\none output sample of the filter. Without going into the details of how the code works, we can see\\nthat one important barrier to performance in this code is how many instructions can be'),\n",
       " Document(page_content='to compute\\none output sample of the filter. Without going into the details of how the code works, we can see\\nthat one important barrier to performance in this code is how many instructions can be executed\\nin each clock cycle. Many improvements in computer architecture are fundamentally attempts\\nto execute more complex instructions that do more useful work more often. One characteristic\\nof HLS is that architectural tradeoffs can be made without needing to fit in the constraints of\\nan instruction set architecture. It is common in HLS designs to generate architectures that issue\\n22INTRODUCTION\\n#define NUM TAPS 4\\nvoid fir(int input, int ∗output, int taps[NUM TAPS])\\n{\\nstatic int delay line[NUM TAPS] = {};\\nint result = 0;\\nfor (int i = NUM TAPS − 1; i > 0; i−−) {\\ndelay line[i] = delay line[i − 1];\\n}\\ndelay line[0] = input;\\nfor (int i = 0; i < NUM TAPS; i++) {\\nresult += delay line[i] ∗ taps[i];\\n}\\n∗output = result;\\n}\\nFigure 1.8: Code for a four tap FIR filter.\\nhundreds or thousands of'),\n",
       " Document(page_content='line[i − 1];\\n}\\ndelay line[0] = input;\\nfor (int i = 0; i < NUM TAPS; i++) {\\nresult += delay line[i] ∗ taps[i];\\n}\\n∗output = result;\\n}\\nFigure 1.8: Code for a four tap FIR filter.\\nhundreds or thousands of RISC-equivalent instructions per clock with pipelines that are hundreds\\nof cycles deep.\\nBy default, the Vivado(cid:13)R HLS tool will generate an optimized, but largely sequential archi-\\ntecture. In a sequential architecture, loops and branches are transformed into control logic that\\nenables the registers, functional units, and the rest of the data path. Conceptually, this is similar\\nto the execution of a RISC processor, except that the program to be executed is converted into a\\nfinite state machine in the generated RTL rather than being fetched from the program memory. A\\nsequential architecture tends to limit the number of functional units in a design with a focus on re-\\nsource sharing over massive parallelism. Since such an implementation can be generated from most\\nprograms with minimal'),\n",
       " Document(page_content='tends to limit the number of functional units in a design with a focus on re-\\nsource sharing over massive parallelism. Since such an implementation can be generated from most\\nprograms with minimal optimization or transformation, this makes it easy for users to get started\\nwith HLS. One disadvantage of a sequential architecture is that analyzing and understanding data\\nrates is often difficult since the control logic can be complex. The control logic dictates the number\\nof cycles for the task interval and task latencies. The control logic can be quite complex, making\\nit difficult to analyze. In addition, the behavior of the control logic may also depend on the data\\nbeing processed, resulting in performance that is data-dependent.\\nHowever, the Vivado(cid:13)R HLS tool can also generate higher performance pipelined and paral-\\nlel architectures. One important class of architectures is called a function pipeline. A function\\npipeline architecture is derived by considering the code within the'),\n",
       " Document(page_content='performance pipelined and paral-\\nlel architectures. One important class of architectures is called a function pipeline. A function\\npipeline architecture is derived by considering the code within the function to be entirely part of a\\ncomputational data path, with little control logic. Loops and branches in the code are converted\\ninto unconditional constructs. As a result, such architectures are relatively simple to characterize,\\nanalyze, and understand and are often used for simple, high data rate designs where data is pro-\\ncessed continuously. Function pipeline architectures are beneficial as components in a larger design\\nsince their simple behavior allows them to be resource shared like primitive functional units. One\\ndisadvantage of a function pipeline architecture is that not all code can be effectively parallelized.\\nThe Vivado(cid:13)R HLS tool can be directed to generate a function pipeline by placing the\\n23INTRODUCTION\\nfir:\\n.frame r1,0,r15 # vars= 0, regs= 0, args= 0\\n.mask'),\n",
       " Document(page_content='code can be effectively parallelized.\\nThe Vivado(cid:13)R HLS tool can be directed to generate a function pipeline by placing the\\n23INTRODUCTION\\nfir:\\n.frame r1,0,r15 # vars= 0, regs= 0, args= 0\\n.mask 0x00000000\\naddik r3,r0,delay line.1450\\nlwi r4,r3,8 # Unrolled loop to shift the delay line\\nswi r4,r3,12\\nlwi r4,r3,4\\nswi r4,r3,8\\nlwi r4,r3,0\\nswi r4,r3,4\\nswi r5,r3,0 # Store the new input sample into the delay line\\naddik r5,r0,4 # Initialize the loop counter\\naddk r8,r0,r0 # Initialize accumulator to zero\\naddk r4,r8,r0 # Initialize index expression to zero\\n$L2:\\nmuli r3,r4,4 # Compute a byte offset into the delay line array\\naddik r9,r3,delay line.1450\\nlw r3,r3,r7 # Load filter tap\\nlwi r9,r9,0 # Load value from delay line\\nmul r3,r3,r9 # Filter Multiply\\naddk r8,r8,r3 # Filter Accumulate\\naddik r5,r5,−1 # update the loop counter\\nbneid r5,$L2\\naddik r4,r4,1 # branch delay slot, update index expression\\nrtsd r15, 8\\nswi r8,r6,0 # branch delay slot, store the output\\n.end'),\n",
       " Document(page_content='# Filter Accumulate\\naddik r5,r5,−1 # update the loop counter\\nbneid r5,$L2\\naddik r4,r4,1 # branch delay slot, update index expression\\nrtsd r15, 8\\nswi r8,r6,0 # branch delay slot, store the output\\n.end fir\\nIn\\nload\\nstore\\nbranch\\nOut\\nFigure 1.9: RISC-style assembly generated from the C code in Figure 1.8, targetting the Xilinx\\nMicroblaze processor. This code is generated using microblazeel-xilinx-linux-gnu-gcc -O1\\n-mno-xl-soft-mul -S fir.c\\n24INTRODUCTION\\ninput(n)\\nTo Register Resets\\nand Clock Enables\\n][spat\\nTask Interval = 4\\nIn\\nOut\\noutput(n)\\nTask Latency = 4\\nFigure 1.10: A “one tap per clock” architecture for an FIR filter. This architecture can be imple-\\nmented from the code in Figure 1.8.\\nTask Interval = 1\\ninput(n)\\nIn\\ntaps[0] taps[1] taps[2] taps[3]\\nOut\\nTask Latency = 1\\noutput(n)\\nFigure 1.11: A “one sample per clock” architecture for an FIR filter. This architecture can be\\nimplemented from the code in Figure 1.8 using function pipeline.\\n25INTRODUCTION\\n#pragma HLS pipeline directive in the'),\n",
       " Document(page_content='A “one sample per clock” architecture for an FIR filter. This architecture can be\\nimplemented from the code in Figure 1.8 using function pipeline.\\n25INTRODUCTION\\n#pragma HLS pipeline directive in the body of a function. This directive takes a parameter\\nthat can be used to specify the initiation interval of the the pipeline, which is the same as a\\ntask interval for a function pipeline. Figure 1.10 shows one potential design – a “one tap per\\nclock” architecture, consisting of a single multiplier and single adder to compute the filter. This\\nimplementation has a task interval of 4 and a task latency of 4. This architecture can take a\\nnew sample to filter once every 4 cycles and will generate a new output 4 cycles after the input\\nis consumed. The implementation in Figure 1.11 shows a “one sample per clock” architecture,\\nconsisting of 4 multipliers and 3 adders. This implementation has a task interval of 1 and a task\\nlatency of 1, which in this case means that the implementation accept a new'),\n",
       " Document(page_content='per clock” architecture,\\nconsisting of 4 multipliers and 3 adders. This implementation has a task interval of 1 and a task\\nlatency of 1, which in this case means that the implementation accept a new input value every\\ncycle. Other implementations are also possible, such as architectures with “two taps per clock”\\nor “two samples per clock”, which might be more appropriate for a particular application. We\\ndiscuss more ways to optimize the FIR function in depth in Chapter 2.\\nInpractice,complexdesignsoftenincludecomplicatedtradeoffsbetweensequentialarchitectures\\nand parallel architectures, in order to achieve the best overall design. In Vivado(cid:13)R HLS, these\\ntradeoffs are largely controlled by the user, through various tool options and code annotations,\\nsuch as #pragma directive.\\nWhat would the task interval of a “two taps per clock” architecture for a 4 tap filter be?\\nWhat about for a “two samples per clock” architecture?\\n1.4.3 Restrictions on Processing Rate\\nAs we have seen, the task'),\n",
       " Document(page_content='the task interval of a “two taps per clock” architecture for a 4 tap filter be?\\nWhat about for a “two samples per clock” architecture?\\n1.4.3 Restrictions on Processing Rate\\nAs we have seen, the task interval of a design can be changed by selecting different kinds of\\narchitectures, often resulting in a corresponding increase in the processing rate. However, it is\\nimportant to realize that the task interval of any processing architecture is fundamentally limited\\nin several ways. The most important limit arises from recurrences or feedback loops in a design.\\nThe other key limitation arises from resource limits.\\nA recurrence is any case where a computation by a component depends on a previous computa-\\ntion by the same component. A key concept is that recurrences fundamentally limit the throughput\\nof a design, even in the presence of pipelining [56, 43]. As a result, analyzing recurrences in algo-\\nrithms and generating hardware that is guaranteed to be correct is a key function of an HLS'),\n",
       " Document(page_content='a design, even in the presence of pipelining [56, 43]. As a result, analyzing recurrences in algo-\\nrithms and generating hardware that is guaranteed to be correct is a key function of an HLS tool.\\nSimilarly, understanding algorithms and selecting those without tight recurrences is a important\\npart of using HLS (and, in fact, parallel programming in general).\\nRecurrences can arrive in different coding constructs, such as static variables (Figure 1.8), se-\\nquentialloops(Figure1.10). Recurrencescanalsoappearinasequentialarchitectureanddisappear\\nwhen pipelining is applied (as in Figures 1.10 and 1.11). In other cases, recurrences can exist with-\\nout limiting the throughput of a sequential architecture, but can become problematic when the\\ndesign is pipelined.\\nAnother key factor that can limit processing rate are resource limitations. One form of resource\\nlimitation is associated with the wires at the boundary of a design, since a synchronous circuit\\ncan only capture or transmit one bit per'),\n",
       " Document(page_content='processing rate are resource limitations. One form of resource\\nlimitation is associated with the wires at the boundary of a design, since a synchronous circuit\\ncan only capture or transmit one bit per wire per clock cycle. As a result, if a function with the\\nsignature int32 t f(int32 t x); is implemented as a single block at 100 MHz with a task interval of 1,\\nthen the most data that it can process is 3.2 Gbits/sec. Another form of resource limitation arises\\nfrom memories since most memories only support a limited number of accesses per clock cycle. Yet\\nanother form of resource limitation comes from user constraints. If a designer limits the number of\\noperators that can instantiated during synthesis, then this places a limit on the processing rate of\\nthe resulting design.\\n26INTRODUCTION\\n#define NUM TAPS 4\\nvoid block fir(int input[256], int output[256], int taps[NUM TAPS],\\nint delay line[NUM TAPS]) {\\nint i, j;\\nfor (j = 0; j < 256; j++) {\\nint result = 0;\\nfor (i = NUM TAPS − 1; i > 0; i−−)'),\n",
       " Document(page_content='NUM TAPS 4\\nvoid block fir(int input[256], int output[256], int taps[NUM TAPS],\\nint delay line[NUM TAPS]) {\\nint i, j;\\nfor (j = 0; j < 256; j++) {\\nint result = 0;\\nfor (i = NUM TAPS − 1; i > 0; i−−) {\\n#pragma HLS unroll\\ndelay line[i] = delay line[i − 1];\\n}\\ndelay line[0] = input;\\nfor (i = 0; i < NUM TAPS; i++) {\\n#pragma HLS pipeline\\nresult += delay line[i] ∗ taps[i];\\n}\\noutput[j] = result;\\n}\\n}\\nFigure 1.12: Alternative code implementing an FIR filter.\\n1.4.4 Coding Style\\nAnother key question you should ask yourself is, “Is this code the best way to capture the algo-\\nrithm?”. In many cases, the goal is not only the highest quality of results, but maintainable and\\nmodifiable code. Although this is somewhat a stylistic preference, coding style can sometimes limit\\nthe architectures that a HLS tool can generate from a particular piece of code.\\nFor instance, while a tool might be able to generate either architecture in Figure 1.11 or 1.10\\nfrom the code in Figure 1.8, additing additional directives'),\n",
       " Document(page_content='generate from a particular piece of code.\\nFor instance, while a tool might be able to generate either architecture in Figure 1.11 or 1.10\\nfrom the code in Figure 1.8, additing additional directives as shown in Figure 1.12 would result in a\\nspecific architecture. In this case the delay line is explicitly unrolled, and the multiply-accumulate\\nfor loop is stated to be implemented in a pipelined manner. This would direct the HLS tool to\\nproduce an architecture that looks more like the pipelined one in Figure 1.11.\\nThe chapter described how to build filters with a range of different processing rates, up to\\none sample per clock cycle. However, many designs may require processing data at a higher\\nrate, perhaps several samples per clock cycle. How would you code such a design? Implement\\na 4 samples per clock cycle FIR filter. How many resources does this architecture require (e.g.,\\nnumber of multipliers and adders)? Which do you think will use more FPGA resources: the 4\\nsamples per clock or'),\n",
       " Document(page_content='per clock cycle FIR filter. How many resources does this architecture require (e.g.,\\nnumber of multipliers and adders)? Which do you think will use more FPGA resources: the 4\\nsamples per clock or the 1 sample per clock cycle design?\\nWe look further into the optimization of an FIR function in more detail in Chapter 2. We\\ndiscuss how to employ different optimizations (pipelining, unrolling, bitwidth optimization, etc.),\\nand describe their effects on the performance and resource utilization of the resulting generated\\narchitectures.\\n27INTRODUCTION\\n1.5 Restructured Code\\nWriting highly optimized synthesizable HLS code is often not a straightforward process. It involves\\na deep understanding of the application at hand, the ability to change the code such that the\\nVivado(cid:13)R HLS tool creates optimized hardware structures and utilizes the directives in an effective\\nmanner.\\nThroughout the rest of the book, we walk through the synthesis of a number of different appli-\\ncation domains –'),\n",
       " Document(page_content='optimized hardware structures and utilizes the directives in an effective\\nmanner.\\nThroughout the rest of the book, we walk through the synthesis of a number of different appli-\\ncation domains – including digital signal processing, sorting, compression, matrix operations, and\\nvideo processing. In order to get the most efficient architecture, it is important to have a deep\\nunderstanding of the algorithm. This enables optimizations that require rewriting the code rather\\nthan just adding synthesis directives – a processing that we call code restructuring.\\nRestructured code maps well into hardware, and often represents the eccentricities of the tool\\nchain, which requires deep understanding of micro-architectural constructs in addition to the al-\\ngorithmic functionality. Standard, off-the-shelf code typically yields very poor quality of results\\nthat are orders of magnitude slower than CPU designs, even when using HLS directives such as\\npipelining, unrolling, and data partitioning. Thus, it'),\n",
       " Document(page_content='code typically yields very poor quality of results\\nthat are orders of magnitude slower than CPU designs, even when using HLS directives such as\\npipelining, unrolling, and data partitioning. Thus, it is important to understand how to write code\\nthat the Vivado(cid:13)R HLS will synthesize in an optimal manner.\\nRestructured code typically differs substantially from a software implementation – even one\\nthat is highly optimized. A number of studies suggest that restructuring code is an essential\\nstep to generate an efficient FPGA design [46, 47, 15, 14, 39]. Thus, in order to get an efficient\\nhardware design, the user must write restructured code with the underlying hardware architecture\\nin mind. Writing restructured code requires significant hardware design expertise and domain\\nspecific knowledge.\\nThroughout the rest of this book, we go through a number of different applications, and show\\nhow to restructure the code for a more efficient hardware design. We present applications such\\nas'),\n",
       " Document(page_content='the rest of this book, we go through a number of different applications, and show\\nhow to restructure the code for a more efficient hardware design. We present applications such\\nas finite impulse response (FIR), discrete Fourier transform (DFT), fast Fourier transform (FFT),\\nsparse matrix vector multiply (SpMV), matrix multiplication, sorting, and Huffman encoding. We\\ndiscuss the impact of restructured code on the final hardware generated from high-level synthesis.\\nAnd we propose a restructuring techniques based on best practices. In each chapter, we aim to:\\n1. Highlight the importance of restructuring code to obtain FPGA designs with good quality of\\nresult, i.e., a design that has high performance and low area usage;\\n2. Provide restructured code for common applications;\\n3. Discuss the impact of the restructuring on the underlying hardware; and\\n4. Perform the necessary HLS directives to achieve the best design\\nThroughout the book, we use example applications to show how to move from a'),\n",
       " Document(page_content='of the restructuring on the underlying hardware; and\\n4. Perform the necessary HLS directives to achieve the best design\\nThroughout the book, we use example applications to show how to move from a baseline imple-\\nmentation and restructure the code to provide more efficient hardware design. We believe that the\\noptimization process is best understood through example. Each chapter performs a different set of\\noptimization directives including pipelining, dataflow, array partitioning, loop optimizations, and\\nbitwidth optimization. Additionally, we provide insight on the skills and knowledge necessary to\\nperform the code restructuring process.\\n1.6 Book Organization\\nWe organized this book to teach by example. Each chapter presents an application (or application\\ndomain) and walks through its implementation using different HLS optimizations. Generally, each\\n28INTRODUCTION\\nchapter focuses on a limited subset of optimization techniques. And the application complexity\\ngenerally increases in the'),\n",
       " Document(page_content='using different HLS optimizations. Generally, each\\n28INTRODUCTION\\nchapter focuses on a limited subset of optimization techniques. And the application complexity\\ngenerally increases in the later chapters. We start with a relatively simple to understand finite\\nimpulse response (FIR) filter in Chapter 2 and move on to implement complete video processing\\nsystems in Chapter 9.\\nThere are of course benefits and drawbacks to this approach. The benefits are: 1) the reader\\ncanseehowtheoptimizationsaredirectlyapplicabletoanapplication, 2)eachapplicationprovides\\nan exemplar of how to write HLS code, and 3) while simple to explain, toy examples can be hard\\nto generalize and thus do not always make the best examples.\\nThe drawbacks to the teach by example approach are: 1) most applications requires some\\nbackground to give the reader a better understanding of the computation being performed. Truly\\nunderstanding the computation often necessitates an extensive discussion on the mathematical\\nbackground'),\n",
       " Document(page_content='to give the reader a better understanding of the computation being performed. Truly\\nunderstanding the computation often necessitates an extensive discussion on the mathematical\\nbackground on the application. For example, implementing the best architecture for the fast\\nFourier transform (FFT) requires that the designer have deep understanding of the mathematical\\nconcepts behind a discrete Fourier transform (DFT) and FFT. Thus, some chapters, e.g., Chapter\\n4 (DFT) and Chapter 5 (FFT), start with a non-trivial amount of mathematical discussion. This\\nmay be off-putting to a reader simply looking to understand the basics of HLS, but we believe that\\nsuch a deep understanding is necessary to understand the code restructuring that is necessary to\\nachieve the best design. 2) some times a concept could be better explained by a toy example that\\nabstracts away some of the non-important application details.\\nTheorganizationforeachchapterfollowsthesamegeneralpattern. Achapterbeginsbyprovid-\\ning a'),\n",
       " Document(page_content='could be better explained by a toy example that\\nabstracts away some of the non-important application details.\\nTheorganizationforeachchapterfollowsthesamegeneralpattern. Achapterbeginsbyprovid-\\ning a background on the application(s) under consideration. In many cases, this is straightforward,\\ne.g., it is not too difficult to explain matrix multiplication (as in Chapter 7) while the DFT requires\\na substantial amount of discussion (Chapter 4). Then, we provide a baseline implementation – a\\nfunctionally correct but unoptimized implementation of the application using Vivado(cid:13)R HLS. After\\nthat,weperformanumberofdifferentoptimizations. Someofthechaptersfocusonasmallnumber\\nof optimizations (e.g., Chapter 3 emphasizes bitwidth optimizations) while others look at a broad\\nrange of optimizations (e.g., Chapter 2 on FIR filters). The key optimizations and design meth-\\nods are typically introduced in-depth in one chapter and then used repeatedly in the subsequent\\nchapters.\\nThe book is made to'),\n",
       " Document(page_content='(e.g., Chapter 2 on FIR filters). The key optimizations and design meth-\\nods are typically introduced in-depth in one chapter and then used repeatedly in the subsequent\\nchapters.\\nThe book is made to be read sequentially. For example, Chapter 2 introduces most of the\\noptimizations and the later chapters provide more depth on using these optimizations. The ap-\\nplications generally get more complex throughout the book. However, each chapter is relatively\\nself-contained. Thus, a more advanced HLS user can read an individual chapter if she only cares\\nto understand a particular application domain. For example, a reader interested in generating a\\nhardware accelerated sorting engine can look at Chapter 10 without necessarily have to read all of\\nthe previous chapters. This is another benefit of our teach by example approach.\\nTable 1.1 provides an overview of the types of optimization and the chapters where they are\\ncovered in at least some level of detail. Chapter 2 provides a gentle'),\n",
       " Document(page_content='of our teach by example approach.\\nTable 1.1 provides an overview of the types of optimization and the chapters where they are\\ncovered in at least some level of detail. Chapter 2 provides a gentle introduction the HLS design\\nprocess. It overviews several different optimizations, and shows how they can be used in the\\noptimization of a FIR filter. The later chapters go into much more detail on the benefits and usage\\nof these optimizations.\\nThenextsetofchapters(Chapters3through5)builddigitalsignalprocessingblocks(CORDIC,\\nDFT, and FFT). Each of these chapters generally focuses on one optimization: bitwidth optimiza-\\ntion (Chapter 3), array optimizations (Chapter 4, and task pipelining (Chapter 5). For example,\\nChapter 3 gives an in-depth discussion on how to perform bitwidth optimizations on the CORDIC\\napplication. Chapter 4 provides an introduction to array optimizations, in particular, how to per-\\nformarraypartitioninginordertoincreasetheon-chipmemorybandwidth. Thischapteralsotalks\\nabout'),\n",
       " Document(page_content='CORDIC\\napplication. Chapter 4 provides an introduction to array optimizations, in particular, how to per-\\nformarraypartitioninginordertoincreasetheon-chipmemorybandwidth. Thischapteralsotalks\\nabout loop unrolling and pipelining, and the need to perform these three optimizations in concert.\\n29INTRODUCTION\\nTable 1.1: A map describing the types of HLS optimizations and the chapters that discuss the\\nconcepts beyond them.\\nChapter\\nRIF\\nCIDROC\\nTFD TFF\\nVMPS\\nluMtaM\\nmargotsiH\\noediV\\ngnitroS\\nnamffuH\\n2 3 4 5 6 7 8 9 10 11\\nLoop Unrolling X X X X X X\\nLoop Pipelining X X X X X X X X\\nBitwidth Optimization X X X\\nFunction Inlining X X\\nHierarchy X X X X X X\\nArray Optimizations X X X X X X X X\\nTask Pipelining X X X X X\\nTestbench X X X X\\nCo-simulation X\\nStreaming X X X\\nInterfacing X\\nChapter 5 describes the optimizations of the FFT, which itself is a major code restructuring of\\nthe DFT. Thus, the chapter gives a background on how the FFT works. The FFT is a staged\\nalgorithm, and thus highly amenable to task'),\n",
       " Document(page_content='of the FFT, which itself is a major code restructuring of\\nthe DFT. Thus, the chapter gives a background on how the FFT works. The FFT is a staged\\nalgorithm, and thus highly amenable to task pipelining. The final optimized FFT code requires a\\nnumber of other optimizations including loop pipelining, unrolling, and array optimizations. Each\\nof these chapters is paired with a project from the Appendix. These projects lead the design\\nand optimization of the blocks, and the integration of these blocks into wireless communications\\nsystems.\\nChapters 6 through 11 provide a discussion on the optimization of more applications. Chapter\\n6 describes how to use a testbench and how to perform RTL co-simulation. It also describes array\\nand loop optimizations; these optimizations are common and thus are used in the optimization\\nof many applications. Chapter 7 introduces the streaming optimization for data flow between\\ntasks. Chapter 8 presents two applications (prefix sum and histogram) that are'),\n",
       " Document(page_content='are used in the optimization\\nof many applications. Chapter 7 introduces the streaming optimization for data flow between\\ntasks. Chapter 8 presents two applications (prefix sum and histogram) that are relatively simple,\\nbut requires careful code restructuring in order to create the optimal architecture. Chapter 9\\ntalks extensively about how to perform different types interfacing, e.g., with a video stream using\\ndifferent bus and memory interfaces. As the name implies, the video streaming requires the use of\\nthestreamprimitive,andextensiveusageofloopandarrayoptimizations. Chapter10goesthrough\\na couple of sorting algorithms. These require a large number of different optimizations. The final\\nchapter creates a complex data compression architecture. It has a large number of complex blocks\\nthat work on a more complex data structure (trees).\\n30Chapter 2\\nFinite Impulse Response (FIR) Filters\\n2.1 Overview\\nFinite Impulse Response (FIR) filters are commonplace in digital signal processing (DSP)'),\n",
       " Document(page_content='work on a more complex data structure (trees).\\n30Chapter 2\\nFinite Impulse Response (FIR) Filters\\n2.1 Overview\\nFinite Impulse Response (FIR) filters are commonplace in digital signal processing (DSP) applica-\\ntions – they are perhaps the most widely used operation in this domain. They are well suited for\\nhardware implementation since they can be implemented as a highly optimized architecture. A key\\nproperty is that they are a linear transform on contiguous elements of a signal. This maps well to\\na data structures (e.g., FIFOs or tap delay lines) that can be implemented efficiently in hardware.\\nIn general, streaming applications tend to map well to FPGAs, e.g., most of the examples that we\\npresent throughout the book have some sort of streaming behavior.\\nTwo fundamental uses for a filter are signal restoration and signal separation. Signal separation\\nis perhaps the more common use case: here one tries to isolate the input signal into different parts.\\nTypically, we think of these as'),\n",
       " Document(page_content='are signal restoration and signal separation. Signal separation\\nis perhaps the more common use case: here one tries to isolate the input signal into different parts.\\nTypically, we think of these as different frequency ranges, e.g., we may want perform a low pass\\nfilter in order remove high frequencies that are not of interest. Or we may wish to perform a band\\npass filter to determine the presence of a particular frequency in order to demodulate it, e.g., for\\nisolating tones during frequency shift keying demodulation. Signal restoration relates to removing\\nnoise and other common distortion artifacts that may have been introduced into the signal, e.g.,\\nas data is being transmitted across the wireless channel. This includes smoothing the signal and\\nremoving the DC component.\\nDigital FIR filters often deal with a discrete signal generated by sampling a continuous signal.\\nThe most familiar sampling is performed in time, i.e., the values from a signal are taken at discrete\\ninstances. These'),\n",
       " Document(page_content='often deal with a discrete signal generated by sampling a continuous signal.\\nThe most familiar sampling is performed in time, i.e., the values from a signal are taken at discrete\\ninstances. These are most often sampled at regular intervals. For instance, we might sample the\\nvoltageacrossanantennaataregularintervalwithananalog-to-digitalconverter. Alternativelywe\\nmight sample the current created by a photo-diode to determine the light intensity. Alternatively,\\nsamples may be taken in space. For instance, we might sample the value of different locations in\\nan image sensor consisting of an array of photo-diodes to create a digital image. More in-depth\\ndescriptions of signals and sampling can be found in [41].\\nThe format of the data in a sample changes depending upon the application. Digital communi-\\ncations often uses complex numbers (in-phase and quadrature or I/Q values) to represent a sample.\\nLater in this chapter we will describe how to design a complex FIR filter to handle such data.'),\n",
       " Document(page_content='often uses complex numbers (in-phase and quadrature or I/Q values) to represent a sample.\\nLater in this chapter we will describe how to design a complex FIR filter to handle such data. In\\nimage processing we often think of a pixel as a sample. A pixel can have multiple fields, e.g., red,\\ngreen, and blue (RGB) color channels. We may wish to filter each of these channels in a different\\nway again depending upon the application.\\nThegoalofthischapteristoprovideabasicunderstandingoftheprocessoftakinganalgorithm\\nand creating a good hardware design using high-level synthesis. The first step in this process\\n31FINITE IMPULSE RESPONSE (FIR) FILTERS\\nis always to have a deep understanding of the algorithm itself. This allows us to make design\\noptimizationslikecoderestructuringmuchmoreeasily. Thenextsectionprovidesanunderstanding\\nof the FIR filter theory and computation. The remainder of the chapter introduces various HLS\\noptimizations on the FIR filter. These are meant to provide an overview of'),\n",
       " Document(page_content='the FIR filter theory and computation. The remainder of the chapter introduces various HLS\\noptimizations on the FIR filter. These are meant to provide an overview of these optimizations.\\nEach of them will described in more depth in subsequent chapters.\\n2.2 Background\\nThe output signal of a filter given an impulse input signal is its impulse response. The impulse\\nresponse of a linear, time invariant filter contains the complete information about the filter. As\\nthe name implies, the impulse response of an FIR filter (a restricted type of linear, time invariant\\nfilter)isfinite, i.e., itisalwayszerofarawayfromzero. GiventheimpulseresponseofanFIRfilter,\\nwe can compute the output signal for any input signal through the process of convolution. This\\nprocess combines samples of the impulse response (also called coefficients or taps) with samples of\\nthe input signal to compute samples of the output signal. The output of filter can be computed\\nin other ways (for instance, in the frequency'),\n",
       " Document(page_content='response (also called coefficients or taps) with samples of\\nthe input signal to compute samples of the output signal. The output of filter can be computed\\nin other ways (for instance, in the frequency domain), but for the purposes of this chapter we will\\nfocus on computing in the time domain.\\nThe convolution of an N-tap FIR filter with coefficients h[] with an input signal x[] is described\\nby the general difference equation:\\nN−1\\n(cid:88)\\ny[i] = h[j]·x[i−j] (2.1)\\nj=0\\nNote that to compute a single value of the output of an N-tap filter requires N multiplies and N-1\\nadditions.\\nMoving average filters are a simple form of lowpass FIR filter where all the coefficients are\\nidentical and sum to one. For instance in the case of the three point moving filter, the coefficients\\nare h = [1, 1, 1]. It is also called a box car filter due to the shape of its convolution kernel.\\n3 3 3\\nAlternatively, you can think of a moving average filter as taking the average of several adjacent\\nsamples of the input'),\n",
       " Document(page_content='is also called a box car filter due to the shape of its convolution kernel.\\n3 3 3\\nAlternatively, you can think of a moving average filter as taking the average of several adjacent\\nsamples of the input signal and averaging them together. We can see that this equivalence by\\nsubstituting1/N forh[j]intheconvolutionequationaboveandrearrangingtoarriveatthefamiliar\\nequation for an average of N elements:\\nN\\n1 (cid:88)\\ny[i] = x[i−j] (2.2)\\nN\\nj=0\\nEach sample in the output signal can be computer by the above equation using N−1 additions\\nand one final multiplication by 1/N. Even the final multiplication can often be regrouped and\\nmerged with other operations. As a result, moving average filters are simpler to compute than a\\ngeneral FIR filter. Specifically, when N = 3 we perform this operation to calculate y[12]:\\n1\\ny[12] = ·(x[12]+x[11]+x[10]) (2.3)\\n3\\nThis filter is causal, meaning that the output is a function of no future values of the input. It is\\npossible and common to change this, for example,'),\n",
       " Document(page_content='y[12]:\\n1\\ny[12] = ·(x[12]+x[11]+x[10]) (2.3)\\n3\\nThis filter is causal, meaning that the output is a function of no future values of the input. It is\\npossible and common to change this, for example, so that the average is centered on the current\\nsample, i.e., y[12] = 1 · (x[11] + x[12] + x[13]). While fundamentally causality is an important\\n3\\nproperty for system analysis, it is less important for a hardware implementation as a finite non-\\ncausal filter can be made causal with buffering and/or reindexing of the data.\\n32FINITE IMPULSE RESPONSE (FIR) FILTERS\\nMovingaveragefilterscanbeusedtosmoothoutasignal,forexampletoremoverandom(mostly\\nhigh frequency) noise. As the number of taps N gets larger, we average over a larger number of\\nsamples, and we correspondingly must perform more computations. For a moving average filter,\\nlargervaluesofN correspondtoreducingthebandwidthoftheoutputsignal. Inessence,itisacting\\nlike a low pass filter (though not a very optimal one). Intuitively, this should make'),\n",
       " Document(page_content='a moving average filter,\\nlargervaluesofN correspondtoreducingthebandwidthoftheoutputsignal. Inessence,itisacting\\nlike a low pass filter (though not a very optimal one). Intuitively, this should make sense. As we\\naverage over larger and larger number of samples, we are eliminating higher frequency variations\\nin the input signal. That is, “smoothing” is equivalent to reducing higher frequencies. The moving\\naverage filter is optimal for reducing white noise while keeping the sharpest step response, i.e., it\\ncreates the lowest noise for a given edge sharpness.\\nNote that in general, filter coefficients can be crafted to create many different kinds of filters:\\nlow pass, high pass, band pass, etc.. In general, a larger value of number of taps provides more\\ndegrees of freedom when designing a filter, generally resulting in filters with better characteristics.\\nThere is substantial amount of literature devoted to generating filter coefficients with particular\\ncharacteristics for a given'),\n",
       " Document(page_content='a filter, generally resulting in filters with better characteristics.\\nThere is substantial amount of literature devoted to generating filter coefficients with particular\\ncharacteristics for a given application. When implementing a filter, the actual values of these\\ncoefficients are largely irrelevant and we can ignore how the coefficients themselves were arrived\\nat. However, as we saw with the moving average filter, the structure of the filter, or the particular\\ncoefficients can have a large impact on the number of operations that need to be performed. For\\ninstance, symmetric filters have multiple taps with exactly the same value which can be grouped to\\nreduce the number of multiplications. In other cases, it is possible to convert the multiplication by\\na known constant filter coefficient into shift and add operations [34]. In that case, the values of the\\ncoefficients can drastically change the performance and area of the filter implementation [52]. But\\nwe will ignore that for the time'),\n",
       " Document(page_content='into shift and add operations [34]. In that case, the values of the\\ncoefficients can drastically change the performance and area of the filter implementation [52]. But\\nwe will ignore that for the time being, and focus on generating architectures that have constant\\ncoefficients, but do not take advantage of the values of the constants.\\n2.3 Base FIR Architecture\\nConsider the code for an 11 tap FIR filter in Figure 2.1. The function takes two arguments, an\\ninput sample x, and the output sample y. This function must be called multiple times to compute\\nan entire output signal, since each time that we execute the function we provide one input sample\\nand receive one output sample. This code is convenient for modeling a streaming architecture,\\nsince we can call it as many times as needed as more data becomes available.\\nThe coefficients for the filter are stored in the c[] array declared inside of the function. These\\nare statically defined constants. Note that the coefficients are symmetric.'),\n",
       " Document(page_content='data becomes available.\\nThe coefficients for the filter are stored in the c[] array declared inside of the function. These\\nare statically defined constants. Note that the coefficients are symmetric. i.e., they are mirrored\\naround the center value c[5] = 500. Many FIR filter have this type of symmetry. We could take\\nadvantage of it in order to reduce the amount of storage that is required for the c[] array.\\nThe code uses typedef for the different variables. While this is not necessary, it is convenient\\nfor changing the types of data. As we discuss later, bit width optimization – specifically setting\\nthe number of integer and fraction bits for each variable – can provide significant benefits in terms\\nof performance and area.\\nRewrite the code so that it takes advantage of the symmetry found in the coefficients. That\\nis, change c[] so that it has six elements (c[0] through c[5]). What changes are necessary in\\nthe rest of the code? How does this effect the number of resources? How does it'),\n",
       " Document(page_content='in the coefficients. That\\nis, change c[] so that it has six elements (c[0] through c[5]). What changes are necessary in\\nthe rest of the code? How does this effect the number of resources? How does it change the\\nperformance?\\nThe code is written as a streaming function. It receives one sample at a time, and therefore\\nit must store the previous samples. Since this is an 11 tap filter, we must keep the previous 10\\n33FINITE IMPULSE RESPONSE (FIR) FILTERS\\n#define N 11\\n#include ”ap int.h”\\ntypedef int coef t;\\ntypedef int data t;\\ntypedef int acc t;\\nvoid fir(data t ∗y, data t x) {\\ncoef t c[N] = {\\n53, 0, −91, 0, 313, 500, 313, 0, −91, 0, 53\\n};\\nstatic\\ndata t shift reg[N];\\nacc t acc;\\nint i;\\nacc = 0;\\nShift Accum Loop:\\nfor (i = N − 1; i >= 0; i−−) {\\nif (i == 0) {\\nacc += x ∗ c[0];\\nshift reg[0] = x;\\n} else {\\nshift reg[i] = shift reg[i − 1];\\nacc += shift reg[i] ∗ c[i];\\n}\\n}\\n∗y = acc;\\n}\\nFigure 2.1: A functionally correct, but highly unoptimized, implementation of an 11 tap FIR filter.\\n34FINITE IMPULSE'),\n",
       " Document(page_content='else {\\nshift reg[i] = shift reg[i − 1];\\nacc += shift reg[i] ∗ c[i];\\n}\\n}\\n∗y = acc;\\n}\\nFigure 2.1: A functionally correct, but highly unoptimized, implementation of an 11 tap FIR filter.\\n34FINITE IMPULSE RESPONSE (FIR) FILTERS\\nsamples. This is the purpose of the shift reg[] array. This array is declared static since the data\\nmust be persistent across multiple calls to the function.\\nThe for loop is doing two fundamental tasks in each iteration. First, it performs the multiply\\nand accumulate operation on the input samples (the current input sample x and the previous input\\nsamples stored in shift reg[]). Each iteration of the loop performs a multiplication of one of the\\nconstants with one of the sample, and stores the running sum in the variable acc. The loop is\\nalso shifting values through shift array, which works as a FIFO. It stores the input sample x into\\nshift array[0], and moves the previous elements “up” through the shift array:\\nshift array[10] = shift array[9]\\nshift array[9] = shift'),\n",
       " Document(page_content='array, which works as a FIFO. It stores the input sample x into\\nshift array[0], and moves the previous elements “up” through the shift array:\\nshift array[10] = shift array[9]\\nshift array[9] = shift array[8]\\nshift array[8] = shift array[7]\\n···\\nshift array[2] = shift array[1]\\nshift array[1] = shift array[0]\\nshift array[0] = x\\nThe label Shift Accum Loop: is not necessary. However it can be useful for debugging. The\\nVivado(cid:13)R HLS tool adds these labels into the views of the code.\\nAfter the for loop completes, the acc variable has the complete result of the convolution of the\\ninput samples with the FIR coefficient array. The final result is written into the function argument\\ny which acts as the output port from this fir function. This completes the streaming process for\\ncomputing one output value of an FIR.\\nThisfunctiondoesnotprovideanefficientimplementationofaFIRfilter. Itislargelysequential,\\nand employs a significant amount of unnecessary control logic. The following sections'),\n",
       " Document(page_content='output value of an FIR.\\nThisfunctiondoesnotprovideanefficientimplementationofaFIRfilter. Itislargelysequential,\\nand employs a significant amount of unnecessary control logic. The following sections describe a\\nnumber of different optimizations that improve its performance.\\n2.4 Calculating Performance\\nBefore we get into the optimizations, it is necessary to define precise metrics. When deriving\\nthe performance of a design, it is important to carefully state the metric. For instance, there\\nare many different ways of specifying how “fast” your design runs. For example, you could say\\nthat it operates at X bits/second. Or that it can perform Y operations/sec. Other common\\nperformance metrics specifically for FIR filters talk about the number of filter operations/second.\\nYet another metric is multiply accumulate operations: MACs/second. Each of these are related\\nto one another, in some manner, but when comparing different implementations it is important to\\ncompareapplestoapples. Forexample,'),\n",
       " Document(page_content='accumulate operations: MACs/second. Each of these are related\\nto one another, in some manner, but when comparing different implementations it is important to\\ncompareapplestoapples. Forexample, directlycomparingonedesignusingbits/secondtoanother\\nusing filter operations/second can be misleading; fully understanding the relative benefits of the\\ndesigns requires that we compare them using the same metric. And this may require additional\\ninformation, e.g., going from filter operations/second to bits/second requires information about the\\nsize of the input and output data.\\nAll of the aforementioned metrics use seconds. High-level synthesis tools talk about the designs\\nintermsofnumberofcycles,andthefrequencyoftheclock. Thefrequencyisinverselyproportional\\nto the time it takes to complete one clock cycle. Using them both gives us the amount of time\\nin seconds to perform some operation. The number of cycles and the clock frequency are both\\n35FINITE IMPULSE RESPONSE (FIR) FILTERS\\nimportant: a'),\n",
       " Document(page_content='cycle. Using them both gives us the amount of time\\nin seconds to perform some operation. The number of cycles and the clock frequency are both\\n35FINITE IMPULSE RESPONSE (FIR) FILTERS\\nimportant: a design that takes one cycle, but with a very low frequency is not necessary better\\nthan another design that takes 10 clock cycles but operates at a much higher frequency.\\nThe clock frequency is a complicated function that the Vivado(cid:13)R HLS tool attempts to optimize\\nalongside the number of cycles. Note that it is possible to specify a target frequency to the\\nVivado(cid:13)R HLS tool. This is done using the create clock tcl command. For example, the command\\ncreate clock −period 5 directs the tool to target a clock period of 5 ns and equivalently a clock\\nfrequency of 200 MHz. Note that this is only a target clock frequency only and primarily affects\\nhow much operation chaining is performed by the tool. After generating RTL, the Vivado(cid:13)R HLS\\ntool provides an initial timing estimate'),\n",
       " Document(page_content='is only a target clock frequency only and primarily affects\\nhow much operation chaining is performed by the tool. After generating RTL, the Vivado(cid:13)R HLS\\ntool provides an initial timing estimate relative to this clock target. However, some uncertainty\\nin the performance of the circuit remains which is only resolved once the design is fully place and\\nrouted.\\nWhile achieving higher frequencies are often critical for reaching higher performance, increasing\\nthetargetclockfrequencyisnotnecessarilyoptimalintermsofanoverallsystem. Lowerfrequencies\\ngive more leeway for the tool to combine multiple dependent operations in a single cycle, a process\\ncalled operation chaining. This can sometimes allow higher performance by enabling improved\\nlogic synthesis optimizations and increasing the amount of code that can fit in a device. Improved\\noperationchainingcanalsoimprove(i.e., lower)theinitiationintervalofpipelineswithrecurrences.\\nIn general providing a constrained, but not over constrained'),\n",
       " Document(page_content='of code that can fit in a device. Improved\\noperationchainingcanalsoimprove(i.e., lower)theinitiationintervalofpipelineswithrecurrences.\\nIn general providing a constrained, but not over constrained target clock latency is a good option.\\nSomething in the range of 5−10 ns is typically a good starting option. Once you optimize your\\ndesign, you can vary the clock period and observe the results. We will describe operation chaining\\nin more detail in the next section.\\nBecause Vivado(cid:13)R HLS deals with clock frequency estimates, it does include some margin to\\naccount for the fact that there is some error in the estimate. The goal of this margin is to ensure\\nenough timing slack in the design that the generated RTL can be successfully placed and routed.\\nThis margin can be directly controlled using the set clock uncertainty TCL command. Note that\\nthis command only affects the HLS generated RTL and is different from the concept of clock\\nuncertainty in RTL-level timing constraints. Timing'),\n",
       " Document(page_content='using the set clock uncertainty TCL command. Note that\\nthis command only affects the HLS generated RTL and is different from the concept of clock\\nuncertainty in RTL-level timing constraints. Timing constraints generated by Vivado(cid:13)R HLS for\\nthe RTL implementation flow are solely based on the target clock period.\\nIt is also necessary to put the task that you are performing in context with the performance\\nmetric that you are calculating. In our example, each execution of the fir function results in one\\noutput sample. But we are performing N = 11 multiply accumulate operations for each execution\\nof fir. Therefore, if your metric of interest is MACs/second, you should calculate the task latency\\nfor fir in terms of seconds, and then divide this by 11 to get the time that it takes to perform the\\nequivalent of one MAC operation.\\nCalculating performance becomes even more complicated as we perform pipelining and other\\noptimizations. In this case, it is important to understand the'),\n",
       " Document(page_content='to perform the\\nequivalent of one MAC operation.\\nCalculating performance becomes even more complicated as we perform pipelining and other\\noptimizations. In this case, it is important to understand the difference between task interval and\\ntask latency. It is a good time to refresh your understanding of these two metrics of performance.\\nThis was discussed in Chapter 1.4. And we will continue to discuss how different optimizations\\neffect different performance metrics.\\n2.5 Operation Chaining\\nOperation chaining is an important optimization that the Vivado(cid:13)R HLS performs in order to\\noptimize the final design. It is not something that a designer has much control over, but it is\\nimportant that the designer understands how this works especially with respect to performance.\\nConsider the multiply accumulate operation that is done in a FIR filter tap. Assume that the add\\noperation takes 2 ns to complete, and a multiply operation takes 3 ns. If we set the clock period to 1\\n36FINITE IMPULSE'),\n",
       " Document(page_content='accumulate operation that is done in a FIR filter tap. Assume that the add\\noperation takes 2 ns to complete, and a multiply operation takes 3 ns. If we set the clock period to 1\\n36FINITE IMPULSE RESPONSE (FIR) FILTERS\\na) Cycle\\n1 2 3 4 5 6 Clock Period = 1 ns\\nNumber\\n+\\n*\\nb) Cycle\\n1 2 3 Clock Period = 2 ns\\nNumber\\n+\\n*\\nc) Cycle\\n1 Clock Period = 5 ns\\nNumber\\n+\\n*\\nFigure 2.2: The performance of multiply accumulate operation changes depending upon the target\\nclock period. Assume the multiply operation takes 3 ns and add operation takes 2 ns. Part a)\\nhas a clock period of 1 ns, and one MAC operation takes 5 cycles. Thus the performance is 200\\nmillion MACs/sec. Part b) has a clock period of 2 ns, and the MAC takes 3 cycles resulting in\\napproximately 167 million MACs/sec. Part c) has a clock period of 5 ns. By using operation\\nchaining, a MAC operation takes 1 cycle for a clock period of 200 million MACs/sec.\\nns (or equivalently a clock frequency of 1 GHz), then it would take 5 cycles for the MAC'),\n",
       " Document(page_content='of 5 ns. By using operation\\nchaining, a MAC operation takes 1 cycle for a clock period of 200 million MACs/sec.\\nns (or equivalently a clock frequency of 1 GHz), then it would take 5 cycles for the MAC operation\\nto complete. This is depicted in Figure 2.2 a). The multiply operation is executed over 3 cycles,\\nand the add operation is executed across 2 cycles. The total time for the MAC operation is 5 cycles\\n× 1 ns per cycle = 5 ns. Thus we can perform 1/5 ns = 200 million MACs/second.\\nIf we increase the clock period to 2 ns, the multiply operation now spans over two cycles, and\\nthe add operation must wait until cycle 3 to start. It can complete in one cycle. Thus the MAC\\noperation requires 3 cycles, so 6 ns total to complete. This allows us to perform approximately 167\\nmillion MACs/second. This result is lower than the previous result with a clock period of 1 ns.\\nThis can be explained by the “dead time” in cycle 2 where no operation is being performed.\\nHowever,'),\n",
       " Document(page_content='167\\nmillion MACs/second. This result is lower than the previous result with a clock period of 1 ns.\\nThis can be explained by the “dead time” in cycle 2 where no operation is being performed.\\nHowever, itisnotalwaystruethatincreasingtheclockperiodresultsinworseperformance. For\\nexample, if we set the clock period to 5 ns, we can perform both the multiply and add operation in\\nthe same cycle using operation chaining. This is shown in Figure 2.2 c). Thus the MAC operation\\ntakes 1 cycle where each cycle is 5 ns, so we can perform 200 million MACs/second. This is the\\nsame performance as Figure 2.2 a) where the clock period is faster (1 ns).\\nSo far we have performed chaining of only two operations in one cycle. It is possible to chain\\nmultiple operations in one cycle. For example, if the clock period is 10 ns, we could perform 5 add\\noperations in a sequential manner. Or we could do two sequential MAC operations.\\nIt should start to become apparent that the clock period plays an important role in'),\n",
       " Document(page_content='is 10 ns, we could perform 5 add\\noperations in a sequential manner. Or we could do two sequential MAC operations.\\nIt should start to become apparent that the clock period plays an important role in how the\\nVivado(cid:13)R HLS tool optimizes the design. This becomes even more complicated with all of the other\\noptimizations that the Vivado(cid:13)R HLS tool performs. It is not that important to fully understand\\nthe entire process of the Vivado(cid:13)R HLS tool. This is especially true since the tool is constantly\\nbeing improved with each new release. However, it is important to have a good idea about how the\\ntool may work. This will allow you to better comprehend the results, and even allow you to write\\nmore optimized code.\\nAlthough the Vivado(cid:13)R HLS can generate different different hardware for different target clock\\n37FINITE IMPULSE RESPONSE (FIR) FILTERS\\nShift Accum Loop:\\nfor (i = N − 1; i > 0; i−−) {\\nshift reg[i] = shift reg[i − 1];\\nacc += shift reg[i] ∗ c[i];\\n}\\nacc += x ∗'),\n",
       " Document(page_content='hardware for different target clock\\n37FINITE IMPULSE RESPONSE (FIR) FILTERS\\nShift Accum Loop:\\nfor (i = N − 1; i > 0; i−−) {\\nshift reg[i] = shift reg[i − 1];\\nacc += shift reg[i] ∗ c[i];\\n}\\nacc += x ∗ c[0];\\nshift reg[0] = x;\\nFigure 2.3: Removing the conditional statement from the for loop creates a more efficient hardware\\nimplementation.\\nperiods, overall performance optimization and determining the optimal target clock period still\\nrequires some creativity on the part of the user. For the most part, we advocate sticking within a\\nsmall subset of clock periods. For example, in the projects we suggest that you set the clock period\\nto 10 ns and focus on understanding how other optimizations, such as pipelining, can be used to\\ncreate different architectures. This 100 MHz clock frequency is relatively easy to achieve, yet it\\nis provides a good first order result. It is certainly possible to create designs that run at a faster\\nclock rate. 200 MHz and faster designs are possible but often require'),\n",
       " Document(page_content='easy to achieve, yet it\\nis provides a good first order result. It is certainly possible to create designs that run at a faster\\nclock rate. 200 MHz and faster designs are possible but often require more careful balance between\\nclock frequency targets and other optimization goals. You can change the target clock period and\\nobserve the differences in the performance. Unfortunately, there is no good rule to pick the optimal\\nfrequency.\\nVary the clock period for the base FIR architecture (Figure 2.1) from 10 ns to 1 ns in\\nincrements of 1 ns. Which clock period provides the best performance? Which gives the best\\narea? Why do you think this is the case? Do you see any trends?\\n2.6 Code Hoisting\\nTheif/elsestatementinsideoftheforloopisinefficient. Foreverycontrolstructureinthecode,the\\nVivado(cid:13)R HLS tool creates logical hardware that checks if the condition is met, which is executed\\nin every iteration of the loop. Furthermore, this conditional structure limits the execution of the\\nstatements'),\n",
       " Document(page_content='HLS tool creates logical hardware that checks if the condition is met, which is executed\\nin every iteration of the loop. Furthermore, this conditional structure limits the execution of the\\nstatements in either the if or else branches; these statements can only be executed after the if\\ncondition statement is resolved.\\nThe if statement checks when x == 0, which happens only on the last iteration. Therefore, the\\nstatements within the if branch can be “hoisted” out of the loop. That is we can execute these\\nstatements after the loop ends, and then remove the if/else control flow in the loop. Finally, we\\nmustchangetheloopboundsfromexecutingthe“0th”iteration. ThistransformisshowninFigure\\n2.3. This shows just the changes that are required to the for loop.\\nThe end results is a much more compact implementation that is ripe for further loop optimiza-\\ntions, e.g., unrolling and pipelining. We discuss those optimizations later.\\nCompare the implementations before and after the removal of the if/else'),\n",
       " Document(page_content='that is ripe for further loop optimiza-\\ntions, e.g., unrolling and pipelining. We discuss those optimizations later.\\nCompare the implementations before and after the removal of the if/else condition done\\nthrough loop hoisting. What is the difference in performance? How do the number of resources\\n38FINITE IMPULSE RESPONSE (FIR) FILTERS\\nTDL:\\nfor (i = N − 1; i > 0; i−−) {\\nshift reg[i] = shift reg[i − 1];\\n}\\nshift reg[0] = x;\\nacc = 0;\\nMAC:\\nfor (i = N − 1; i >= 0; i−−) {\\nacc += shift reg[i] ∗ c[i];\\n}\\nFigure 2.4: A code snippet corresponding to splitting the for loop into two separate loops.\\nchange?\\n2.7 Loop Fission\\nWearedoingtwofundamentaloperationswithintheforloop. Thefirstpartshiftsthedatathrough\\nthe shift reg array. The second part performs the multiply and accumulate operations in order to\\ncalculate the output sample. Loop fission takes these two operations and implements each of them\\nin their own loop. While it may not intuitively seem like a good idea, it allows us to'),\n",
       " Document(page_content='in order to\\ncalculate the output sample. Loop fission takes these two operations and implements each of them\\nin their own loop. While it may not intuitively seem like a good idea, it allows us to perform\\noptimizations separately on each loop. This can be advantageous especially in cases when the\\nresulting optimizations on the split loops are different.\\nThe code in Figure 2.4 shows the result of manual loop fission optimization. The code snippet\\nsplits the loop from Figure 2.3 into two loops. Note the label names for the two loops. The first\\nis TDL and the second is MAC. Tapped delay line (TDL) is a common DSP term for the FIFO\\noperation; MAC is short-hand for “multiply accumulate”.\\nCompare the implementations before and after loop fission. What is the difference in\\nperformance? How do the number of resources change?\\nLoop fission alone often does not provide a more efficient hardware implementation. However,\\nit allows each of the loops to be optimized independently, which could lead to'),\n",
       " Document(page_content='number of resources change?\\nLoop fission alone often does not provide a more efficient hardware implementation. However,\\nit allows each of the loops to be optimized independently, which could lead to better results than\\noptimizing the single, original for loop. The reverse is also true; merging two (or more) for loops\\ninto one for loop may yield the best results. This is highly dependent upon the application, which\\nis true for most optimizations. In general, there is not a single ‘rule of thumb’ for how to optimize\\nyour code. There are many tricks of the trade, and your mileage may vary. Thus, it is important\\nto have many tricks at your disposal, and even better, have a deep understanding of how the\\noptimizations work. Only then will you be able to create the best hardware implementation. Let\\nus continue to learn some additional tricks...\\n39FINITE IMPULSE RESPONSE (FIR) FILTERS\\nTDL:\\nfor (i = N − 1; i > 1; i = i − 2) {\\nshift reg[i] = shift reg[i − 1];\\nshift reg[i − 1] = shift reg[i −'),\n",
       " Document(page_content='Let\\nus continue to learn some additional tricks...\\n39FINITE IMPULSE RESPONSE (FIR) FILTERS\\nTDL:\\nfor (i = N − 1; i > 1; i = i − 2) {\\nshift reg[i] = shift reg[i − 1];\\nshift reg[i − 1] = shift reg[i − 2];\\n}\\nif (i == 1) {\\nshift reg[1] = shift reg[0];\\n}\\nshift reg[0] = x;\\nFigure 2.5: Manually unrolling the TDL loop in the fir11 function.\\n2.8 Loop Unrolling\\nBy default, the Vivado(cid:13)R HLS tool synthesizes for loops in a sequential manner. The tool creates\\na data path that implements one execution of the statements in the body of the loop. The data\\npath executes sequentially for each iteration of the loop. This creates an area efficient architecture;\\nhowever, it limits the ability to exploit parallelism that may be present across loop iterations.\\nLoop unrolling replicates the body of the loop by some number of times (called the factor).\\nAnd it reduces the number of iterations of the loop by the same factor. In the best case, when none\\nof the statements in the loop depend upon any of the'),\n",
       " Document(page_content='by some number of times (called the factor).\\nAnd it reduces the number of iterations of the loop by the same factor. In the best case, when none\\nof the statements in the loop depend upon any of the data generated in the previous iterations,\\nthis can substantially increase the available parallelism, and thus enables an architecture that runs\\nmuch faster.\\nThe first for loop (with the label TDL) in Figure 2.4 shifts the values up through the shift reg\\narray. The loop iterates from largest value (N−1) to the smallest value (i = 1). By unrolling this\\nloop, we can create a data path that executes a number of these shift operations in parallel.\\nFigure 2.5 shows the result of unrolling the loop by a factor of two. This code replicates the\\nloop body twice. Each iteration of the loop now performs two shift operations. Correspondingly,\\nwe must perform half of the number of iterations.\\nNote that there is an additional if condition after the for loop. This is required in the case when\\nthe loop does'),\n",
       " Document(page_content='shift operations. Correspondingly,\\nwe must perform half of the number of iterations.\\nNote that there is an additional if condition after the for loop. This is required in the case when\\nthe loop does not have an even number of iterations. In this case, we must perform the last “half”\\niteration by itself. The code in the if statement performs this last “half” iteration, i.e., moving the\\ndata from shift reg[0] into shift reg[1].\\nAlso note the effect of the loop unrolling on the for loop header. The decrement operation\\nchanges from i−− to i=i−2. This is due to the fact that we are doing two times the “work” in each\\niteration, thus we should decrement by 2 instead of 1.\\nFinally, the condition for terminating the for loop changes from i > 0 to i > 1. This is related\\nto the fact that we should make sure that the “last” iteration can fully complete without causing\\nan error. If the last iteration of the for loop executes when i = 1, then the second statement would\\ntry to read from shift'),\n",
       " Document(page_content='make sure that the “last” iteration can fully complete without causing\\nan error. If the last iteration of the for loop executes when i = 1, then the second statement would\\ntry to read from shift reg[−1]. Rather than perform this illegal operation, we do the final shift in\\nthe if statement after the for loop.\\nWrite the code corresponding to manually unrolling this TDL for loop by a factor of three.\\nHow does this change the loop body? What changes are necessary to the loop header? Is the\\nadditional code in the if statement after the for loop still necessary? If so, how is it different?\\n40FINITE IMPULSE RESPONSE (FIR) FILTERS\\nLoopunrollingcanincreasetheoverallperformanceprovidedthatwehavetheabilitytoexecute\\nsome (or all) of the statements in parallel. In the unrolled code, each iteration requires that we\\nread two values from the shift reg array; and we write two values to the same array. Thus, if we\\nwish to execute both statements in parallel, we must be able to perform two read'),\n",
       " Document(page_content='requires that we\\nread two values from the shift reg array; and we write two values to the same array. Thus, if we\\nwish to execute both statements in parallel, we must be able to perform two read operations and\\ntwo write operations from the shift reg array in the same cycle.\\nAssume that we store the shift reg array in one BRAM, and that BRAM has two read ports and\\none write port. Thus we can perform two read operations in one cycle. But we must sequentialize\\nthe write operations across two consecutive cycles.\\nThere are ways to execute these two statements in one cycle. For example, we could store all of\\nthevaluesoftheshift regarrayinseparateregisters. Itispossibletoreadandwritetoeachindivid-\\nual register on every cycle. In this case, we can perform both of the statements in this unrolled for\\nloop in one cycle. You can tell the Vivado(cid:13)R HLS tool to put all of the values in the shift reg array\\ninto registers using the directive #pragma HLS array partition variable=shift reg'),\n",
       " Document(page_content='for\\nloop in one cycle. You can tell the Vivado(cid:13)R HLS tool to put all of the values in the shift reg array\\ninto registers using the directive #pragma HLS array partition variable=shift reg complete. This is\\nan important optimization, thus we discuss the array partition directive in more detail later.\\nAusercantelltheVivado(cid:13)R HLStooltoautomaticallyunrolltheloopusingtheunrolldirective.\\nTo automatically perform the unrolling done manually in Figure 2.5, we should put the directive\\n#pragma HLS unroll factor=2 into the body of the code, right after the for loop header. While we\\ncan always manually perform loop unrolling, it is much easier to allow the tool to do it for us. It\\nmakes the code easier to read; and it will result in fewer coding errors.\\nUnrolltheTDL forloopautomaticallyusingtheunrolldirective. Asyouincreasetheunroll\\nfactor,howdoesthischangethenumberofresources(FFs,LUTs,BRAMs,DSP48s,etc.)? How\\ndoes it effect the throughput? What happens when you use the array'),\n",
       " Document(page_content='Asyouincreasetheunroll\\nfactor,howdoesthischangethenumberofresources(FFs,LUTs,BRAMs,DSP48s,etc.)? How\\ndoes it effect the throughput? What happens when you use the array partition directive in\\nconjunction with the unroll directive? What happens if you do not use the unroll directive?\\nNow, consider the second for loop (with the label MAC) in Figure 2.4. This loop multiplies a\\nvalue from the array c[] with a value from the array shift array[]. In each iteration it accesses the\\nith value from both arrays. And then it adds the result of that multiplication into the acc variable.\\nEachiterationofthisloopperformsonemultiplyandoneaddoperation. Eachiterationperforms\\none read operation from array shift reg[] and array c[]. The result of the multiplication of these two\\nvalues is accumulated into the variable acc.\\nThe load and multiplication operations are independent across all of the iterations of the for\\nloop. The addition operation, depending on how it is implemented, may depend upon the'),\n",
       " Document(page_content='the variable acc.\\nThe load and multiplication operations are independent across all of the iterations of the for\\nloop. The addition operation, depending on how it is implemented, may depend upon the values\\nof the previous iterations. However, it is possible to unroll this loop and remove this dependency.\\nFigure 2.6 shows the code corresponding to unrolling the MAC for loop by a factor of four. The\\nfirst for loop is the unrolled loop. The for loop header is modified in a similar manner to when we\\nunrolled the TDL loop. The bound is changed to i>=3, and i is decremented by a factor of 4 for\\neach iteration of the unrolled loop.\\nWhile there was loop carried dependency in the original, unrolled for, it is no longer present\\nin the unrolled loop. The loop carried dependency came due to the acc variable; since the result\\nof the multiply accumulate is written to this variable ever iteration, and we read from this register\\nin every iteration (to perform the running sum), it creates a'),\n",
       " Document(page_content='the acc variable; since the result\\nof the multiply accumulate is written to this variable ever iteration, and we read from this register\\nin every iteration (to perform the running sum), it creates a read-after-write (RAW) dependency\\nacross iterations. Note that there is not a dependency on the acc variable in the unrolled for loop\\ndue to the way this is written. Thus we are free to parallelize the four individual MAC operations\\nin the unrolled for loop.\\nThere is an additional for loop after the unrolled for loop. This is necessary to perform any\\npartialiterations. JustlikewerequiredtheifstatementintheTDL,thisperformsanycomputations\\n41FINITE IMPULSE RESPONSE (FIR) FILTERS\\nacc = 0;\\nMAC:\\nfor (i = N − 1; i >= 3; i −= 4) {\\nacc += shift reg[i] ∗ c[i] + shift reg[i − 1] ∗ c[i − 1] +\\nshift reg[i − 2] ∗ c[i − 2] + shift reg[i − 3] ∗ c[i − 3];\\n}\\nfor (; i >= 0; i−−) {\\nacc += shift reg[i] ∗ c[i];\\n}\\nFigure 2.6: Manually unrolling the MAC loop in the fir11 function by a factor of four.\\non a'),\n",
       " Document(page_content='reg[i − 2] ∗ c[i − 2] + shift reg[i − 3] ∗ c[i − 3];\\n}\\nfor (; i >= 0; i−−) {\\nacc += shift reg[i] ∗ c[i];\\n}\\nFigure 2.6: Manually unrolling the MAC loop in the fir11 function by a factor of four.\\non a potential last iteration. This occurs when the number of iterations in the original, unrolled\\nfor loop is not an even multiple of 4.\\nOnce again, we can tell the Vivado(cid:13)R HLS tool to automatically unroll the loop by a factor of\\n4 by inserting the code #pragma HLS unroll factor=4 into the MAC loop body.\\nBy specifying the optional argument skip exit check in that directive, the Vivado(cid:13)R HLS tool\\nwill not add the final for loop to check for partial iterations. This is useful in the case when you\\nknow that the loop will never require these final partial iterations. Or perhaps performing this\\nlast few iterations does not have an (major) effect on the results, and thus it can be skipped. By\\nusing this option, the Vivado(cid:13)R HLS tool does not have to create that additional for'),\n",
       " Document(page_content='this\\nlast few iterations does not have an (major) effect on the results, and thus it can be skipped. By\\nusing this option, the Vivado(cid:13)R HLS tool does not have to create that additional for loop. Thus\\nthe resulting hardware is simpler, and more area efficient.\\nThe for loop is completely unrolled when no factor argument is specified. This is equivalent to\\nunrolling by the maximum number of iterations; in this case a complete unrolling and unrolling\\nby a factor of 11 is equivalent. In both cases, the loop body is replicated 11 times. And the loop\\nheader is unnecessary; there is no need to keep a counter or check if the loop exit condition is met.\\nInordertoperformacompleteunrolling, theboundsoftheloopmustbestaticallydetermined, i.e.,\\nthe Vivado(cid:13)R HLS tool must be able to know the number of iterations for the for loop at compile\\ntime.\\nComplete loop unrolling exposes a maximal amount of parallelism at the cost of creating an\\nimplementation that requires a significant amount of'),\n",
       " Document(page_content='number of iterations for the for loop at compile\\ntime.\\nComplete loop unrolling exposes a maximal amount of parallelism at the cost of creating an\\nimplementation that requires a significant amount of resources. Thus, it ok to perform a complete\\nloopunrollon“smaller”forloops. Butcompletelyunrollingaloopwithalargenumberofiterations\\n(e.g., one that iterates a million times) is typically infeasible. Often times, the Vivado(cid:13)R HLS tool\\nwill run for a very long time (and many times fail to complete after hours of synthesis) if the\\nresulting loop unrolling creates code that is very large.\\nIf you design does not synthesize in under 15 minutes, you should carefully consider the\\neffect of your optimizations. It is certainly possible that large designs can take a significant\\namount for the Vivado(cid:13)R HLS tool to synthesize them. But as a beginning user, your designs\\nshould synthesize relatively quickly. If they take a long time, that most likely means that you\\nused some directives that'),\n",
       " Document(page_content='HLS tool to synthesize them. But as a beginning user, your designs\\nshould synthesize relatively quickly. If they take a long time, that most likely means that you\\nused some directives that significantly expanded the code, perhaps in a way that you did not\\nintend.\\n42FINITE IMPULSE RESPONSE (FIR) FILTERS\\nSynthesize a number of designs by varying the unroll factor for the MAC loop. How does\\nthe performance change? How does the unroll factor number affect the number of resources?\\nCompare these results with the trends that you found by unrolling the TDL.\\n2.9 Loop Pipelining\\nBy default, the Vivado(cid:13)R HLS tool synthesizes for loops in a sequential manner. For example, the\\nfor loop in Figure 2.1 will perform each iteration of the loop one after the other. That is, all of the\\nstatements in the second iteration happen only when all of the statements from the first iteration\\nare complete; the same is true for the subsequent iterations. This happens even in cases when it\\nis possible to'),\n",
       " Document(page_content='in the second iteration happen only when all of the statements from the first iteration\\nare complete; the same is true for the subsequent iterations. This happens even in cases when it\\nis possible to perform statements from the iterations in parallel. In other cases, it is possible to\\nstart some of the statements in a later iteration before all of the statements in a former iteration\\nare complete. This does not happen unless the designer specifically states that it should. This\\nmotivates the idea of loop pipelining, which allows for multiple iterations of the loop to execute\\nconcurrently.\\nConsider the MAC for loop from Figure 2.4. This performs one multiply accumulate (MAC)\\noperation per iteration. This MAC for loop has four operations in the loop body:\\n• Read c[]: Load the specified data from the C array.\\n• Read shift reg[]: Load the specified data from the shift reg array.\\n• ∗: Multiply the values from the arrays c[] and shift reg[].\\n• +: Accumulate this multiplied result into the'),\n",
       " Document(page_content='from the C array.\\n• Read shift reg[]: Load the specified data from the shift reg array.\\n• ∗: Multiply the values from the arrays c[] and shift reg[].\\n• +: Accumulate this multiplied result into the acc variable.\\nA schedule corresponding to one iteration of the MAC for loop is shown in Figure 2.7 a). The\\nRead operations each require 2 cycles. This is due to the fact that the first cycle provides the\\naddress to the memory, and the data from the memory is delivered during the second cycle. These\\ntwo Read operations can be done in parallel since there are no dependencies between them. The ∗\\noperation can begin in Cycle 2; assume that it takes three cycles to complete, i.e., it is finished is\\nCycle 4. The + operation is chained to start and complete during Cycle 4. The entire body of the\\nMAC for loop takes 4 cycles to complete.\\nThere are a number of performance metrics associated with a for loop. The iteration latency is\\nthe number of cycles that it takes to perform one iteration of the'),\n",
       " Document(page_content='for loop takes 4 cycles to complete.\\nThere are a number of performance metrics associated with a for loop. The iteration latency is\\nthe number of cycles that it takes to perform one iteration of the loop body. The iteration latency\\nfor this MAC for loop is 4 cycles. The for loop latency is the number of cycles required to complete\\nthe entire execution of the loop. This includes time to calculate the initialization statement (e.g.,\\ni = 0), the condition statement (e.g., i >= 0), and the increment statement (e.g., i−−). Assuming\\nthat these three header statements can be done in parallel with the loop body execution, the\\nVivado(cid:13)R HLS tool reports the latency of this MAC for loop as 44 cycles. This is the number of\\niterations (11) multiplied by the iteration latency (4 cycles) plus one additional cycle to determine\\nthat the loop should stop iterating. And then you subtract one. Perhaps the only strange thing\\nhere is the “subtract 1”. We will get to that in a second. But first, there'),\n",
       " Document(page_content='additional cycle to determine\\nthat the loop should stop iterating. And then you subtract one. Perhaps the only strange thing\\nhere is the “subtract 1”. We will get to that in a second. But first, there is one additional cycle\\nthat is required at the beginning of the next iteration, which checks if the condition statement is\\nsatisfied (it is not) and then exits the loop. Now the “subtract 1”: Vivado(cid:13)R HLS determines the\\nlatency as the cycle in which the output data is ready. In this case, the final data is ready during\\nCycle 43. This would be written into a register at the end of Cycle 43 and correspondingly the\\n43FINITE IMPULSE RESPONSE (FIR) FILTERS\\na) Cycle\\n1 2 3 4 5\\nNumber\\nRead\\nc[]\\n+\\n*\\nRead\\nshift_reg[]\\nb) Cycle\\n1 2 3 4 5 6\\nNumber\\nRead\\nc[]\\n+\\nIteration 1 *\\nRead\\nshift_reg[]\\nRead\\nc[]\\n+\\nIteration 2 *\\nRead\\nshift_reg[]\\nRead\\nc[]\\nIteration 3 * +\\nRead\\nshift_reg[]\\nFigure 2.7: Part a) shows a schedule for the body of the MAC for loop. Part b) shows the schedule\\nfor three iterations of a'),\n",
       " Document(page_content='2 *\\nRead\\nshift_reg[]\\nRead\\nc[]\\nIteration 3 * +\\nRead\\nshift_reg[]\\nFigure 2.7: Part a) shows a schedule for the body of the MAC for loop. Part b) shows the schedule\\nfor three iterations of a pipelined version of the MAC for loop.\\n44FINITE IMPULSE RESPONSE (FIR) FILTERS\\nbeginning of Cycle 44. Another way to think of this is that the latency is equal to the maximum\\nnumber of registers between the input data and the output data.\\nLoop pipelining is an optimization that overlaps multiple iterations of a for loop. Figure 2.7\\nb) provides an example of pipelining for the MAC for loop. The figure shows three iterations of\\nthe for which are executed simultaneously. The first iteration is equivalent the the non-pipelined\\nversion as depicted in Figure 2.7 a). The difference is the start times of the subsequent iterations.\\nIn the non-pipelined version, the second iteration begins after the first iteration is completed, i.e.,\\nin Cycle 5. However, the pipelined version can start the subsequent iteration'),\n",
       " Document(page_content='iterations.\\nIn the non-pipelined version, the second iteration begins after the first iteration is completed, i.e.,\\nin Cycle 5. However, the pipelined version can start the subsequent iteration before the previous\\niterations complete. In the figure, Iteration 2 starts at Cycle 2, and Iteration 3 starts at Cycle\\n3. The remaining iterations start every consecutive cycle. Thus, the final iteration, Iteration 11,\\nwould start at Cycle 11 and it would complete during Cycle 14. Thus, the loop latency is 14.\\nThe loop initiation interval (II) is another important performance metric. It is defined as the\\nnumber of clock cycles until the next iteration of the loop can start. In our example, the loop II is\\n1, which means that we start a new iteration of the loop every cycle. This is graphically depicted\\nin Figure 2.7 b). The II can be explicitly set using the directive. For example, the directive\\n#pragma HLS pipeline II=2 informs the Vivado(cid:13)R HLS tool to attempt to set the II=2. Note'),\n",
       " Document(page_content='depicted\\nin Figure 2.7 b). The II can be explicitly set using the directive. For example, the directive\\n#pragma HLS pipeline II=2 informs the Vivado(cid:13)R HLS tool to attempt to set the II=2. Note that\\nthis may not always be possible due to resource constraints and/or dependencies in the code. The\\noutput reports will tell you exact what the Vivado(cid:13)R HLS tool was able to achieve.\\nExplicitly set the loop initiation interval starting at 1 and increasing in increments of 1\\ncycle. How does increasing the II effect the loop latency? What are the trends? At some point\\nsetting the II to a larger value does not make sense. What is that value in this example? How\\ndo you describe that value for a general for loop?\\nAny for loop can be pipelined, so let us now consider the TDL for loop. This for loop as a\\nsimilar header to the MAC for loop. The body of the loop performs an element by element shift of\\ndata through the array as described in Section 2.3. There are two operations: one Read'),\n",
       " Document(page_content='for loop as a\\nsimilar header to the MAC for loop. The body of the loop performs an element by element shift of\\ndata through the array as described in Section 2.3. There are two operations: one Read and one\\nWrite to the shift reg array. The iteration latency of this loop is 2 cycles. The Read operation takes\\ntwo cycles, and the Write operation is performed at the end of Cycle 2. The for loop latency for\\nthis non-pipelined loop is 20 cycles.\\nWe can pipeline this loop by inserting the directive #pragma HLS pipeline II=1 after the loop\\nheader. The result of the synthesis is a loop initiation interval equal to 1 cycle. This means that\\nwe can start a loop iteration every cycle.\\nBy modifying the example slightly, we can demonstrate a scenario where the resource con-\\nstraints do not allow the Vivado(cid:13)R HLS tool to achieve an II=1. To do this, we explicitly set\\nthe type of memory for the shift reg array. By not specifying the resource, we leave it up to the\\nVivado(cid:13)R HLS tool to'),\n",
       " Document(page_content='Vivado(cid:13)R HLS tool to achieve an II=1. To do this, we explicitly set\\nthe type of memory for the shift reg array. By not specifying the resource, we leave it up to the\\nVivado(cid:13)R HLS tool to decide. But we can specify the memory using a directive, e.g., the direc-\\ntive #pragma HLS resource variable=shift reg core=RAM 1P forces the Vivado(cid:13)R HLS tool to use\\na single port RAM. When using this directive in conjunction with the loop pipelining objective,\\nthe Vivado(cid:13)R HLS tool will fail to pipeline this loop with an II=1. This is due to the fact that a\\npipelined version of this code requires both a Read and Write operation in the same cycle. This is\\nnot possible using a single port RAM. This is evident in Figure 2.8 b). Looking at Cycle 2, we re-\\nquire a Write operation to the array shift reg in Iteration 1, and a Read operation to the same array\\nin Iteration 2. We can modify the directive to allow HLS more scheduling freedom by removing the\\nexplicit request for II=1,'),\n",
       " Document(page_content='the array shift reg in Iteration 1, and a Read operation to the same array\\nin Iteration 2. We can modify the directive to allow HLS more scheduling freedom by removing the\\nexplicit request for II=1, e.g. #pragma HLS pipeline. In this case, HLS will automatically increase\\nthe initiation interval until it can find a feasible schedule.\\n45FINITE IMPULSE RESPONSE (FIR) FILTERS\\na) Cycle\\n1 2 3 4\\nNumber\\nRead Write Read Write\\nshift_reg[] shift_reg[] shift_reg[] shift_reg[]\\nb) Cycle\\n1 2 3 4\\nNumber\\nRead Write\\nIteration 1\\nshift_reg[] shift_reg[]\\nRead Write\\nIteration 2\\nshift_reg[] shift_reg[]\\nRead Write\\nIteration 3\\nshift_reg[] shift_reg[]\\nFigure 2.8: Part a) shows a schedule for two iterations of the body of the TDL for loop. Part b)\\nshows the schedule for three iterations of a pipelined version of the TDL for loop with II=1.\\n46FINITE IMPULSE RESPONSE (FIR) FILTERS\\nThe RESOURCE directive allows the user to force the Vivado(cid:13)R HLS tool to map\\nan operation to a hardware core. This can be done'),\n",
       " Document(page_content='TDL for loop with II=1.\\n46FINITE IMPULSE RESPONSE (FIR) FILTERS\\nThe RESOURCE directive allows the user to force the Vivado(cid:13)R HLS tool to map\\nan operation to a hardware core. This can be done on arrays (as shown above) and\\nalso for variables. Consider the code a = b + c;. We can use the RESOURCE directive\\n#pragma HLS RESOURCE variable=a core=AddSub DSP to tell the Vivado(cid:13)R HLS tool that\\nthe add operation is implemented using a DSP48. There are a wide variety of cores described\\nin the Vivado(cid:13)R HLS documentation [63]. In general, it is advised to let the Vivado(cid:13)R HLS\\ndecide the resources. If these are not satisfactory, then the designer can use directives.\\n2.10 Bitwidth Optimization\\nTheClanguageprovidesmanydifferentdatatypestodescribedifferentkindsofbehavior. Upuntil\\nthis point, we have focused on the int type, which Vivado(cid:13)R HLS treats as a 32-bit signed integer.\\nThe C language also provides floating point data types, such as float and double, and'),\n",
       " Document(page_content='Upuntil\\nthis point, we have focused on the int type, which Vivado(cid:13)R HLS treats as a 32-bit signed integer.\\nThe C language also provides floating point data types, such as float and double, and integer data\\ntypes, such as char, short, long, and long long. Integer datatypes may be unsigned. All of these\\ndata types have a size which is a power of 2.\\nThe actual number of bits for these C data types may vary depending upon the processor\\narchitecture. For example, an int can be 16 bits on a micro controller and 32 bits on a general\\npurpose processor. The C standard dictates minimum bit widths (e.g., an int is at least 16 bits)\\nand relations between the types (e.g., a long is not smaller than an int which is not smaller than\\na short). The C99 language standard eliminated this ambiguity with types such as int8 t, int16 t,\\nint32 t, and int64 t.\\nThe primary benefits of using these different data types in software revolve around the amount\\nof storage that the data type requires. For large'),\n",
       " Document(page_content='types such as int8 t, int16 t,\\nint32 t, and int64 t.\\nThe primary benefits of using these different data types in software revolve around the amount\\nof storage that the data type requires. For large arrays, using 8 bit values instead of 16 bit values\\ncan cut memory usage in half. The drawback is that the range of values that you can represent\\nis reduced. An 8 bit signed value allows numbers in the range [-128,127] while the 16 bit signed\\ndata type has a range of [-32,768, 32,767]. Smaller operations may also require fewer clock cycles\\nto execute, or may allow more instructions to be executed in parallel.\\nThe same benefits are seen in an FPGA implementation, but they are even more pronounced.\\nSincetheVivado(cid:13)R HLStoolgeneratesacustomdatapath,itwillcreateanimplementationmatched\\nto the specified data types. For example, the statement a = b ∗ c will have different latency and\\nresource usage depending upon the data type. If all of the variables are 32 bits wide, then more\\nprimitive'),\n",
       " Document(page_content='specified data types. For example, the statement a = b ∗ c will have different latency and\\nresource usage depending upon the data type. If all of the variables are 32 bits wide, then more\\nprimitive boolean operations need to be performed than if the variables are only 8 bits wide. As\\na result, more FPGA resources (or more complex resources) must be used. Additionally, more\\ncomplex logic typically requires more pipelining in order to achieve the same frequency. A 32-\\nbit multiplication might require 5 internal registers to meet the same frequency that an 8 bit\\nmultiplication can achieve with only one internal register. As a result, the latency of the operation\\nwill be larger (5 cycles instead of 1 cycle) and HLS must take this into account.\\nCreate a simple design that implements the code a = b ∗ c. Change the data type of the\\nvariablestochar, short, int, long, andlong long. Howmanycyclesdoesthemultiplyoperation\\ntake in each case? How many resources are used for the different data'),\n",
       " Document(page_content='code a = b ∗ c. Change the data type of the\\nvariablestochar, short, int, long, andlong long. Howmanycyclesdoesthemultiplyoperation\\ntake in each case? How many resources are used for the different data types?\\n47FINITE IMPULSE RESPONSE (FIR) FILTERS\\nWhat primitive boolean operations are needed to implement the multiplication of 8-bit\\nnumbers? How does this change when implementing multiplication of 32-bit numbers? Hint:\\nHow many primitive decimal operations are needed to implement multiplication of two 8 digit\\ndecimal numbers?\\nIn many cases to implement optimized hardware, it is necessary to process data where the\\nbitwidth is not a power of two. For example, analog to digital converters often output results in\\n10 bits, 12 bits, or 14 bits. We could map these to 16 bit values, but this would likely reduce\\nperformance and increase resource usage. To more accurately describe such values, Vivado(cid:13)R HLS\\nprovides arbitrary precision data types which allow for signed and unsigned data'),\n",
       " Document(page_content='likely reduce\\nperformance and increase resource usage. To more accurately describe such values, Vivado(cid:13)R HLS\\nprovides arbitrary precision data types which allow for signed and unsigned data types of any\\nbitwidth.\\nThere are two separate classes for unsigned and signed data types:\\n• Unsigned: ap uint<width>\\n• Signed: ap int<width>\\nwhere the width variable is an integer between 1 and 10241. For example, ap int<8> is an 8 bit\\nsigned value (same as char), and ap uint<32> is a 32 bit unsigned value (same as unsigned int).\\nThis provides a more powerful data type since it can do any bitwidth, e.g., ap uint<4> or\\nap int<537>. To use these data types you must use C++ and include the file ap int.h, i.e., add\\nthe code #include ”ap int.h” in your project and use a filename ending in ‘.cpp’2.\\nConsider coefficients array c[] from the fir filter code in Figure 2.1. It is reprinted here for you\\nconvenience: coef t c[N] = {53, 0, −91, 0, 313, 500, 313, 0, −91, 0, 53};. The data type coef t'),\n",
       " Document(page_content='coefficients array c[] from the fir filter code in Figure 2.1. It is reprinted here for you\\nconvenience: coef t c[N] = {53, 0, −91, 0, 313, 500, 313, 0, −91, 0, 53};. The data type coef t is\\ndefined as an int meaning that we have 32 bits of precision. This is unnecessary for these constants\\nsince they range from -91 to 500. Thus we could use a smaller data type for this. We will need a\\nsigned data type since we have positive and negative values. And the maximum absolute value for\\nany of these 11 entries is 500, which requires (cid:100)log 500(cid:101) = 9 bits. Since we need negative numbers,\\n2\\nwe add an additional bit. Thus coef t can be declared as ap int<10>.\\nWhat is the appropriate data type for the variable i in the fir function (see Figure 2.1)?\\nWe can also more accurately define the data types for the other variables in the fir function,\\ne.g., acc and shift reg. Consider the shift reg array first. This is storing the last 11 values of the\\ninput variable x. So we know that the'),\n",
       " Document(page_content='the data types for the other variables in the fir function,\\ne.g., acc and shift reg. Consider the shift reg array first. This is storing the last 11 values of the\\ninput variable x. So we know that the shift reg values can safely have the same data type as x.\\nBy “safe”, we mean that there will be no loss in precision, i.e., if shift reg had a data type with\\na smaller bitwidth, then some significant bits of x would need to be eliminated to fit them into a\\nvalue in shift reg. For example, if x was defined as 16 bits (ap uint<16>) and shift reg was defined\\nas 12 bits (ap uint<12>), then we would cut off the 4 most significant bits of x when we stored it\\ninto shift reg.\\nDefining the appropriate data type for acc is a more difficult task. The acc variable stores the\\nmultiply and accumulated sum over the shift reg and the coefficient array c[] i.e., the output value\\nof the filter. If we wish to be safe, then we calculate the largest possible value that could be stored\\nin acc, and set the'),\n",
       " Document(page_content='sum over the shift reg and the coefficient array c[] i.e., the output value\\nof the filter. If we wish to be safe, then we calculate the largest possible value that could be stored\\nin acc, and set the bitwidth as that.\\n11024 is the default maximum value, and this can be changed if needed. See the Vivado(cid:13)R HLS user manuals for\\nmore information on how to do this.\\n2Similar capabilities are also available in C using the file ap cint.h\\n48FINITE IMPULSE RESPONSE (FIR) FILTERS\\nTo accomplish this, we must understand how the bitwidth increases as we perform arithmetic\\noperations. Consider the operation a = b + c where ap uint<10> b and ap uint<10> c. What is\\nthe data type for the variable a? We can perform a worst case analysis here, and assume both a\\nand b are the largest possible value 210 = 1024. Adding them together results in a = 2024 which\\ncan be represented as an 11 bit unsigned number, i.e., ap uint<11>. In general we will need one\\nmore bit than the largest bitwidth of the two'),\n",
       " Document(page_content='210 = 1024. Adding them together results in a = 2024 which\\ncan be represented as an 11 bit unsigned number, i.e., ap uint<11>. In general we will need one\\nmore bit than the largest bitwidth of the two number being added. That is, when ap uint<x> b\\nand ap uint<y> c, the data type for a is ap uint<z> where z = max(x,y)+1. This same is also\\ntrue when adding signed integers.\\nThat handles one part of the question for assigning the appropriate data type to acc, but we\\nmust also deal with the multiply operation. Using the same terminology, we wish to determine the\\nvalue the bitwidth z given the bitwidths x and y (i.e., ap int<z> a, ap int<x> b, ap int<y> c) for\\nthe operation a = b ∗ c. While we will not go into the details, the formula is z = x+y.\\nGiven these two formulas, determine the bitwidth of acc such that it is safe.\\nUltimately we are storing acc into the variable y which is an output port of the function.\\nTherefore, if the bitwidth of acc is larger than the bitwidth of c, the value in'),\n",
       " Document(page_content='of acc such that it is safe.\\nUltimately we are storing acc into the variable y which is an output port of the function.\\nTherefore, if the bitwidth of acc is larger than the bitwidth of c, the value in acc will be truncated\\nto be stored into y. Thus, is it even important to insure that the bitwidth of acc is large enough so\\nthat it can handle the full precision of the multiply and accumulate operations?\\nThe answer to the question lies in the tolerance of the application. In many digital signal pro-\\ncessing applications, the data itself is noisy, meaning that the lower several bits may not have any\\nsignificance. In addition, in signal processing applications, we often perform numerical approxima-\\ntions when processing data which can introduce additional error. Thus, it may not be important\\nthat we insure that the acc variable has enough precision to return a completely precise result.\\nOn the other hand, it may be desirable to keep more bits in the accumulation and then round the\\nfinal'),\n",
       " Document(page_content='we insure that the acc variable has enough precision to return a completely precise result.\\nOn the other hand, it may be desirable to keep more bits in the accumulation and then round the\\nfinal answer again to reduce the overall rounding error in the computation. Other applications,\\nsuch as scientific computing, more dynamic range is often required, which may lead to the use of\\nfloating point numbers instead of integer or fixed-point arithmetic. So what is the correct answer?\\nUltimately it is up to the tolerance of the application designer.\\n2.11 Complex FIR Filter\\nTo this point, we have solely looked at filtering real numbers. Many digital wireless communication\\nsystemsdealwithcomplexnumbersusingin-phase(I)andquadrature(Q)components(seeChapter\\n?? for more details). Fortunately, it is possible to create a complex FIR filter using real FIR filter\\nas we describe in the following.\\nTo understand how to build a complex FIR filter from real FIR filters consider Equation 2.4.\\nAssume that (I ,Q'),\n",
       " Document(page_content='to create a complex FIR filter using real FIR filter\\nas we describe in the following.\\nTo understand how to build a complex FIR filter from real FIR filters consider Equation 2.4.\\nAssume that (I ,Q ) is one sample of the input data that we wish to filter. And one of the\\nin in\\ncomplex FIR filter coefficients is denoted as (I ,Q ). There will be more than one input sample\\nfir fir\\nand complex coefficient, but let us not worry about that for now.\\n(I +jQ )(I +jQ ) = (I I −Q Q )+j(Q I +I Q ) (2.4)\\nin in fir fir in fir in fir in fir in fir\\nEquation 2.4 shows the multiplication of the input complex number by one coefficient of the\\ncomplex FIR filter. The right side of the equation shows that the real portion of the output of\\ncomplex input filtered by a complex FIR filter is I I −Q Q and the imaginary output is\\nin fir in fir\\n49FINITE IMPULSE RESPONSE (FIR) FILTERS\\nComplex FIR Filter\\nI I\\nin fir\\nI FIR I1\\nin\\n- I\\nout\\nQ FIR Q1\\nin Q Q\\nin fir\\nQ I\\nin fir\\nFIR I2\\n+ Q\\nout\\nFIR Q2\\nI Q\\nin fir\\nFigure 2.9: A'),\n",
       " Document(page_content='output is\\nin fir in fir\\n49FINITE IMPULSE RESPONSE (FIR) FILTERS\\nComplex FIR Filter\\nI I\\nin fir\\nI FIR I1\\nin\\n- I\\nout\\nQ FIR Q1\\nin Q Q\\nin fir\\nQ I\\nin fir\\nFIR I2\\n+ Q\\nout\\nFIR Q2\\nI Q\\nin fir\\nFigure 2.9: A complex FIR filter built from four real FIR filters. The input I and Q samples are\\nfeed into four different real FIR filters. The FIR filters hold the in-phase (FIR I) and quadrature\\n(FIR Q) complex coefficients.\\n50FINITE IMPULSE RESPONSE (FIR) FILTERS\\ntypedef int data t;\\nvoid firI1(data t ∗y, data t x);\\nvoid firQ1(data t ∗y, data t x);\\nvoid firI2(data t ∗y, data t x);\\nvoid firQ2(data t ∗y, data t x);\\nvoid complexFIR(data t Iin, data t Qin, data t ∗Iout, data t ∗Qout) {\\ndata t IinIfir, QinQfir, QinIfir, IinQfir;\\nfirI1(&IinIfir, Iin);\\nfirQ1(&QinQfir, Qin);\\nfirI2(&QinIfir, Qin);\\nfirQ2(&IinQfir, Iin);\\n∗Iout = IinIfir + QinQfir;\\n∗Qout = QinIfir − IinQfir;\\n}\\nFigure 2.10: The Vivado(cid:13)R HLS code to hierarchically implement a complex FIR filter using four\\nreal FIR filters.\\nQ I +I Q . This implies'),\n",
       " Document(page_content='= IinIfir + QinQfir;\\n∗Qout = QinIfir − IinQfir;\\n}\\nFigure 2.10: The Vivado(cid:13)R HLS code to hierarchically implement a complex FIR filter using four\\nreal FIR filters.\\nQ I +I Q . This implies that we can separate the complex FIR filter operation into four real\\nin fir in fir\\nfilters as shown in Figure 2.9.\\nA complex FIR filter takes as input a complex number (I ,Q ) and outputs a complex filtered\\nin in\\nvalue (I ,Q ). Figure 2.9 provides a block diagram of this complex filter using four real FIR\\nout out\\nfilters (FIR I1, FIR Q1, FIR I2, FIR Q2). The filters FIR I1 and FIR I2 are equivalent, i.e., they\\nhave the exact same coefficients. FIR Q1 and FIR Q2 are also equivalent. The output of each of\\nthese filters corresponds to a term from Equation 2.4. These output are then added or subtracted\\nto provide the final filtered complex output (I ,Q ).\\nout out\\nWe used a hierarchical structure to define this complex FIR filter. Vivado(cid:13)R HLS implements\\nhierarchy using functions. Taking the'),\n",
       " Document(page_content='provide the final filtered complex output (I ,Q ).\\nout out\\nWe used a hierarchical structure to define this complex FIR filter. Vivado(cid:13)R HLS implements\\nhierarchy using functions. Taking the previous real FIR function void fir (data t ∗y, data t x) we\\ncancreateanotherfunctionthatencapsulatesfourversionsofthisfirfunctiontocreatethecomplex\\nFIR filter. This code is shown in Figure 2.10.\\nThecodedefinesfourfunctionsfirI1, firQ1, firI2, andfirQ2. Eachofthesefunctionshastheexact\\nsame code, e.g., that of the fir function from Figure 2.1. Typically, we would not need to replicate\\nthe function; we could simply call the same function four times. However, this is not possible in\\nthis case due to the static keyword used within the fir function for the shift reg.\\nThe function calls act as interfaces. The Vivado(cid:13)R HLS tool does not optimize across function\\nboundaries. That is, each fir function synthesized independently, and treated more or less as a\\nblack box in the complexFIR function.'),\n",
       " Document(page_content='The Vivado(cid:13)R HLS tool does not optimize across function\\nboundaries. That is, each fir function synthesized independently, and treated more or less as a\\nblack box in the complexFIR function. You can use the inline directive if you want the Vivado(cid:13)R\\nHLS tool to co-optimize a particular function within its parent function. This will add the code\\nfrom that function into the parent function and eliminate the hierarchical structure. While this can\\nincrease the potential for benefits in performance and area, it also creates a large amount of code\\nthat the tool must synthesize. That may take a long time, even fail to synthesize, or may result\\nin a non-optimal design. Therefore, use the inline directive carefully. Also note that the Vivado(cid:13)R\\n51FINITE IMPULSE RESPONSE (FIR) FILTERS\\nfloat mul(int x, int y) {\\nreturn x ∗ y;\\n}\\nfloat top function(float a, float b, float c, float d) {\\nreturn mul(a, b) + mul(c, d) + mul(b, c) + mul(a, d);\\n}\\nfloat inlined top function(float a, float'),\n",
       " Document(page_content='mul(int x, int y) {\\nreturn x ∗ y;\\n}\\nfloat top function(float a, float b, float c, float d) {\\nreturn mul(a, b) + mul(c, d) + mul(b, c) + mul(a, d);\\n}\\nfloat inlined top function(float a, float b, float c, float d) {\\nreturn a ∗ b + c ∗ d + b ∗ c + a ∗ d;\\n}\\nFigure 2.11: A simple and trivial example to demonstrate the inline directive. The top function has\\nfour function calls to the function mul. If we placed an inline directive on the mul function, the\\nresult is similar to what you see in the function inlined top function.\\nHLS tool may choose to inline functions on its own. These are typically functions with a small\\namount of code.\\nTheinlinedirectiveremovesfunctionboundaries,whichmayenableadditionalopportunities\\nfor the Vivado(cid:13)R HLS tool at the cost of increasing the complexity of the synthesis problem,\\ni.e., it will likely make the synthesis time longer. It also eliminates any overhead associated\\nwith performing the function call. It allows for different implementations while'),\n",
       " Document(page_content='the synthesis problem,\\ni.e., it will likely make the synthesis time longer. It also eliminates any overhead associated\\nwith performing the function call. It allows for different implementations while maintaining\\nthe structure of the code, and making it hierarchical and more readable.\\nThecodeinFigure2.11providesanexampleofhowtheinlinedirectiveworks. Thefunction\\ninlined top function is the result of using the inline directive on the mul function.\\nThe Vivado(cid:13)R HLS tool will sometimes choose to inline functions automatically. For exam-\\nple, it will very likely choose to inline the mul function from Figure 2.11 since it is small. You\\ncan force the tool to keep the function hierarchy by placing an inline directive in the function\\nwith the off argument.\\nThe inline directive also has a recursive argument that inlines all functions called within the\\ninlined function to also be inlined. That is, it will recursively add the code into the parent\\nfunctions from every child function. This'),\n",
       " Document(page_content='recursive argument that inlines all functions called within the\\ninlined function to also be inlined. That is, it will recursively add the code into the parent\\nfunctions from every child function. This could create a substantial code base, so use this\\nfunction carefully.\\nAn inlined function will not have separate entries in the report since all of the logic will be\\nassociated with the parent function.\\n2.12 Conclusion\\nThis chapter describes the specification and optimization of a FIR filter using the Vivado(cid:13)R HLS\\ntool. The goal is to provide an overview of the HLS process. The first step in this process is\\nunderstanding the basic concepts behind the computation of the FIR filter. This does not require\\na deep mathematical understanding, but certainly enough knowledge to write it in a manner that\\nis synthesizable by the Vivado(cid:13)R HLS tool. This may require translating the code from a different\\nlanguage (e.g., MATLAB, Java, C++, Python, etc.). Many times it requires rewriting'),\n",
       " Document(page_content='manner that\\nis synthesizable by the Vivado(cid:13)R HLS tool. This may require translating the code from a different\\nlanguage (e.g., MATLAB, Java, C++, Python, etc.). Many times it requires rewriting to use\\n52FINITE IMPULSE RESPONSE (FIR) FILTERS\\nsimpler data structures, e.g., one that is explicitly implemented in an array. And it often involves\\nremoving system calls and other code not supported by the HLS tool.\\nCreating an optimum architecture requires a basic understanding about how the HLS tool\\nperforms its synthesis and optimization process to RTL code. It is certainly not unnecessary to\\nunderstand the exact HLS algorithms for schedule, binding, resource allocation, etc. (and many\\ntimes these are proprietary). But having a general idea of the process does aid the designer in\\nwriting code that maps well to hardware. Throughout the chapter, we talked about some of\\nthe key features of the HLS synthesis process that are necessary to understand when performing\\nvarious optimizations. It'),\n",
       " Document(page_content='that maps well to hardware. Throughout the chapter, we talked about some of\\nthe key features of the HLS synthesis process that are necessary to understand when performing\\nvarious optimizations. It is especially important to understand the way that the HLS tool reports\\nperformance, which we describe in Chapter 2.4.\\nAdditionally, we presented some basic HLS optimizations (including loop and bitwidth op-\\ntimizations). We highlighted their benefits and potential drawbacks using the FIR filter as an\\nexample. These are common optimizations that can be applied across a wide range of applications.\\nWe provide more details about these optimizations in subsequent chapters as we walk through the\\nimplementation of other more complex applications.\\nFinally, there is an entire project devoted to further optimizing the FIR filter. This is located\\nin the Appendix in Chapter ??.\\n53FINITE IMPULSE RESPONSE (FIR) FILTERS\\n54Chapter 3\\nCORDIC\\n3.1 Overview\\nCORDIC (Coordinate Rotation DIgital Computer) is an'),\n",
       " Document(page_content='optimizing the FIR filter. This is located\\nin the Appendix in Chapter ??.\\n53FINITE IMPULSE RESPONSE (FIR) FILTERS\\n54Chapter 3\\nCORDIC\\n3.1 Overview\\nCORDIC (Coordinate Rotation DIgital Computer) is an efficient technique to calculate trigono-\\nmetric, hyperbolic, andothermathematicalfunctions. Itisadigit-by-digitalgorithmthatproduces\\none output digit per iteration. This allows us to tune the accuracy of the algorithm to the applica-\\ntion requirements; additional iterations produce a more precise output result. Accuracy is another\\ncommon design evaluation metric alongside performance and resource usage. CORDIC performs\\nsimple computations using only addition, subtraction, bit shifting, and table lookups, which are\\nefficient to implement in FPGAs and more generally in hardware.\\nThe CORDIC method was developed by Jack Volder in the 1950’s as a digital solution to\\nreplace an analog resolver for real-time navigation on a B-58 bomber. A resolver measures\\ndegrees of rotation. At that time'),\n",
       " Document(page_content='method was developed by Jack Volder in the 1950’s as a digital solution to\\nreplace an analog resolver for real-time navigation on a B-58 bomber. A resolver measures\\ndegrees of rotation. At that time hardware implementations of multiply operations were pro-\\nhibitively expense and CPUs had very limited amount of state. Thus the algorithm needed\\nto have low complexity and use simple operations. Over the years, it has been used in math\\nco-processors [24], linear systems [3], radar signal processing [4], Fourier transforms [21], and\\nmany other digital signal processing algorithms. It is now commonly used in FPGA designs.\\nVivado(cid:13)R HLS uses a CORDIC core for calculating trigonometric functions and it is a common\\nelement of modern FPGA IP core libraries.\\nThegoalofthischapteristodemonstratehowtocreateanoptimizedCORDICcoreusinghigh-\\nlevel synthesis. We are gradually increasing the complexity of the types of hardware cores that we\\nare developing as we progress through the book. The CORDIC'),\n",
       " Document(page_content='synthesis. We are gradually increasing the complexity of the types of hardware cores that we\\nare developing as we progress through the book. The CORDIC method is an iterative algorithm;\\nthus most of the computation is performed within a single for loop. The code itself is not all\\nthat complex. However, understanding the code such that we can create an optimal hardware\\nimplementation requires deep insight. And a good HLS designers must always understand the\\ncomputation if they wish to create the optimal design. Thus, we spend the early part of this\\nchapter giving the mathematical and computational background of the CORDIC method.\\nThe major HLS optimization that we wish to highlight in this chapter is choosing the correct\\nnumber representation for the variables. As we discuss later in the chapter, the designer must\\ncarefully tradeoff between the accuracy of the results, the performance, and resource utilization of\\nthe design. Number representation is one big factor in this tradeoff –'),\n",
       " Document(page_content='chapter, the designer must\\ncarefully tradeoff between the accuracy of the results, the performance, and resource utilization of\\nthe design. Number representation is one big factor in this tradeoff – “larger” numbers (i.e., those\\nwith more bits) generally provide more precision at the cost of increased resource usage (more FFs\\n55CORDIC\\nand logic blocks) and reduced performance. We provide a background on number representation\\nand arbitrary data types in Chapter 3.5.5.\\nThis chapter is coupled with the project described in Chapter ?? that allows more in-depth\\nexperimentationwiththetradeoffsbetweenprecision(accuracyofthecomputation),resourceusage,\\nand performance. The aim of this chapter is to provide enough insight so that one can perform\\nthe exercises from that project, i.e., this chapter and that project are meant to complement each\\nother. The goal of the project is to build a phase detector which uses a CORDIC and a complex\\nmatched filter which we have conveniently covered in this and'),\n",
       " Document(page_content='that project are meant to complement each\\nother. The goal of the project is to build a phase detector which uses a CORDIC and a complex\\nmatched filter which we have conveniently covered in this and the previous chapter.\\n3.2 Background\\nThe core idea behind the CORDIC is to efficiently perform a set of vector rotations in a two-\\ndimensionalplane. Byoverlayingtheserotationswithsomesimplecontroldecisions,wecanperform\\na variety of fundamental operations, e.g., trigonometric, hyperbolic, and logarithmic functions, real\\nand complex multiplication, and matrix decompositions and factorizations. CORDIC has been\\nused in a wide range of applications including signal processing, robotics, communications, and\\nmany scientific computations. CORDIC is commonly used in FPGA design since it has a small\\nresource usage.\\nInthefollowing, wewalkthroughtheprocessofhowaCORDICperformsthesineandcosineof\\nagivenaninputangleθ. Thisisdoneusingaseriesofvectorrotationsusingonlysimpleoperations\\nwhich are very efficient'),\n",
       " Document(page_content='usage.\\nInthefollowing, wewalkthroughtheprocessofhowaCORDICperformsthesineandcosineof\\nagivenaninputangleθ. Thisisdoneusingaseriesofvectorrotationsusingonlysimpleoperations\\nwhich are very efficient to implement in hardware. At the high level, the algorithm works using a\\nseriesofrotationswiththegoalofreachingthetargetinputangleθ. Thekeyinnovationthatmakes\\nthis efficient is that the rotations can be done in a manner that requires minimal computation. In\\nparticular,weperformtherotationsusingmultiplicationsbyconstantpowersoftwo. Thistranslates\\nto simply moving bits around in hardware which is extremely efficient as it does not require any\\nsort of logic.\\nFigure 3.1 provides a high level overview of the CORDIC procedure for calculating cosφ and\\nsinφ. In this case, we start our initial rotating vector on the x-axis, i.e, at a 0◦ angle. Then,\\nwe perform an iterative series of rotations; in this example we only perform four rotations, but\\ngenerally this is on the order of 40 rotations. Each of'),\n",
       " Document(page_content='vector on the x-axis, i.e, at a 0◦ angle. Then,\\nwe perform an iterative series of rotations; in this example we only perform four rotations, but\\ngenerally this is on the order of 40 rotations. Each of the subsequent rotation uses an increasingly\\nsmaller angle, which means that every iteration adds a bit more precision to the output value.\\nAt each iteration, we decide between doing a positive or negative rotation by that smaller angle.\\nThe angle values that we rotate are fixed a priori; thus, we can easily store their values in a small\\nmemory and keep a running sum of the cumulative angle that we have rotated so far. If this\\ncumulative angle is larger than our target angle φ, then we perform a negative rotation. If it is\\nsmaller, then the rotation is positive. Once we have completed a sufficient number of rotations,\\nwe can determine the cosφ and sinφ by directly reading the x and y values from the final rotated\\nvector. If our final vector has a magnitude of 1, then x = cosφ and y ='),\n",
       " Document(page_content='sufficient number of rotations,\\nwe can determine the cosφ and sinφ by directly reading the x and y values from the final rotated\\nvector. If our final vector has a magnitude of 1, then x = cosφ and y = sinφ.\\nWe start with some terminology. The goal is to refresh your memory about some basic trigono-\\nmetric and vector concepts. Feel free to skim this if it is familiar. But keep in mind that one\\nof the most important aspects of creating an efficient hardware design is to truly understand the\\napplication; only then can the designer effectively utilize the optimization directives and perform\\ncode refactoring which is required to get the most efficient designs.\\nThefundamentalgoaloftheCORDICalgorithmistoperformaseriesofrotationsinanefficient\\nmanner. Let us start by thinking about how to generally perform a rotation. In two dimensions,\\nthe rotation matrix is:\\n(cid:20) (cid:21)\\ncosθ −sinθ\\nR(θ) = (3.1)\\nsinθ cosθ\\n56CORDIC\\nTarget Angle φ\\ny = sin φ\\n4\\n3\\n2\\n1\\nx = cos φ\\nFigure 3.1: Using the CORDIC to'),\n",
       " Document(page_content='perform a rotation. In two dimensions,\\nthe rotation matrix is:\\n(cid:20) (cid:21)\\ncosθ −sinθ\\nR(θ) = (3.1)\\nsinθ cosθ\\n56CORDIC\\nTarget Angle φ\\ny = sin φ\\n4\\n3\\n2\\n1\\nx = cos φ\\nFigure 3.1: Using the CORDIC to calculate the functions sinφ and cosφ. Here, the CORDIC\\nstarts at the x-axis with a corresponding 0◦ angle. It then performs four iterative positive/negative\\nrotations in increasingly smaller rotation angle with the ultimate goal of reaching the target angle\\nφ. Once we finish our rotations we are close to the target angle. We take the corresponding x and y\\nvalues of the final vector which correspond to cosφ and sinφ (respectively) assuming the length of\\nthe vector is 1. The key to the CORDIC is doing all of this in a computationally efficient manner.\\nThe CORDIC uses an iterative algorithm that rotates a vector v to some target of angle which\\ndepends on the function that the CORDIC is performing. One rotation is a matrix vector mul-\\ntiplications in the form of v = R ·v . Thus in each'),\n",
       " Document(page_content='that rotates a vector v to some target of angle which\\ndepends on the function that the CORDIC is performing. One rotation is a matrix vector mul-\\ntiplications in the form of v = R ·v . Thus in each iteration of the CORDIC we perform the\\ni i i−1\\nfollowing operations to perform one rotation which is the matrix vector multiply:\\n(cid:20) (cid:21)(cid:20) (cid:21) (cid:20) (cid:21)\\ncosθ −sinθ x x\\ni−1 = i (3.2)\\nsinθ cosθ y y\\ni−1 i\\nWriting out the linear equations, the coordinates of the newly rotated vector are:\\nx = x cosθ−y sinθ (3.3)\\ni i−1 i−1\\nand\\ny = x sinθ+y cosθ (3.4)\\ni i−1 i−1\\nThisispreciselytheoperationthatweneedtosimplify. Wewanttoperformtheserotationswithout\\nhaving to perform any multiplications.\\nConsider first a 90◦ rotation. In this case the rotation matrix is:\\n(cid:20) cos90◦ −sin90◦(cid:21) (cid:20) 0 −1(cid:21)\\nR(90◦) = = (3.5)\\nsin90◦ cos90◦ 1 0\\nand thus we only have to perform the operations:\\nx = x cos90◦−y sin90◦\\ni i−1 i−1\\n= x ·0−y ·1\\ni−1 i−1\\n= −y (3.6)\\ni−1\\n57CORDIC\\nand\\ny = x'),\n",
       " Document(page_content='(cid:20) 0 −1(cid:21)\\nR(90◦) = = (3.5)\\nsin90◦ cos90◦ 1 0\\nand thus we only have to perform the operations:\\nx = x cos90◦−y sin90◦\\ni i−1 i−1\\n= x ·0−y ·1\\ni−1 i−1\\n= −y (3.6)\\ni−1\\n57CORDIC\\nand\\ny = x sin90◦+y cos90◦\\ni i−1 i−1\\n= x ·1+y ·0\\ni−1 i−1\\n= x (3.7)\\ni−1\\nPutting this altogether we get\\n(cid:20) (cid:21)(cid:20) (cid:21) (cid:20) (cid:21)\\n0 −1 x −y\\n= (3.8)\\n1 0 y x\\nYou can see that this is requires a very minimal amount of calculation; the rotated vector simply\\nnegates the y value, and then swaps the x and y values. A two’s complement negation requires the\\nhardware equivalent to an adder. Thus, we have achieved our goal of performing a 90◦ rotation\\nefficiently.\\nWhat if you wanted to rotation by −90◦? What is the rotation matrix R(−90◦)? What\\ntypeofcalculationisrequiredforthisrotation? Howwouldonedesignthemostefficientcircuit\\nthat could perform a positive and negative rotation by −90◦, i.e., the direction of rotation is\\nan input to the circuit?\\nWhile it is great that we can rotate by ±90◦, we'),\n",
       " Document(page_content='could perform a positive and negative rotation by −90◦, i.e., the direction of rotation is\\nan input to the circuit?\\nWhile it is great that we can rotate by ±90◦, we also need to rotate by smaller angles if we wish\\nto have any sort of good resolution in moving to the target angle. Perhaps the next natural angle\\nthat we might wish to rotate would be ±45◦. Using the rotation matrix from Equation 3.1, we get\\n√ √\\n(cid:20) cos45◦ −sin45◦(cid:21) (cid:20) 2/2 − 2/2(cid:21)\\nR(45◦) = = √ √ (3.9)\\nsin45◦ cos45◦ 2/2 2/2\\nCalculating out the computation for performing the rotation, we get\\nx = x cos45◦−y sin45◦\\ni i−1 i−1\\n√ √\\n= x · 2/2−y · 2/2 (3.10)\\ni−1 i−1\\nand\\ny = x sin45◦+y cos45◦\\ni i−1 i−1\\n√ √\\n= x · 2/2+y · 2/2 (3.11)\\ni−1 i−1\\nwhich when put back into matrix vector notation is\\n√ √ √ √\\n(cid:20) (cid:21)(cid:20) (cid:21) (cid:20) (cid:21)\\n2/2 − 2/2 x 2/2x− 2/2y\\n√ √ = √ √ (3.12)\\n2/2 2/2 y 2/2x+ 2/2y\\nThis certainly is not as efficient of a computation as compared to rotating by ±90◦. The ±90◦\\nrotation'),\n",
       " Document(page_content='(cid:21) (cid:20) (cid:21)\\n2/2 − 2/2 x 2/2x− 2/2y\\n√ √ = √ √ (3.12)\\n2/2 2/2 y 2/2x+ 2/2y\\nThis certainly is not as efficient of a computation as compared to rotating by ±90◦. The ±90◦\\nrotation was ideal because the multiplication were by very simple constants (in this case 0, 1, and\\n−1). The key to the CORDIC is doing these rotations in an efficient manner, i.e., defining the\\nrotation matrix in a way that their multiplication is trivial to compute. That is, we wish to be\\nmore like the previous ±90◦ and less like the much more difficult computation required for the\\n±45◦ rotation that we just described.\\n58CORDIC\\nWhat if we “forced” the rotation matrix to be constants that were easy to multiply? For exam-\\nple, a multiplication by any power of two turns into a shift operation. If we set the constants in the\\nrotation matrix to be powers of two, we could very easily perform rotations without multiplication.\\nThis is the key idea behind the CORDIC – finding rotations that are very efficient to'),\n",
       " Document(page_content='in the\\nrotation matrix to be powers of two, we could very easily perform rotations without multiplication.\\nThis is the key idea behind the CORDIC – finding rotations that are very efficient to compute\\nwhile minimizing any side effects. We will discuss these “side effects” in more detail, but there is\\nan engineering decision that is being made here. In order to get efficient computation, we have\\nto give up something; in this case we have to deal with the fact that the rotation also performs\\nscaling, i.e., it changes the magnitude of the rotated vector – more on that later.\\nTo further explore the idea of “simple” rotation matrices, consider the matrix\\n(cid:20) (cid:21)\\n1 −1\\nR() = (3.13)\\n1 1\\nwith the corresponding computation for the transformation\\nx = x −y (3.14)\\ni i−1 i−1\\nand\\ny = x +y (3.15)\\ni i−1 i−1\\nwith the matrix vector form of\\n(cid:20) (cid:21)(cid:20) (cid:21) (cid:20) (cid:21)\\n1 −1 x x−y\\n= (3.16)\\n1 1 y'),\n",
       " Document(page_content='computation for the transformation\\nx = x −y (3.14)\\ni i−1 i−1\\nand\\ny = x +y (3.15)\\ni i−1 i−1\\nwith the matrix vector form of\\n(cid:20) (cid:21)(cid:20) (cid:21) (cid:20) (cid:21)\\n1 −1 x x−y\\n= (3.16)\\n1 1 y x+y\\nThisiscertainlyeasytocomputeanddoesnotrequireany“difficult”multiplications. Butwhat\\nis the consequence of this operation? It turns out that this performs a rotation by 45◦ which is\\nperfect; we now have an efficient way to perform a 45◦ rotation. But, this transform also scales the\\n√\\nvector by a factor of 2. The square root of the determinant of this matrix tells us how much the\\ntransformation scales the vector, i.e., how the length of the vector has changed. The determinant\\n√\\nof this matrix here is 1·1−(−1)·1 = 2. Thus, this operation rotates by 45◦ and scales by 2.\\nThis is the tradeoff that the CORDIC makes; we can make the computation for the rotation easy\\nto compute but it has the side effect that scales the length of the vector. This may or may not be\\na problem depending on the'),\n",
       " Document(page_content='that the CORDIC makes; we can make the computation for the rotation easy\\nto compute but it has the side effect that scales the length of the vector. This may or may not be\\na problem depending on the application. But for now, we put aside the scaling issue and focus on\\nhow to generalize the idea of performing rotations that are computationally efficient to perform.\\nNow we generalize the notion of performing efficient matrix rotations, i.e., performing rota-\\ntions by only performing addition/subtraction and multiplication by a power of two (i.e., by shift\\noperations). Consider again the rotation matrix\\n(cid:20) (cid:21)\\ncos(θ ) −sin(θ )\\nR (θ) = i i (3.17)\\ni sin(θ ) cos(θ )\\ni i\\nBy using the following trigonometric identities,\\n1\\ncos(θ ) = (3.18)\\ni (cid:112)\\n1+tan2(θ )\\ni\\ntan(θ )\\ni\\nsin(θ ) = (3.19)\\ni (cid:112)\\n1+tan2(θ )\\ni\\nwe can rewrite the rotation matrix as\\n(cid:20) (cid:21)\\n1 1 −tan(θ )\\nR = i (3.20)\\ni (cid:112) 1+tan2(θ i) tan(θ i) 1\\n59CORDIC\\nIf we restrict the values of tan(θ ) to be a'),\n",
       " Document(page_content='(cid:112)\\n1+tan2(θ )\\ni\\nwe can rewrite the rotation matrix as\\n(cid:20) (cid:21)\\n1 1 −tan(θ )\\nR = i (3.20)\\ni (cid:112) 1+tan2(θ i) tan(θ i) 1\\n59CORDIC\\nIf we restrict the values of tan(θ ) to be a multiplication by a factor of two, the rotation can\\ni\\nbe performed using a shifts (for the multiplication) and additions. More specifically, we use let\\ntan(θ ) = 2−i. The rotation then becomes\\ni\\n(cid:20)\\n1\\n−2−i(cid:21)(cid:20)\\nx\\n(cid:21)\\nv = K i−1 (3.21)\\ni i 2−i 1 y\\ni−1\\nwhere\\n1\\nK = √ (3.22)\\ni\\n1+2−2i\\nA few things to note here. The 2−i is equivalent to a right shift by i bits, i.e., a division by a\\npower of two. This is essentially just a simple rewiring which does not require any sort of logical\\nresources, i.e., itisessentially“free”tocomputeinhardware. Thisisahugebenefit, butitdoesnot\\ncome without some drawbacks. First, we are limited to rotate by angles θ such that tan(θ ) = 2−i.\\ni\\nWe will show that this is not much of a problem. Second, we are only showing rotation in one\\ndirection; the CORDIC'),\n",
       " Document(page_content='drawbacks. First, we are limited to rotate by angles θ such that tan(θ ) = 2−i.\\ni\\nWe will show that this is not much of a problem. Second, we are only showing rotation in one\\ndirection; the CORDIC requires the ability to rotation by ±θ. This is simple to correct by adding\\nin σ which can have a value of 1 or −1, which corresponds to performing a positive or negative\\nrotation. We can have a different σ at every iteration/rotation. Thus the rotation operation\\ni\\ngeneralizes to\\n(cid:20)\\n1 −σ\\n2−i(cid:21)(cid:20)\\nx\\n(cid:21)\\nv = K i i−1 (3.23)\\ni i σ 2−i 1 y\\ni i−1\\nFinally, the rotation requires a multiplication by K . K is typically ignored in the iterative process\\ni i\\nand then adjusted for after the series of rotations is completed. The cumulative scaling factor is\\nn−1 n−1\\n(cid:89) (cid:89) 1\\nK(n) = K = √ (3.24)\\ni\\n1+2−2i\\ni=0 i=0\\nand\\nK = lim K(n) ≈ 0.6072529350088812561694 (3.25)\\nn→∞\\nThe scaling factors for different iterations can be calculated in advance and stored in a table. If we\\nalways'),\n",
       " Document(page_content='= K = √ (3.24)\\ni\\n1+2−2i\\ni=0 i=0\\nand\\nK = lim K(n) ≈ 0.6072529350088812561694 (3.25)\\nn→∞\\nThe scaling factors for different iterations can be calculated in advance and stored in a table. If we\\nalways perform a fixed number of rotations, this is simply one constant. This correction could also\\nbe made in advance by scaling v appropriately before performing the rotations. Sometimes it is\\n0\\nok to ignore this scaling, which results in a processing gain\\nn−1\\n1 (cid:89)(cid:112)\\nA = = lim 1+2−2i ≈ 1.64676025812107 (3.26)\\nK n→∞\\ni=0\\nAt each iteration, we need to know the angle θ of the rotation that was just performed. This\\ni\\nis derived as θ = arctan2−i. We can precompute these values for each value of i and store them in\\ni\\nan on-chip memory and use them as a lookup table. Additionally, we have a control decision that\\ndetermines whether the rotation is clockwise or counterclockwise, i.e., we must determine if σ is 1\\nor −1. This decision depends on the desired CORDIC mode. For example, for'),\n",
       " Document(page_content='a control decision that\\ndetermines whether the rotation is clockwise or counterclockwise, i.e., we must determine if σ is 1\\nor −1. This decision depends on the desired CORDIC mode. For example, for calculating cosφ\\nand sinφ, we keep a running sum of the cumulative angle that we have rotated so far. We compare\\nthis to the target angle φ and perform a positive rotation if our current angle is less than φ and a\\nnegative rotation is our current angle is greater than φ.\\nTable 3.1 provides the statistics for the first seven iterations of a CORDIC. The first row is the\\n“zeroth” rotation (i.e., when i = 0), which is a 45◦ rotation. It performs a scaling of the vector by\\n60CORDIC\\na factor of 1.41421. The second row is the does a rotation by 2−1 = 0.5. This results in a rotation\\nby θ = arctan2−1 = 26.565◦. This rotation scales the vector by 1.11803. The CORDIC gain is\\nthe overall scaling of the vector. In this case, it is the scaling factor of the first two rotations,\\ni.e., 1.58114 ='),\n",
       " Document(page_content='= 26.565◦. This rotation scales the vector by 1.11803. The CORDIC gain is\\nthe overall scaling of the vector. In this case, it is the scaling factor of the first two rotations,\\ni.e., 1.58114 = 1.41421·1.11803. This process continues by incrementing i which results in smaller\\nand smaller rotating angles and scaling factors. Note that the CORDIC gain starts to stabilize to\\n≈ 1.64676025812107 as described in Equation 3.26. Also, note as the angles get smaller, they have\\nless effect on the most significant digits.\\nDescribe the effect if the ith iteration on the precision of the results? That is, what bits\\ndoes it change? How does more iterations change the precision of the final result, i.e., how do\\nthe values of sinφ and cosφ change as the CORDIC performs more iterations?\\nTable 3.1: The rotating angle, scaling factor, and CORDIC gain for the first seven iterations of\\na CORDIC. Note that the angle decreases by approximately half each time. The scaling factor\\nindicates how much the length'),\n",
       " Document(page_content='angle, scaling factor, and CORDIC gain for the first seven iterations of\\na CORDIC. Note that the angle decreases by approximately half each time. The scaling factor\\nindicates how much the length the the vector increases during that rotation. The CORDIC gain is\\nthe overall increase in the length of the vector which is the product of all of the scaling factors for\\nthe current and previous rotations.\\ni 2−i Rotating Angle Scaling Factor CORDIC Gain\\n0 1.0 45.000◦ 1.41421 1.41421\\n1 0.5 26.565◦ 1.11803 1.58114\\n2 0.25 14.036◦ 1.03078 1.62980\\n3 0.125 7.125◦ 1.00778 1.64248\\n4 0.0625 3.576◦ 1.00195 1.64569\\n5 0.03125 1.790◦ 1.00049 1.64649\\n6 0.015625 0.895◦ 1.00012 1.64669\\n3.3 Calculating Sine and Cosine\\nNow we describe more precisely our running example of using a CORDIC to calculate the sine and\\ncosine of some given angle φ. In order to do this, we start with a vector on the positive x-axis\\n(i.e., with an initial angle of 0◦) and perform a series of rotations until we are approximately at\\nthe'),\n",
       " Document(page_content='of some given angle φ. In order to do this, we start with a vector on the positive x-axis\\n(i.e., with an initial angle of 0◦) and perform a series of rotations until we are approximately at\\nthe given angle φ. Then we can simply read the x and y values of the resulting rotated vector to\\nget the values cosφ and sinφ, respectively. This assumes that the amplitude of the final vector is\\nequal to 1, which as you will see is not too difficult to achieve.\\nLet us illustrate this with an example: calculating cos60◦ and sin60◦, i.e., φ = 60◦. This\\nprocess is depicted graphically in Figure 3.2. Here we perform five rotations in order to give a final\\nvector with an angle approximately equal to 60◦. Our initial vector has a 0◦ angle, i.e., it starts on\\nthe positive x-axis. The first rotation corresponds to i = 0 which has a 45◦ angle (see Table 3.1).\\nSince we want to get to 60◦, we rotate in the positive direction. The resulting rotated vector has\\na 45◦ angle; also note that its amplitude is scaled'),\n",
       " Document(page_content='to i = 0 which has a 45◦ angle (see Table 3.1).\\nSince we want to get to 60◦, we rotate in the positive direction. The resulting rotated vector has\\na 45◦ angle; also note that its amplitude is scaled by approximately 1.414. Now, we move on to\\ni = 1. As we wish to get to a 60◦ angle, we rotate again in the positive direction. This rotation\\nresults in a vector that has an angle of 45◦+26.565◦ = 71.565◦ and is scaled by a factor of 1.118;\\n61CORDIC\\n4) 57.529°+7.125°=64.654°\\n5) 64.64°-3.576°=61.078°\\n2) 45°+26.565°=71.565°\\n3) 71.565°-14.036°=57.529°\\n5\\n1) 0°+45°=45°\\n4\\n3\\n2\\n1\\nFigure 3.2: Calculating cos60◦ and sin60◦ using the CORDIC algorithm. Five rotations are per-\\nformed using incrementally larger i values (0,1,2,3,4). The result is a vector with an angle of\\n61.078◦. The corresponding x and y values of that vector give the approximate desired cosine and\\nsine values.\\nthe total scaling resulting from the two rotations is 1.414×1.118 = 1.581. This is the CORDIC\\ngain. Moving on to i = 2, we now'),\n",
       " Document(page_content='of that vector give the approximate desired cosine and\\nsine values.\\nthe total scaling resulting from the two rotations is 1.414×1.118 = 1.581. This is the CORDIC\\ngain. Moving on to i = 2, we now determine that our current angle is larger than the 60◦ target,\\nso we rotate by a negative angle resulting in a vector with a 57.529◦ angle and scaled by a factor\\nof 1.630. This process continues by rotating the vector with incrementally larger i values, resulting\\nin smaller and smaller rotations that will eventually (approximately) reach the desired angle. Also,\\nnote that the CORDIC gain begins to stabilize as the number of rotation increases.\\nAfter we perform a sufficient number of rotations, which is a function of the desired accuracy,\\nwe get a vector with an angle close to the desired input angle. The x and y values of that vector\\ncorrespond to approximately A cos60◦ and A sin60◦, which is exactly what we want if A = 1.\\nR R R\\nSince we typically know a priori the number of rotations that we'),\n",
       " Document(page_content='The x and y values of that vector\\ncorrespond to approximately A cos60◦ and A sin60◦, which is exactly what we want if A = 1.\\nR R R\\nSince we typically know a priori the number of rotations that we will perform, we can insure that\\nA = 1 by setting the magnitude of the initial vector to the reciprocal of the CORDIC gain. In\\nR\\nthe case of our example, assuming that we perform five rotations as shown in Figure 3.2, this value\\nis 1.64649−1 = 0.60735 (the reciprocal of the CORDIC gain when i = 5; see Table 3.1). We can\\neasily set the amplitude of the initial vector by starting at a vector (0.60735,0).\\nHow would the answer change if we performed one more rotation? How about two (three,\\nfour,etc.) morerotations? Whatistheaccuracy(e.g.,comparedtoMATLABimplementation)\\nas we perform more rotations? How many rotations do you think is sufficient in the general\\ncase?\\nIs it possible to get worse accuracy by performing more rotations? Provide an example\\nwhen this would occur.\\n62CORDIC\\nFigure 3.3'),\n",
       " Document(page_content='How many rotations do you think is sufficient in the general\\ncase?\\nIs it possible to get worse accuracy by performing more rotations? Provide an example\\nwhen this would occur.\\n62CORDIC\\nFigure 3.3 provides code that implements sine and cosine calculation using the CORDIC algo-\\nrithm. Ittakesasinputatargetangle, andoutputsthesineandcosinevaluescorrespondingtothat\\nangle. Thecodeusesanarraycordic phaseasalookuptablethatholdstheangleofrotationforeach\\niteration. This corresponds to the values in the “Rotating Angle” column in Table 3.1. We assume\\nthat the cordic.h file defines the different data types (i.e., COS SIN TYPE and THETA TYPE) and\\nsets NUM ITERATIONS to some constant value. The data types can be changed to different fixed\\nor floating point types, and NUM ITERATIONS set depending on our desired accuracy, area, and\\nthroughput.\\nNotice that the variable sigma is set as a two bit integer. Since we know that this will only\\ntake the value of ±1 we can change its data type which will'),\n",
       " Document(page_content='on our desired accuracy, area, and\\nthroughput.\\nNotice that the variable sigma is set as a two bit integer. Since we know that this will only\\ntake the value of ±1 we can change its data type which will result in smaller area and better\\nperformance than if we were to use the current int data type. We discuss data types and how\\nto specify them in Vivado(cid:13)R HLS shortly.\\nThis code is close to a “software” version. It can be optimized in many ways to increase its\\nperformance and reduce its area. We will discuss how to optimize this code later in the chapter.\\n3.4 Cartesian to Polar Conversion\\nWith some modifications, the CORDIC can perform other functions. For example, it can convert\\nbetween Cartesian and polar representations; we describe that in more detail in this section. The\\nCORDIC can also do many other functions, which we will leave as an exercise to the reader.\\nA two-dimensional vector v can be represented using a Cartesian coordinate system (x,y) or in\\nthe polar coordinate'),\n",
       " Document(page_content='can also do many other functions, which we will leave as an exercise to the reader.\\nA two-dimensional vector v can be represented using a Cartesian coordinate system (x,y) or in\\nthe polar coordinate system (r,θ) where r is the radial coordinate (length of the vector) and θ is\\nthe angular coordinate. Both of these coordinate systems have their benefits and drawbacks. For\\nexample, if we want to do a rotation, then it is easier to think about the polar form while a linear\\ntransform is more easily described using the Cartesian system.\\nThe relationship between these coordinates is shown in the following equations:\\nx = rcosθ (3.27)\\ny = rsinθ (3.28)\\n(cid:112)\\nr = x2+y2 (3.29)\\nθ = atan2(y,x) (3.30)\\nwhere atan2 is a common variation on the arctangent function defined as\\n\\uf8f1\\n\\uf8f4arctan(y) if x > 0\\n\\uf8f4 \\uf8f4 x\\n\\uf8f4 \\uf8f4 \\uf8f4arctan(y)+π if x < 0 and y ≥ 0\\n\\uf8f4 x\\n\\uf8f4\\n\\uf8f4 \\uf8f2arctan(y)−π if x < 0 and y < 0\\natan2(y,x) = x (3.31)\\n\\uf8f4π if x = 0 and y > 0\\n\\uf8f42\\n\\uf8f4\\n\\uf8f4 \\uf8f4 \\uf8f4−π if x = 0 and y < 0\\n\\uf8f4 2\\n\\uf8f4\\n\\uf8f4\\n\\uf8f3undefined if x = 0 and y ='),\n",
       " Document(page_content='x > 0\\n\\uf8f4 \\uf8f4 x\\n\\uf8f4 \\uf8f4 \\uf8f4arctan(y)+π if x < 0 and y ≥ 0\\n\\uf8f4 x\\n\\uf8f4\\n\\uf8f4 \\uf8f2arctan(y)−π if x < 0 and y < 0\\natan2(y,x) = x (3.31)\\n\\uf8f4π if x = 0 and y > 0\\n\\uf8f42\\n\\uf8f4\\n\\uf8f4 \\uf8f4 \\uf8f4−π if x = 0 and y < 0\\n\\uf8f4 2\\n\\uf8f4\\n\\uf8f4\\n\\uf8f3undefined if x = 0 and y = 0\\n63CORDIC\\n// The file cordic.h holds definitions for the data types and constant values\\n#include ”cordic.h”\\n// The cordic phase array holds the angle for the current rotation\\nTHETA TYPE cordic phase[NUM ITERATIONS] = {\\n45, 26.565, 14.036, 7.125,\\n3.576, 1.790, 0.895, ...\\n};\\nvoid\\ncordic(THETA TYPE theta, COS SIN TYPE &s, COS SIN TYPE &c)\\n{\\n// Set the initial vector that we will rotate\\n// current cos = I; current sin = Q\\nCOS SIN TYPE current cos = 0.60735;\\nCOS SIN TYPE current sin = 0.0;\\n// Factor is the 2ˆ(−L) value\\nCOS SIN TYPE factor = 1.0;\\n// This loop iteratively rotates the initial vector to find the\\n// sine and cosine values corresponding to the input theta angle\\nfor (int j = 0; j < NUM ITERATIONS; j++) {\\n// Determine if we are rotating by a positive or negative angle\\nint sigma ='),\n",
       " Document(page_content='to find the\\n// sine and cosine values corresponding to the input theta angle\\nfor (int j = 0; j < NUM ITERATIONS; j++) {\\n// Determine if we are rotating by a positive or negative angle\\nint sigma = (theta < 0) ? −1 : 1;\\n// Save the current cos, so that it can be used in the sine calculation\\nCOS SIN TYPE temp cos = current cos;\\n// Perform the rotation\\ncurrent cos = current cos − current sin ∗ sigma ∗ factor;\\ncurrent sin = temp cos ∗ sigma ∗ factor + current sin;\\n// Determine the new theta\\ntheta = theta − sigma ∗ cordic phase[j];\\n// Calculate next 2ˆ(−L) value\\nfactor = factor >> 1;\\n}\\n// Set the final sine and cosine values\\ns = current sin; c = current cos;\\n}\\nFigure 3.3: CORDIC code implementing the sine and cosine of a given angle.\\n64CORDIC\\ny = r sin θ (x,y)\\nr\\nθ\\nx = r cos θ\\nFigure3.4: Thefigureshowsatwo-dimensionalplaneandavectorrepresentedinboththeCartesian\\nform (x,y) and the polar form (r,θ) and provides the relationship between those two'),\n",
       " Document(page_content='r sin θ (x,y)\\nr\\nθ\\nx = r cos θ\\nFigure3.4: Thefigureshowsatwo-dimensionalplaneandavectorrepresentedinboththeCartesian\\nform (x,y) and the polar form (r,θ) and provides the relationship between those two coordinate\\nsystems.\\nThisprovidesawaytotranslatebetweenthetwocoordinatesystems. However,theseoperations\\nare not easy to implement in hardware. For example, sine, cosine, square root, and arctan are not\\nsimple operations and they require significant amount of resources. But we can use the CORDIC\\nto perform these operations using a series of simple iterative rotation operations.\\nGiven a number in Cartesian form (x,y), we can calculates its radial and amplitude coordinate\\n(i.e., convertittopolarform)usingtheCORDIC.Todothis, werotatethegivenCartesiannumber\\nto 0◦. Once this rotation is complete, the amplitude is the x value of the final rotated vector. To\\ndetermine the radial coordinate, we simply keep track of the cumulative angle of the rotations that\\nthe CORDIC performs. The angles of the'),\n",
       " Document(page_content='the amplitude is the x value of the final rotated vector. To\\ndetermine the radial coordinate, we simply keep track of the cumulative angle of the rotations that\\nthe CORDIC performs. The angles of the rotating vector (for i = 0,1,2,3,...) are known and can\\nbe stored in a lookup table as done for calculating sine/cosine. Therefore, we simply need to keep\\ntrack of the total rotation angle by performing an addition or subtraction of these angles, which\\ndepends on the direction of rotation.\\nThe algorithm is similar to that of calculating the sine and cosine of a given angle. We perform\\na set of rotations with increasing values of i such that the final vector resides on (close to) the\\npositive x-axis (i.e., an angle of 0◦). This can be done using positive or negative rotations which is\\npredicated on the y value of the vector whose amplitude and phase we wish to determine.\\nThe first step of the algorithm performs a rotation to get the initial vector into either Quadrant\\nI or IV. This rotates'),\n",
       " Document(page_content='on the y value of the vector whose amplitude and phase we wish to determine.\\nThe first step of the algorithm performs a rotation to get the initial vector into either Quadrant\\nI or IV. This rotates the vector by ±90◦ depending on the sign of the y value of the initial vector. If\\nthe y value is positive, we know that we are in either Quadrant I or II. A rotation by −90◦ will put\\nus into Quadrant IV or I, respectively. Once we are in either of those quadrants, we can guarantee\\nthatwewillbeabletoasymptoticallyapproachthetarget0◦ angle. IfweareinQuadrantIIIorIV,\\nthe y value of the initial vector will be negative. And a rotation by 90◦ will put us into Quadrant\\nIV or I, respectively. Recall that a ±90◦ rotation is done by negating either the x or y values of\\n65CORDIC\\nthe vector and then swapping those values (see Equations ?? and ??). The concept of these ±90◦\\nis shown in Figure 3.5.\\nThere is an issue with the final radial value of the rotated vector; its magnitude is not the\\nsame as the'),\n",
       " Document(page_content='those values (see Equations ?? and ??). The concept of these ±90◦\\nis shown in Figure 3.5.\\nThere is an issue with the final radial value of the rotated vector; its magnitude is not the\\nsame as the initial magnitude before the rotations; it is scaled by the CORDIC gain. Or course,\\none could calculate the precise radial value of the vector by multiplying by the reciprocal of the\\nappropriate CORDIC gain (approximately 1/1.647 = 0.607)1. However, this defeats the purpose\\nof having a CORDIC, which eliminates the need for costly multiplication. And unfortunately this\\nmultiplication cannot be performed trivially using shifts and adds. Fortunately, this factor is often\\nnot important. e.g., in amplitude shift keying used in modulation in wireless communications, you\\nonlyneedtoknowarelativemagnitude. Orinothertimes, thisamplitudegaincanbecompensated\\nby other parts of the system.\\n3.5 Number Representation\\nThe cordic function uses currently uses common types for the variables. For example, the'),\n",
       " Document(page_content='Orinothertimes, thisamplitudegaincanbecompensated\\nby other parts of the system.\\n3.5 Number Representation\\nThe cordic function uses currently uses common types for the variables. For example, the variable\\nsigma is defined as an int and other variables use custom data types (e.g., THETA TYPE and\\nCOS SIN TYPE). In many cases, HLS tools are able to further optimize the representation of\\nthese values to simplify the generated hardware. For instance, in Figure 3.3, the variable sigma\\nis restricted to be either 1 or −1. Even though the variable is declared as an int type of at least\\n32 bits, many fewer bits can be used to implement the variable without changing the behavior of\\nthe program. In other cases, particularly function inputs, memories, and variables that appear in\\nrecurrences, the representation cannot be automatically optimized. In these cases, modifying the\\ncode to use smaller datatypes is a key optimization to avoid unnecessary resource usage.\\nAlthough reducing the size of'),\n",
       " Document(page_content='representation cannot be automatically optimized. In these cases, modifying the\\ncode to use smaller datatypes is a key optimization to avoid unnecessary resource usage.\\nAlthough reducing the size of variables is generally a good idea, this optimization can change\\nthe behavior of the program. A data type with fewer number of bits will not be able to express as\\nmuchinformationasadatatypewithmorebitsandnofinitebinaryrepresentationcanrepresentall\\nreal numbers with infinite accuracy. Fortunately, as designers we can pick numeric representations\\nthat are tuned to accuracy requirements of particular applications and tradeoff between accuracy,\\nresource usage, and performance.\\nBefore discussing these number representation optimizations further using our cordic function,\\nwe first give a background on number representation. We provide the basics, as this is important\\nin understand the data type specific representations provided by Vivado(cid:13)R HLS. The next section\\nstarts with a fundamental'),\n",
       " Document(page_content='on number representation. We provide the basics, as this is important\\nin understand the data type specific representations provided by Vivado(cid:13)R HLS. The next section\\nstarts with a fundamental background on number representation, and then proceeds to discuss the\\narbitrary precision variables available in Vivado(cid:13)R HLS.\\n3.5.1 Binary and Hexadecimal Numbers\\nComputers and FPGAs typically represent numbers using binary representation, which enables\\nnumberstobeefficientlyrepresentedusingon-offsignalscalledbinarydigits, orsimplybits. Binary\\nnumbers work in most ways like normal decimal numbers, but can often be the cause of confusing\\nerrors if you are not familiar with how they work. This is particularly true in many embedded\\nsystems and FPGAs where minimizing the number of bits used to represent variables can greatly\\nincrease the overall performance or efficiency of a system. In this section, we will summarize binary\\narithmetic and the basic ways that computers represent'),\n",
       " Document(page_content='used to represent variables can greatly\\nincrease the overall performance or efficiency of a system. In this section, we will summarize binary\\narithmetic and the basic ways that computers represent numbers.\\nManyreadersmayalreadybefamiliarwiththeseideas. Inthatcase,youmayskimthesesections\\nor skip them entirely. We do suggest that look at Section 3.5.5 as this provides information specific\\n1Recall that the CORDIC gain is a function of the number of rotations as show in Table 3.1.\\n66CORDIC\\na) Q b) Q\\n90°\\n90°\\nI I\\nc) Q d) Q\\n90°\\nI I\\n90°\\nFigure 3.5: The first step in performing a Cartesian to polar conversion is to perform a rotation by\\n±90◦ in order to get the initial vector into either Quadrant I or IV. Once it is in either of these two\\nquadrants, subsequent rotations will allow the vector to reach a final angle of 0◦. At this point, the\\nradial value of the initial vector is the x value of the final rotated vector and the phase of the initial\\nvector is the summation of all the angles that the'),\n",
       " Document(page_content='a final angle of 0◦. At this point, the\\nradial value of the initial vector is the x value of the final rotated vector and the phase of the initial\\nvector is the summation of all the angles that the CORDIC performed. Parts a) and b) show an\\nexample with the initial y value is positive, which means that the vector resides in either Quadrant\\nI or II. Rotating by −90◦ puts them into the appropriate quadrant. Parts c) and d) show a similar\\nsituation when the y value of the initial vector is negative. Here we wish to rotate by 90◦ to get\\nthe vector into Quadrant I or IV.\\n67CORDIC\\nto Vivado(cid:13)R HLS on how to declare arbitrary data types. This is a key idea for optimizing the\\nnumber representation of the cordic function and any HLS code.\\nWhen we write a normal integer, such as 4062, what we really mean is implicitly (4∗1000)+\\n(0∗100)+(6∗10)+(2∗1) = 4062, or written in columns:\\n103 102 101 100 unsigned\\n4 0 6 2 = 4062\\nA binary number is similar, except instead of using digits from zero to'),\n",
       " Document(page_content='really mean is implicitly (4∗1000)+\\n(0∗100)+(6∗10)+(2∗1) = 4062, or written in columns:\\n103 102 101 100 unsigned\\n4 0 6 2 = 4062\\nA binary number is similar, except instead of using digits from zero to nine and powers of ten,\\nwe use numbers from zero to one and powers of 2:\\n23 22 21 20 unsigned\\n1 0 1 1 = 11\\nsince (1∗8)+(0∗4)+(1∗2)+(1∗1) = 11. To avoid ambiguity, binary numbers are often prefixed\\nwith ”0b”. This makes it obvious that 0b1011 is the number decimal 11 and not the number 1011.\\nThe bit associated with the highest power of two is the most significant bit, and the bit associated\\nwith the lowest power of two is the least significant bit.\\nHexadecimal numbers use the digits representing numbers from zero to 15 and powers of 16:\\n163 162 161 160 unsigned\\n8 0 3 15 = 32831\\nIn order to avoid ambiguity, the digits from 10 to 15 are represented by the letters ”A” through\\n”F”, and hexadecimal numbers are prefixed with ”0x”. So the number above would normally be\\nwritten in C code as'),\n",
       " Document(page_content='to avoid ambiguity, the digits from 10 to 15 are represented by the letters ”A” through\\n”F”, and hexadecimal numbers are prefixed with ”0x”. So the number above would normally be\\nwritten in C code as 0x803F.\\nNotethatbinaryrepresentationcanalsorepresentfractionalnumbers, usuallycalledfixed-point\\nnumbers, by simply extending the pattern to include negative exponents, so that ”0b1011.01” is\\nequivalent to:\\n23 22 21 20 2−1 2−2 unsigned\\n1 0 1 1 0 1 = 11.25\\nsince 8+2+1+ 1 = 11.25. Unfortunately, the C standard doesn’t provide a way of specifying\\n4\\nconstants in binary representation, although gcc and many other compilers allow integer constants\\n(without a decimal point) to be specified with the ”0b” prefix. The C99 standard does provide a\\nway to describe floating-point constants with hexadecimal digits and a decimal exponent, however.\\nNote that the decimal exponent is required, even if it is zero.\\nfloat p1 = 0xB.4p0; // Initialize p1 to ”11.25”\\nfloat p2 = 0xB4p−4; // Initialize p2 to'),\n",
       " Document(page_content='digits and a decimal exponent, however.\\nNote that the decimal exponent is required, even if it is zero.\\nfloat p1 = 0xB.4p0; // Initialize p1 to ”11.25”\\nfloat p2 = 0xB4p−4; // Initialize p2 to ”11.25”\\nNotice that in general, it is only necessary to write non-zero digits and any digits not shown\\ncan be assumed to be zero without changing the represented value of an unsigned number. As a\\nresult, it is easy to represent the same value with more digits: simply add as many zero digits as\\nnecessary. This process is often called zero-extension. Note that each additional digit increases the\\namount of numbers that can be represented. Adding an additional bit to a binary number doubles\\nthe amount of numbers that can be represented, while an additional hexadecimal digit increases\\nthe amount of numbers by a factor of 16.\\n27 26 25 24 23 22 21 20 2−1 2−2 2−3 2−4 unsigned\\n0 0 0 0 1 0 1 1 0 1 0 0 = 11.25\\n68CORDIC\\nNote that it is possible to have any number of bits in a binary number, not just 8, 16,'),\n",
       " Document(page_content='by a factor of 16.\\n27 26 25 24 23 22 21 20 2−1 2−2 2−3 2−4 unsigned\\n0 0 0 0 1 0 1 1 0 1 0 0 = 11.25\\n68CORDIC\\nNote that it is possible to have any number of bits in a binary number, not just 8, 16, or\\n32. SystemC [2], for instance, defines several template classes for handling arbitrary precision\\nintegers and fixed-point numbers (including sc int<>, sc uint<>, sc bigint<>, sc ubigint<>,\\nsc fixed<>,andsc ufixed<>). TheseclassescanbecommonlyusedinHLStools,althoughthey\\nwereoriginallydefinedforsystemmodelingandnotnecessarilysynthesis. Vivado(cid:13)R HLS,forin-\\nstance, includes similar template classes (ap int<>, ap uint<>, ap fixed<>, and ap ufixed<>)\\nthattypicallyworkbetterthantheSystemCtemplateclasses,bothinsimulationandsynthesis.\\nArbitrary precision numbers are even well defined (although not terribly useful) with zero\\ndigits. List all the numbers that are representable with zero digits.\\n3.5.2 Negative numbers\\nNegative numbers are slightly more complicated than positive numbers, partly'),\n",
       " Document(page_content='not terribly useful) with zero\\ndigits. List all the numbers that are representable with zero digits.\\n3.5.2 Negative numbers\\nNegative numbers are slightly more complicated than positive numbers, partly because there are\\nseveral common ways to do it. One simple way is represent negative numbers with a sign bit,\\noften called signed-magnitude representation. This representation just includes an additional bit\\nto the front of the number to indicate whether it is signed or not. One somewhat odd thing about\\nsigned-magnitude representation is that there is more than one way to represent zero. This tends\\nto make even apparently simple operations, like operator ==(), more complex to implement.\\n+/- 21 20 signed magnitude\\n0 1 1 = 3\\n0 1 0 = 2\\n0 0 1 = 1\\n0 0 0 = 0\\n1 0 0 = −0\\n1 0 1 = −1\\n1 1 0 = −2\\n1 1 1 = −3\\nAnother way to represent negative numbers is with biased representation. This representation\\nadds a constant offset (usually equal in magnitude to the value of the largest bit) to the value,\\nwhich'),\n",
       " Document(page_content='−3\\nAnother way to represent negative numbers is with biased representation. This representation\\nadds a constant offset (usually equal in magnitude to the value of the largest bit) to the value,\\nwhich are otherwise treated as positive numbers:\\n22 21 20 biased\\n1 1 1 = 3\\n1 1 0 = 2\\n1 0 1 = 1\\n1 0 0 = 0\\n0 1 1 = −1\\n0 1 0 = −2\\n0 0 1 = −3\\n0 0 0 = −4\\nHowever by far the most common technique for implementing negative numbers is known as\\ntwo’s complement. In two’s complement representation, the most significant bit represents the\\nsign of the number (as in signed-magnitude representation), and also whether or not an offset is\\n69CORDIC\\napplied. One way of thinking about this situation is that the high order bit represents a negative\\ncontribution to the overall number.\\n−22 21 20 two’s complement\\n0 1 1 = 3\\n0 1 0 = 2\\n0 0 1 = 1\\n0 0 0 = 0\\n1 1 1 = −1\\n1 1 0 = −2\\n1 0 1 = −3\\n1 0 0 = −4\\n−24 23 22 21 20 two’s complement\\n0 0 0 1 1 = 3\\n0 0 0 1 0 = 2\\n0 0 0 0 1 = 1\\n0 0 0 0 0 = 0\\n1 1 1 1 1 = −1\\n1 1 1 1 0 = −2\\n1 1 1'),\n",
       " Document(page_content='3\\n0 1 0 = 2\\n0 0 1 = 1\\n0 0 0 = 0\\n1 1 1 = −1\\n1 1 0 = −2\\n1 0 1 = −3\\n1 0 0 = −4\\n−24 23 22 21 20 two’s complement\\n0 0 0 1 1 = 3\\n0 0 0 1 0 = 2\\n0 0 0 0 1 = 1\\n0 0 0 0 0 = 0\\n1 1 1 1 1 = −1\\n1 1 1 1 0 = −2\\n1 1 1 0 1 = −3\\n1 1 1 0 0 = −4\\nOne significant difference between unsigned numbers and two’s complement numbers is that we\\nneed to know exactly how many bits are used to represent the number, since the most significant\\nbit is treated differently than the remaining bits. Furthermore, when widening a signed two’s\\ncomplement number with more bits, the sign bit is replicated to all the new most significant bits.\\nThis process is normally called sign-extension. For the rest of the book, we will generally assume\\nthat all signed numbers are represented in two’s complement unless otherwise mentioned.\\nWhat is the largest positive number representable with N bits in two’s complement? What\\nis the largest negative number?\\nGiven a positive number x, how can you find the two’s complement representation of'),\n",
       " Document(page_content='is the largest positive number representable with N bits in two’s complement? What\\nis the largest negative number?\\nGiven a positive number x, how can you find the two’s complement representation of −x?\\nWhat is −0 in two’s complement? if x is the largest negative number representable with N bits\\nin two’s complement, what is −x?\\n3.5.3 Overflow, Underflow, and Rounding\\nOverflow occurs when a number is larger than the largest number that can be represented in a\\ngiven number of bits. Similarly, underflow occurs when a number is smaller than the smallest\\nnumber that can be represented. One common way of handling overflow or underflow is to simply\\ndrop the most significant bits of the original number, often called wrapping.\\n70CORDIC\\n25 24 23 22 21 20 2−1 2−2 2−3 2−4\\n0 0 1 0 1 1 0 1 0 0 = 11.25\\n0 1 0 1 1 0 1 0 0 = 11.25\\n1 0 1 1 0 1 0 0 = 11.25\\n0 1 1 0 1 0 0 = 3.25\\nHandling overflow and underflow by wrapping two’s complement numbers can even cause a\\npositive number to become negative, or a'),\n",
       " Document(page_content='1 0 1 1 0 1 0 0 = 11.25\\n1 0 1 1 0 1 0 0 = 11.25\\n0 1 1 0 1 0 0 = 3.25\\nHandling overflow and underflow by wrapping two’s complement numbers can even cause a\\npositive number to become negative, or a negative number to become positive.\\n−23 22 21 20 2−1 2−2 2−3 2−4 two’s complement\\n1 0 1 1 0 1 0 0 = −4.75\\n−22 21 20 2−1 2−2 2−3 2−4 two’s complement\\n0 1 1 0 1 0 0 = 3.25\\nSimilarly, when a number cannot be represented precisely in a given number of fractional bits,\\nit is necessary to apply rounding. Again, there are several common ways to round numbers. The\\nsimplestwayistojustdroptheextrafractionalbits,whichtendstoresultinnumbersthataremore\\nnegative. This method of rounding is often called rounding down or rounding to negative infinity.\\nWhen rounding down to the nearest integer, this corresponds to the floor() function, although it’s\\npossible to round to other bit positions as well.\\n0b0100.00 = 4.0 0b0100.0 = 4.0\\n0b0011.11 = 3.75 0b0011.1 = 3.5\\n0b0011.10 = 3.5 0b0011.1 = 3.5\\n0b0011.01 = 3.25'),\n",
       " Document(page_content='to the floor() function, although it’s\\npossible to round to other bit positions as well.\\n0b0100.00 = 4.0 0b0100.0 = 4.0\\n0b0011.11 = 3.75 0b0011.1 = 3.5\\n0b0011.10 = 3.5 0b0011.1 = 3.5\\n0b0011.01 = 3.25 0b0011.0 = 3.0\\nRound to\\n0b0011.00 = 3.0 0b0011.0 = 3.0\\n→ Negative →\\n0b1100.00 = −4.0 0b1100.0 = −4.0\\nInfinity\\n0b1011.11 = −4.25 0b1011.1 = −4.5\\n0b1011.10 = −4.5 0b1011.1 = −4.5\\n0b1011.01 = −4.75 0b1011.0 = −5.0\\n0b1011.00 = −5.0 0b1011.0 = −5.0\\nIt is also possible to handle rounding in other similar ways which force rounding to a more\\npositive numbers (called rounding up or rounding to positive infinity and corresponding to the\\nceil() function), to smaller absolute values (called rounding to zero and corresponding to the\\ntrunc() function), or to larger absolute values (called rounding away from zero or rounding to\\ninfinity and corresponding to the round() function)). None of these operations always minimizes\\nthe error caused by rounding, however. A better approach is called rounding to'),\n",
       " Document(page_content='from zero or rounding to\\ninfinity and corresponding to the round() function)). None of these operations always minimizes\\nthe error caused by rounding, however. A better approach is called rounding to nearest even,\\nconvergent rounding, or banker’s rounding and is implemented in the lrint() function. As you might\\nexpect, this approach to rounding always picks the nearest representable number. In addition, If\\nthere are two numbers equally distant, then the even one is always picked. An arbitrary-precision\\nnumber is even if the last digit is zero. This approach is the default handling of rounding with\\nIEEE floating point, as it not only minimizes rounding errors but also ensures that the rounding\\nerror tends to cancel out when computing sums of random numbers.\\n71CORDIC\\n0b0100.00 = 4.0 0b0100.0 = 4.0\\n0b0011.11 = 3.75 0b0100.0 = 4.0\\n0b0011.10 = 3.5 0b0011.1 = 3.5\\n0b0011.01 = 3.25 0b0011.0 = 3.0\\nRound to\\n0b0011.00 = 3.0 0b0011.0 = 3.0\\n→ Nearest →\\n0b1100.00 = −4.0 0b1100.0 ='),\n",
       " Document(page_content='= 4.0 0b0100.0 = 4.0\\n0b0011.11 = 3.75 0b0100.0 = 4.0\\n0b0011.10 = 3.5 0b0011.1 = 3.5\\n0b0011.01 = 3.25 0b0011.0 = 3.0\\nRound to\\n0b0011.00 = 3.0 0b0011.0 = 3.0\\n→ Nearest →\\n0b1100.00 = −4.0 0b1100.0 = −4.0\\nEven\\n0b1011.11 = −4.25 0b1100.0 = −4.0\\n0b1011.10 = −4.5 0b1011.1 = −4.5\\n0b1011.01 = −4.75 0b1011.0 = −5.0\\n0b1011.00 = −5.0 0b1011.0 = −5.0\\n3.5.4 Binary arithmetic\\nBinary addition is very similar to decimal addition, simply align the binary points and add digits,\\ntaking care to correctly handle bits carried from one column to the next. Note that the result of\\nadding or subtracting two N-bit numbers generally takes N+1 bits to represent correctly without\\noverflow. The added bit is always an additional most significant bit for fractional numbers\\n23 22 21 20 unsigned\\n0 1 1 = 3\\n+ 0 1 1 = 3\\n= 0 1 1 0 = 6\\n23 22 21 20 2−1 unsigned\\n1 1 1 1 = 7.5\\n+ 1 1 1 1 = 7.5\\n= 1 1 1 1 0 = 15\\nNote that since the result of subtraction can be negative, the ’extra bit’ becomes the sign-bit\\nof a two’s complement'),\n",
       " Document(page_content='1 1 0 = 6\\n23 22 21 20 2−1 unsigned\\n1 1 1 1 = 7.5\\n+ 1 1 1 1 = 7.5\\n= 1 1 1 1 0 = 15\\nNote that since the result of subtraction can be negative, the ’extra bit’ becomes the sign-bit\\nof a two’s complement number.\\n23 22 21 20 unsigned\\n0 0 1 1 = 3\\n- 0 0 1 1 = 3\\n= 0 0 0 0 = 0\\n-24 23 22 21 20 unsigned\\n0 0 1 1 = 3\\n- 1 1 1 1 = 15\\n= 1 0 1 0 0 = −12 (two’s complement)\\nMultiplication for binary numbers also works similarly to familiar decimal multiplication. In\\ngeneral, multiplying 2 N-bit numbers results in a 2*N bit result.\\n72CORDIC\\n26 25 24 23 22 21 20 two’s complement\\n1 0 0 1 = 9\\n* 1 0 0 1 = 9\\n1 0 0 1 = 9\\n0 0 0 0 = 0\\n0 0 0 0 = 0\\n+ 1 0 0 1 = 72\\n1 0 1 0 0 0 1 = 81\\nOperationsonsignednumbersaresomewhatmorecomplexbecauseofthesign-bithandlingand\\nwon’t be covered in detail. However, the observations regarding the width of the result still applies:\\nadding or subtracting two N-bit signed numbers results in an N+1-bit result, and Multiplying two\\nN-bit signed numbers results in an 2*N-bit result.\\nWhat'),\n",
       " Document(page_content='the width of the result still applies:\\nadding or subtracting two N-bit signed numbers results in an N+1-bit result, and Multiplying two\\nN-bit signed numbers results in an 2*N-bit result.\\nWhat about division? Can the number of bits necessary to exactly represent the result a\\ndivision operation of 2 N-bit numbers be computed?\\n3.5.5 Representing Arbitrary Precision Integers in C and C++\\nAccording to the C99 language standard, the precision of many standard types, such as int and\\nlong are implementation defined. Although many programs can be written with these types in a\\nway that does not have implementation-defined behavior, many cannot. One small improvement\\nis the inttypes.h header in C99, which defines the types int8 t,int16 t,int32 t, and int64 t repre-\\nsenting signed numbers of a given width and the corresponding types uint8 t,uint16 t,uint32 t, and\\nuint64 t representing unsigned numbers. Although these types are defined to have exactly the\\ngiven bitwidths, they can still be somewhat'),\n",
       " Document(page_content='and the corresponding types uint8 t,uint16 t,uint32 t, and\\nuint64 t representing unsigned numbers. Although these types are defined to have exactly the\\ngiven bitwidths, they can still be somewhat awkward to use. For instance, even relatively simple\\nprograms like the code below can have unexpected behavior.\\n#include ”inttypes.h”\\nuint16 t a =0x4000;\\nuint16 t b = 0x4000;\\n// Danger! p depends on sizeof(int)\\nuint32 t p = a∗b;\\nAlthough the values of a and b can be represented in 16 bits and their product (0x10000000) can\\nbe represented exactly in 32 bits, the behavior of this code by the conversion rules in C99 is to first\\nconvert a and b to type int, compute an integer result, and then to extend the result to 32 bits.\\nAlthough uncommon, it is correct for a C99 compiler to only have integers with only 16 bits of\\nprecision. Furthermore,theC99standardonlydefines4bitwidthsforintegernumbers,whileFPGA\\nsystems often use a wide variety of bitwidths for arithmetic. Also, printing these datatypes'),\n",
       " Document(page_content='with only 16 bits of\\nprecision. Furthermore,theC99standardonlydefines4bitwidthsforintegernumbers,whileFPGA\\nsystems often use a wide variety of bitwidths for arithmetic. Also, printing these datatypes using\\nprintf() is awkward, requiring the use of additional macros to write portable code. The situation is\\neven worse if we consider a fixed-point arithmetic example. In the code below, we consider a and\\nb to be fixed point numbers, and perform normalization correctly to generate a result in the same\\nformat.\\n#include ”inttypes.h”\\n// 4.0 represented with 12 fractional bits\\nuint16 t a =0x4000;\\n// 4.0 represented with 12 fractional bits.\\n73CORDIC\\nuint16 t b = 0x4000;\\n// Danger! p depends on sizeof(int)\\nuint32 t p = (a∗b) >> 12;\\nThe correct code in both cases requires casting the input variables to the width of the result\\nbefore multiplying.\\n#include ”inttypes.h”\\nuint16 t a = 0x4000;\\nuint16 t b = 0x4000;\\n// p is assigned to 0x10000000\\nuint32 t p = (uint32 t) a∗(uint32 t) b;\\n#include'),\n",
       " Document(page_content='variables to the width of the result\\nbefore multiplying.\\n#include ”inttypes.h”\\nuint16 t a = 0x4000;\\nuint16 t b = 0x4000;\\n// p is assigned to 0x10000000\\nuint32 t p = (uint32 t) a∗(uint32 t) b;\\n#include ”inttypes.h”\\n// 4.0 represented with 12 fractional bits.\\nuint16 t a =0x4000;\\n// 4.0 represented with 12 fractional bits.\\nuint16 t b = 0x4000;\\n// p assigned to 16.0 represented with 12 fractional bits\\nuint32 t p = ( (uint32 t) a∗(uint32 t) b ) >> 12;\\nWhen using integers to represent fixed-point numbers, it is very important to document the\\nfixed point format used, so that normalization can be performed correctly after multiplication.\\nUsuallythisisdescribedusing”Q”formatsthatgivethenumberoffractionalbits. Forinstance,\\n”Q15” format uses 15 fractional bits and usually applies to 16 bit signed variables. Such a\\nvariable has values in the interval [−1,1). Similarly ”Q31” format uses 31 fractional bits.\\nFor these reasons, it’s usually preferable to use C++ and the Vivado(cid:13)R HLS template'),\n",
       " Document(page_content='variables. Such a\\nvariable has values in the interval [−1,1). Similarly ”Q31” format uses 31 fractional bits.\\nFor these reasons, it’s usually preferable to use C++ and the Vivado(cid:13)R HLS template classes\\nap int<>, ap uint<>, ap fixed<>, and ap ufixed<> to represent arbitrary precision numbers. The\\nap int<> and ap uint<> template classes require a single integer template parameter that defines\\ntheir width. Arithmetic functions generally produce a result that is wide enough to contain a\\ncorrect result, following the rules in section 3.5.4. Only if the result is assigned to a narrower\\nbitwidth does overflow or underflow occur.\\n#include ”ap int.h”\\nap uint<15> a =0x4000;\\nap uint<15> b = 0x4000;\\n// p is assigned to 0x10000000.\\nap uint<30> p = a∗b;\\nThe ap fixed<> and ap ufixed<> template classes are similar, except that they require two\\ninteger template arguments that define the overall width (the total number of bits) and the number\\nof integer bits.\\n#include ”ap fixed.h”\\n// 4.0'),\n",
       " Document(page_content='classes are similar, except that they require two\\ninteger template arguments that define the overall width (the total number of bits) and the number\\nof integer bits.\\n#include ”ap fixed.h”\\n// 4.0 represented with 12 fractional bits.\\nap ufixed<15,12> a = 4.0;\\n// 4.0 represented with 12 fractional bits.\\nap ufixed<15,12> b = 4.0;\\n74CORDIC\\n// p is assigned to 16.0 represented with 12 fractional bits\\nap ufixed<18,12> p = a∗b;\\nNote that the ap fixed<> and ap ufixed<> template classes require the overall width of\\nthe number to be positive, but the number of integer bits can be arbitrary. In particular, the\\nnumber of integer bits can be 0 (indicating a number that is purely fractional) or can be the\\nsame as the overall width (indicating a number that has no fractional part). However, the\\nnumber of integer bits can also be negative or greater than the overall width! What do such\\nformats describe? What are the largest and smallest numbers that can be represented by an\\nap fixed<8,−3>? ap'),\n",
       " Document(page_content='of integer bits can also be negative or greater than the overall width! What do such\\nformats describe? What are the largest and smallest numbers that can be represented by an\\nap fixed<8,−3>? ap fixed<8,12>?\\n3.5.6 Floating Point\\nVivado(cid:13)R HLScanalsosynthesizefloatingpointcalculations. Floatingpointnumbersprovidealarge\\namountofprecision,butthiscomesatacost; itrequiressignificantamountofcomputationwhichin\\nturntranslatestoalargeamountofresourceusageandmanycyclesoflatency. Thus, floatingpoint\\nnumbers should be avoided unless absolutely necessary as dictated by the accuracy requirements\\napplication. In fact, the primary goal of this chapter is to allow the reader to understand how\\nto effectively move from floating point to fixed point representations. Unfortunately, this is often\\na non-trivial task and there are not many good standard methods to automatically perform this\\ntranslation. This is partially due to the fact that moving to fixed point will reduce the accuracy of\\nthe'),\n",
       " Document(page_content='non-trivial task and there are not many good standard methods to automatically perform this\\ntranslation. This is partially due to the fact that moving to fixed point will reduce the accuracy of\\nthe application and this tradeoff is best left to the designer.\\nThestandardtechniqueforhigh-levelsynthesisstartswithafloatingpointrepresentationduring\\nthe initial development of the application. This allows the designer to focus on getting a function-\\nally correct implementation. Once that is achieved, then she can move optimizing the number\\nrepresentation in order to reduce the resource usage and/or increase the performance.\\nChange all of the variables in the CORDIC from float to int. How does this effect the\\nresource usage? How does it change the latency? How about the throughput? Does the\\naccuracy change?\\n3.6 Further Optimizations\\nIn this section, we provide some brief thoughts and suggestions on the best way to optimize the\\nCORDIC function. We focus on how the different optimizations change'),\n",
       " Document(page_content='change?\\n3.6 Further Optimizations\\nIn this section, we provide some brief thoughts and suggestions on the best way to optimize the\\nCORDIC function. We focus on how the different optimizations change the precision of the result\\nwhile providing the ability tradeoff between throughput, precision, and area.\\nUltimately, CORDIC produces an approximation. The error on that approximation generally\\ndecreases as the number of iterations increases. This corresponds to the number of times that we\\nexecutetheforloopinthecordicfunction, whichissetbyNUM ITERATIONS. Evenifweperforma\\nvery large number of iterations, we may still have an approximation. One reason for this is that we\\nmay approach but never exactly match the desired target angle. We can, however, tune precision\\nby choosing to perform greater or fewer iterations. All that needs to change in the algorithm is to\\nmodify the value of NUM ITERATIONS. The choice of NUM ITERATIONS depends on the number\\nof digits of precision required by application'),\n",
       " Document(page_content='fewer iterations. All that needs to change in the algorithm is to\\nmodify the value of NUM ITERATIONS. The choice of NUM ITERATIONS depends on the number\\nof digits of precision required by application using this CORDIC core.\\n75CORDIC\\nHow do the area, throughput, and precision of the sine and cosine results change as you\\nvary the data type?\\nHow does the constant NUM ITERATIONS affect the area, throughput, and precision? How\\ndoesthisaffecttheinitialvaluesofcurrent cosandcurrent sin? Doyouneedtomodifythearray\\ncordic phase? Can you optimize the data types depending on the value of NUM ITERATIONS?\\nThe computations in the for loop occupy most of the overall time. How do you best\\nperform code transforms and/or use pragmas to optimize it?\\nSetting the variable sigma can be efficient in hardware using a two input multiplexer. Can\\nyou transform the code so that the high level synthesis tool implements it in this manner?\\nThe current code assumes that the given angle is between ±90◦. Can you add'),\n",
       " Document(page_content='using a two input multiplexer. Can\\nyou transform the code so that the high level synthesis tool implements it in this manner?\\nThe current code assumes that the given angle is between ±90◦. Can you add code to allow\\nit to handle any angle between ±180◦?\\n3.7 Conclusion\\nIn this chapter, we looked the Coordinate Rotation DIgital Computer (CORDIC) method for\\ncalculating trigonometric and hyperbolic functions based on vector rotations. We start with a\\nbackground on the computation being performed by the CORDIC method. In particular, we focus\\non how to use the CORDIC method to calculate the sine and cosine values for a given angle.\\nAdditionally, we discuss how the same CORDIC method can be used to determine the amplitude\\nand phase of a given complex number.\\nAfter this, we focus on the optimizations that can be done on the CORDIC method. Since it\\nis an iterative method, there are fundamental tradeoffs between the number of iterations that are\\nperformed and the precision and accuracy of the'),\n",
       " Document(page_content='that can be done on the CORDIC method. Since it\\nis an iterative method, there are fundamental tradeoffs between the number of iterations that are\\nperformed and the precision and accuracy of the resulting computation. We discuss how to reduce\\nthe precision/accuracy and get savings in FPGA resource usage and increases in performance.\\nWe introduce the notion of using custom arbitrary data types for the variables in our cordic\\nfunction. This provides another method to reduce the latency, increase the throughput, and min-\\nimize the area while changing the precision of the intermediate and final results. Vivado(cid:13)R HLS\\nprovides a method to specifically generate a large number of data types. We provide a background\\non number representation and introduce these custom data types.\\nIn general, there is a complex relationship between precision, resource utilization, and perfor-\\nmance. We touch on some of these tradeoffs, and provide some insights on how to best optimize\\nthe cordic function.'),\n",
       " Document(page_content='there is a complex relationship between precision, resource utilization, and perfor-\\nmance. We touch on some of these tradeoffs, and provide some insights on how to best optimize\\nthe cordic function. We leave many of the optimization as well as the analysis of these tradeoffs, as\\nan exercise to the reader. The CORDIC method is an integral part of the Phase Detector project\\ndescribed in Chapter ?? – a lab provided in the Appendix.\\n76Chapter 4\\nDiscrete Fourier Transform\\nTheDiscreteFourierTransform(DFT)playsafundamentalroleindigitalsignalprocessingsystems.\\nIt is a method to change a discrete signal in the time domain to the same signal in the frequency\\ndomain. By describing the signal as the sum of sinusoids, we can more easily compute some\\nfunctions on the signal, e.g., filtering and other linear time invariant functions. Therefore, it plays\\nan important role in many wireless communications, image processing, and other digital signal\\nprocessing'),\n",
       " Document(page_content='on the signal, e.g., filtering and other linear time invariant functions. Therefore, it plays\\nan important role in many wireless communications, image processing, and other digital signal\\nprocessing applications.\\nThischapterprovidesanintroductiontotheDFTwithafocusonitsoptimizationforanFPGA\\nimplementation. At its core, the DFT performs a matrix-vector multiplication where the matrix is\\na fixed set of coefficients. The initial optimizations in Chapter 4.6 treat the DFT operation as a\\nsimplified matrix-vector multiplication. Then, Chapter 4.6 introduces a complete implementation\\nof the DFT in Vivado(cid:13)R HLS code. Additionally, we describe how to best optimize the DFT\\ncomputation to increase the throughput. We focus our optimization efforts on array partitioning\\noptimizations in Chapter 4.5.\\nThereisalotofmathinthefirsttwosectionsofthischapter. Thismayseemsuperfluous, butis\\nnecessary to fully comprehend the code restructuring optimizations, particularly for understanding\\nthe'),\n",
       " Document(page_content='Chapter 4.5.\\nThereisalotofmathinthefirsttwosectionsofthischapter. Thismayseemsuperfluous, butis\\nnecessary to fully comprehend the code restructuring optimizations, particularly for understanding\\nthe computational symmetries that are utilized by the Fast Fourier Transform (FFT) in the next\\nchapter. That being said, if you are more interested in the HLS optimizations, you can skip to\\nChapter 4.6.\\n4.1 Fourier Series\\nIn order to explain the discrete Fourier transform, we must first understand the Fourier series.\\nThe Fourier series provides an alternative way to look at a real valued, continuous, periodic signal\\nwhere the signal runs over one period from −π to π. The seminal result from Jean Baptiste Joseph\\nFourier states that any continuous, periodic signal over a period of 2π can be represented by a sum\\nof cosines and sines with a period of 2π. Formally, the Fourier Series is given as\\nf(t) ∼ a0 +a cos(t)+a cos(2t)+a cos(3t)+...\\n2 1 2 3\\n+b sin(t)+b sin(2t)+b sin(3t)+...\\n1 2 3\\n∞'),\n",
       " Document(page_content='be represented by a sum\\nof cosines and sines with a period of 2π. Formally, the Fourier Series is given as\\nf(t) ∼ a0 +a cos(t)+a cos(2t)+a cos(3t)+...\\n2 1 2 3\\n+b sin(t)+b sin(2t)+b sin(3t)+...\\n1 2 3\\n∞ (4.1)\\n(cid:88)\\n∼ a0 + (a cos(nt)+b sin(nt))\\n2 n n\\nn=1\\nwhere the coefficients a ,a ,... and b ,b ,... are computed as\\n0 1 1 2\\n77DISCRETE FOURIER TRANSFORM\\na = 1 (cid:82)π f(t)dt\\n0 π −π\\na = 1 (cid:82)π f(t)cos(nt)dt (4.2)\\nn π −π\\nb = 1 (cid:82)π f(t)sin(nt)dt\\nn π −π\\nThere are several things to note. First, the coefficients a ,a ,a ,...,b ,b ,... in Equation 4.2\\n0 1 2 1 2\\nare called the Fourier coefficients. The coefficient a is often called the direct current (DC) term\\n0\\n(a reference to early electrical current analysis), the n = 1 frequency is called the fundamental,\\nwhile the other frequencies (n ≥ 2) are called higher harmonics. The notions of fundamental and\\nharmonicfrequenciesoriginate fromacousticsandmusic. Second, the functionf, andthecos()and\\nsin() functions all have a period of 2π;'),\n",
       " Document(page_content='(n ≥ 2) are called higher harmonics. The notions of fundamental and\\nharmonicfrequenciesoriginate fromacousticsandmusic. Second, the functionf, andthecos()and\\nsin() functions all have a period of 2π; changing this period to some other value is straightforward\\nas we will show shortly. The DC value a is equivalent to the coefficient of cos(0·t) = 1, hence the\\n0\\nuse of symbol a. The b value is not needed since sin(0·t) = 0. Finally, the relation between the\\n0\\nfunction f and its Fourier series is approximate in some cases when there are discontinuities in f\\n(known as Gibbs phenomenon). This is a minor issue, and only relevant for the Fourier series, and\\nnot other Fourier Transforms. Therefore, going forward we will disregard this “approximation” (∼)\\nfor “equality” (=).\\nRepresenting functions that are periodic on something other than π requires a simple change\\nin variables. Assume a function is periodic on [−L,L] rather than [−π,π]. Let\\nπt(cid:48)\\nt ≡ (4.3)\\nL\\nand\\nπdt(cid:48)\\ndt ='),\n",
       " Document(page_content='functions that are periodic on something other than π requires a simple change\\nin variables. Assume a function is periodic on [−L,L] rather than [−π,π]. Let\\nπt(cid:48)\\nt ≡ (4.3)\\nL\\nand\\nπdt(cid:48)\\ndt = (4.4)\\nL\\nwhich is a simple linear translation from the old [−π,π] interval to the desired [−L,L] interval.\\nSolving for t(cid:48) and substituting t(cid:48) = Lt into Equation 4.1 gives\\nπ\\nf(t(cid:48)) =\\na\\n0\\n+(cid:88)∞\\n(a\\ncos(nπt(cid:48)\\n)+b\\nsin(nπt(cid:48)\\n)) (4.5)\\nn n\\n2 L L\\nn=1\\nSolving for the a and b coefficients is similar:\\na = 1 (cid:82)L f(t(cid:48))dt(cid:48)\\n0 L −L\\na = 1 (cid:82)L f(t(cid:48))cos(nπt(cid:48) )dt(cid:48) (4.6)\\nn L −L L\\nb = 1 (cid:82)L f(t(cid:48))sin(nπt(cid:48) )dt(cid:48)\\nn L −L L\\nWe can use Euler’s formula ejnt = cos(nt)+jsin(nt) to give a more concise formulation\\n∞\\n(cid:88)\\nf(t) = c ejnt. (4.7)\\nn\\nn=−∞\\nIn this case, the Fourier coefficients c are a complex exponential given by\\nn\\n1 (cid:90) π\\nc = f(t)e−jntdt (4.8)\\nn\\n2π\\n−π\\nwhich assumes that f(t) is a periodic'),\n",
       " Document(page_content='= c ejnt. (4.7)\\nn\\nn=−∞\\nIn this case, the Fourier coefficients c are a complex exponential given by\\nn\\n1 (cid:90) π\\nc = f(t)e−jntdt (4.8)\\nn\\n2π\\n−π\\nwhich assumes that f(t) is a periodic function with a period of 2π, i.e., this equation is equivalent\\nto Equation 4.1.\\n78DISCRETE FOURIER TRANSFORM\\nThe Fourier coefficients a , b , and c are related as\\nn n n\\na = c +c for n = 0,1,2,...\\nn n −n\\nb = j(c −c ) for n = 1,2,...\\nn n −n\\n\\uf8f1 1(a −jb ) n > 0 (4.9)\\n\\uf8f2 2 n n\\nc = 1a n = 0\\nn 2 0\\n\\uf8f3 1(a +jb ) n < 0\\n2 −n −n\\nNote that the equations for deriving a , b , and c introduce the notion of a “negative” fre-\\nn n n\\nquency. While this physically does not make much sense, mathematically we can think about as\\na “negative” rotation on the complex plane. A “positive” frequency indicates that the complex\\nnumber rotates in a counterclockwise direction in the complex plane. A negative frequency simply\\nmeans that we are rotating in the opposite (clockwise) direction on the complex plane.\\nThis idea is further'),\n",
       " Document(page_content='rotates in a counterclockwise direction in the complex plane. A negative frequency simply\\nmeans that we are rotating in the opposite (clockwise) direction on the complex plane.\\nThis idea is further illustrated by the relationship of cosine, sine, and the complex exponential.\\nCosine can be viewed as the real part of the complex exponential and it can also be derived as the\\nsum of two complex exponentials – one with a positive frequency and the other with a negative\\nfrequency as shown in Equation 4.10.\\nejx+e−jx\\ncos(x) = Re{ejx} = (4.10)\\n2\\nThe relationship between sine and the complex exponential is similar as shown in Equation 4.11.\\nHere we subtract the negative frequency and divide by 2j.\\nejx−e−jx\\nsin(x) = Im{ejx} = (4.11)\\n2j\\nBoth of these relationships can be visualized as vectors in the complex plane as shown in Figure\\n4.1. Part a) shows the cosine derivation. Here we add the two complex vectors ejx and e−jx. Note\\nthat the sum of these two vectors results in a vector on the real'),\n",
       " Document(page_content='the complex plane as shown in Figure\\n4.1. Part a) shows the cosine derivation. Here we add the two complex vectors ejx and e−jx. Note\\nthat the sum of these two vectors results in a vector on the real (in-phase or I) axis. The magnitude\\nof that vector is 2cos(x). Thus, by dividing the sum of these two complex exponentials by 2, we\\nget the value cos(x) as shown in Equation 4.10. Figure 4.1 b) shows the similar derivation for\\nsine. Here we are adding the complex vectors ejx and −e−jx. The result of this is a vector on the\\nimaginary (quadrature or Q) axis with a magnitude of 2sin(x). Therefore, we must divide by 2j\\nin order to get sin(x). Therefore, this validates the relationship as described in Equation 4.11.\\n4.2 DFT Background\\nThe previous section provided a mathematical foundation for the Fourier series, which works on\\nsignals that are continuous and periodic. The Discrete Fourier Transform requires discrete periodic\\nsignals. The DFT converts a finite number of equally spaced samples'),\n",
       " Document(page_content='Fourier series, which works on\\nsignals that are continuous and periodic. The Discrete Fourier Transform requires discrete periodic\\nsignals. The DFT converts a finite number of equally spaced samples into a finite number of\\ncomplex sinusoids. In other words, it converts a sampled function from one domain (most often\\nthe time domain) to the frequency domain. The frequencies of the complex sinusoids are integer\\nmultiples of the fundamental frequency which is defined as the frequency related to the sampling\\nperiod of the input function. Perhaps the most important consequence of the discrete and periodic\\nsignal is that it can be represented by a finite set of numbers. Thus, a digital system can be used\\nto implement the DFT.\\nThe DFT works on input functions that uses both real and complex numbers. Intuitively, it is\\neasier to first understand how the real DFT works, so we will ignore complex numbers for the time\\nbeing and start with real signals in order to gain ease into the mathematics a'),\n",
       " Document(page_content='Intuitively, it is\\neasier to first understand how the real DFT works, so we will ignore complex numbers for the time\\nbeing and start with real signals in order to gain ease into the mathematics a bit.\\n79DISCRETE FOURIER TRANSFORM\\na) Q b) Q\\n2j sin(x)\\n-jx\\n-e\\nj sin(x)\\nejx e-jx\\njx\\ne\\nx I x I\\ncos(x) 2 cos(x)\\nFigure4.1: Avisualizationoftherelationshipbetweenthecosine,sine,andthecomplexexponential.\\nPart a) shows the sum of two complex vectors, ejx and e−jx. The result of this summation lands\\nexactly on the real axis with the value 2cos(x). Part b) shows a similar summation except this\\ntime summing the vectors ejx and −e−jx. This summation lands on the imaginary axis with the\\nvalue 2sin(x).\\nA quick note on terminology: We use lower case function variables to denote signals in the\\ntime domain. Upper case function variables are signals in the frequency domain. We use ()\\nfor continuous functions and [] for discrete functions. For example, f() is a continuous time\\ndomain function and F() is its'),\n",
       " Document(page_content='case function variables are signals in the frequency domain. We use ()\\nfor continuous functions and [] for discrete functions. For example, f() is a continuous time\\ndomain function and F() is its continuous frequency domain representation. Similarly g[] is a\\ndiscrete function in the time domain and G[] is that function transformed into the frequency\\ndomain.\\nTo start consider Figure 4.2. The figure shows on the left a real valued time domain signal g[]\\nwithN samplesorpointsrunningfrom0toN−1. TheDFTisperformedresultinginthefrequency\\ndomain signals corresponding to the cosine and sine amplitudes for the various frequencies. These\\ncan be viewed as a complex number with the cosine amplitudes corresponding to the real value\\nof the complex number and the sine amplitudes providing the imaginary portion of the complex\\nnumber. There are N/2+1 cosine (real) and N/2+1 sine (imaginary) values. We will call this\\nresulting complex valued frequency domain function G[]. Note that the number of samples'),\n",
       " Document(page_content='of the complex\\nnumber. There are N/2+1 cosine (real) and N/2+1 sine (imaginary) values. We will call this\\nresulting complex valued frequency domain function G[]. Note that the number of samples in\\nfrequency domain (N/2+1) is due to the fact that we are considering a real valued time domain\\nsignal; a complex valued time domain signal results in a frequency domain signal with N samples.\\nAn N point DFT can be determined through a N ×N matrix multiplied by a vector of size N,\\n80DISCRETE FOURIER TRANSFORM\\nTime Domain Frequency Domain\\ng[ ] Real G[ ]: Cosine Amplitudes\\n0 N-1 0 N/2\\nImaginary G[ ]: Sine Amplitudes\\n0 N/2\\nFigure 4.2: A real valued discrete function g[] in the time domain with N points has a frequency\\ndomain representation with N/2 + 1 samples. Each of these frequency domain samples has one\\ncosine and one sine amplitude value. Collectively these two amplitude values can be represented\\nby a complex number with the cosine amplitude representing the real part and the sine'),\n",
       " Document(page_content='samples has one\\ncosine and one sine amplitude value. Collectively these two amplitude values can be represented\\nby a complex number with the cosine amplitude representing the real part and the sine amplitude\\nthe imaginary part.\\nG = S ·g where\\n\\uf8ee \\uf8f9\\n1 1 1 ··· 1\\n\\uf8ef1 s s2 ··· sN−1 \\uf8fa\\n\\uf8ef \\uf8fa\\n\\uf8ef1 s2 s4 ··· s2(N−1) \\uf8fa\\nS = \\uf8ef \\uf8ef1 s3 s6 ··· s3(N−1) \\uf8fa \\uf8fa (4.12)\\n\\uf8ef \\uf8fa\\n\\uf8ef \\uf8f0. .\\n.\\n. .\\n.\\n. .\\n.\\n... . .\\n.\\n\\uf8fa\\n\\uf8fb\\n1 sN−1 s2(N−1) ··· s(N−1)(N−1)\\n−j2π\\nand s = e N . Thus the samples in frequency domain are derived as\\nN−1\\n(cid:88)\\nG[k] = g[n]skn for k = 0,...,N −1 (4.13)\\nn=0\\nFigure 4.3 provides a visualization of the DFT coefficients for an 8 point DFT operation. The\\neight frequency domain samples are derived by multiplying the 8 time domain samples with the\\ncorrespondingrowsoftheS matrix. Row0oftheS matrixcorrespondstotheDCcomponentwhich\\nis proportional to the average of the time domain samples. Multiplying Row 1 of the S matrix with\\ng provides the cosine and sine amplitudes values for when there is one rotation around the'),\n",
       " Document(page_content='proportional to the average of the time domain samples. Multiplying Row 1 of the S matrix with\\ng provides the cosine and sine amplitudes values for when there is one rotation around the unit\\ncircle. Since this is an 8 point DFT, this means that each phasor is offset by 45◦. Performing eight\\n45◦ rotations does one full rotation around the unit circle. Row 2 is similar except is performs two\\nrotations around the unit circle, i.e., each rotation is 90◦. This is a higher frequency. Row 3 does\\nthree rotations; Row 4 four rotations and so on. Each of these row times column multiplications\\ngives the appropriate frequency domain sample.\\nNotice that the S matrix is diagonally symmetric, that is S[i][j] = S[j][i]. In addition, S[i][j] =\\nsi∗sj = s(i+j). There is also interesting symmetry around Row 4. The phasors in Rows 3 and 5 are\\ncomplexconjugatesofeachother,i.e.,S[3][j] = S[5][j]∗. Similarly,Rows2and6(S[2][j] = S[6][j]∗),\\nand Rows 1 and 7 (S[1][j] = S[7][j]∗) are each related by the complex'),\n",
       " Document(page_content='4. The phasors in Rows 3 and 5 are\\ncomplexconjugatesofeachother,i.e.,S[3][j] = S[5][j]∗. Similarly,Rows2and6(S[2][j] = S[6][j]∗),\\nand Rows 1 and 7 (S[1][j] = S[7][j]∗) are each related by the complex conjugate operation. It is\\nfor this reason that the DFT of a real valued input signal with N samples has only N/2+1 cosine\\nand sine values in the frequency domain. The remaining N/2 frequency domain values provide\\nredundant information so they are not needed. However, this is not true when the input signal is\\ncomplex. In this case, the frequency domain will have N +1 cosine and sine values.\\n81DISCRETE FOURIER TRANSFORM\\nG[0] g[0]\\nG[1] g[1]\\nG[2] g[2]\\nG[3] g[3]\\n=\\nG[4] g[4]\\nG[5] g[5]\\nG[6] g[6]\\nG[7] g[7]\\nFigure 4.3: The elements of the S shown as a complex vectors.\\n#define SIZE 8\\ntypedef int BaseType;\\nvoid matrix vector(BaseType M[SIZE][SIZE], BaseType V In[SIZE], BaseType V Out[SIZE]) {\\nBaseType i, j;\\ndata loop:\\nfor (i = 0; i < SIZE; i++) {\\nBaseType sum = 0;\\ndot product loop:\\nfor (j = 0; j <'),\n",
       " Document(page_content='matrix vector(BaseType M[SIZE][SIZE], BaseType V In[SIZE], BaseType V Out[SIZE]) {\\nBaseType i, j;\\ndata loop:\\nfor (i = 0; i < SIZE; i++) {\\nBaseType sum = 0;\\ndot product loop:\\nfor (j = 0; j < SIZE; j++) {\\nsum += V In[j] ∗ M[i][j];\\n}\\nV Out[i] = sum;\\n}\\n}\\nFigure 4.4: Simple code implementing a matrix-vector multiplication.\\n4.3 Matrix-Vector Multiplication Optimizations\\nMatrix-vector multiplication is the core computation of a DFT. The input time domain vector is\\nmultiplied by a matrix with fixed special values. The result is a vector that corresponds to the\\nfrequency domain representation of the input time domain signal.\\nIn this section, we look at the hardware implementation of matrix-vector multiplication. We\\nbreak this operation down into its most basic form (see Figure 4.4). This allows us to better focus\\nthe discussion on the optimizations rather than deal with all the complexities of using functionally\\ncorrect DFT code. We will build a DFT core in the next section.\\nThe code in Figure'),\n",
       " Document(page_content='to better focus\\nthe discussion on the optimizations rather than deal with all the complexities of using functionally\\ncorrect DFT code. We will build a DFT core in the next section.\\nThe code in Figure 4.4 provides an initial starting point for synthesizing this operation into\\nhardware. We use a custom data type called BaseType that is currently mapped as a float.\\nThis may seem superfluous at the time, but this will allow us in the future to easily experiment\\nwith different number representations for our variables (e.g., signed or unsigned fixed point with\\ndifferent precision). The matrix vector function has three arguments. The first two arguments\\n82DISCRETE FOURIER TRANSFORM\\nM\\nV_Out\\nV_In * +\\nV in[i]\\nV out[i]\\nLatency = 4*SIZE*SIZE\\nFigure 4.5: A possible implementation of matrix-vector multiplication from the code in Figure 4.4.\\nBaseType M[SIZE][SIZE] and BaseType V In[SIZE] are the input matrix and vector to be multiplied.\\nThe third argument BaseType V Out[SIZE] is the resultant vector.'),\n",
       " Document(page_content='from the code in Figure 4.4.\\nBaseType M[SIZE][SIZE] and BaseType V In[SIZE] are the input matrix and vector to be multiplied.\\nThe third argument BaseType V Out[SIZE] is the resultant vector. By setting M = S and V In to a\\nsampled time domain signal, the V Out will contain the DFT. SIZE is a constant that determines\\nthe number of samples in the input signal and correspondingly the size of the DFT.\\nThe algorithm itself is simply a nested for loop. The inner loop (dot product loop) computes\\nthe DFT coefficients starting from 0 and going to SIZE − 1. However, this relatively simple code\\nhas many design choices that can be performed when mapping to hardware.\\nWhenever you perform HLS, you should think about the architecture that you wish to synthe-\\nsize. Memory organization is one of the more important decisions. The question boils down to\\nwhere do you store the data from your code? There are a number of options when mapping vari-\\nables to hardware. The variable could simply be a set of'),\n",
       " Document(page_content='important decisions. The question boils down to\\nwhere do you store the data from your code? There are a number of options when mapping vari-\\nables to hardware. The variable could simply be a set of wires (if its value never needs saved across\\na cycle), a register, RAM or FIFO. All of these options provide tradeoffs between performance and\\narea.\\nAnother major factor is the amount of parallelism that is available within the code. Purely\\nsequential code has few options for implementation. On the other hand, code with a significant\\namount of parallelism has implementation options that range from purely sequentially to fully\\nparallel. These options obviously have different area and performance. We will look at how both\\nmemory configurations and parallelism effect the hardware implementation for the matrix-vector\\nimplementation of the DFT.\\nFigure 4.5 shows a sequential architecture for matrix-vector multiplication with one multiply\\nand one addition operator. Logic is created to access the V'),\n",
       " Document(page_content='the matrix-vector\\nimplementation of the DFT.\\nFigure 4.5 shows a sequential architecture for matrix-vector multiplication with one multiply\\nand one addition operator. Logic is created to access the V In and M arrays which are stored in\\nBRAMs. Each element of V Out is computed and stored into the BRAM. This architecture is\\nessentially what will result from synthesizing the code from Figure 4.4 with no directives. It does\\nnot consume a lot of area, but the task latency and task interval are relatively large.\\n4.4 Pipelining and Parallelism\\nThere is substantial opportunity to exploit parallelism in the matrix-multiplication example. We\\nstart by focusing on the inner loop. The expression sum += V In[j] ∗ M[i][j]; is executed in each\\n83DISCRETE FOURIER TRANSFORM\\n#define SIZE 8\\ntypedef int BaseType;\\nvoid matrix vector(BaseType M[SIZE][SIZE], BaseType V In[SIZE], BaseType V Out[SIZE]) {\\nBaseType i, j;\\ndata loop:\\nfor (i = 0; i < SIZE; i++) {\\nBaseType sum = 0;\\nV Out[i] = V In[0] ∗ M[i][0] + V'),\n",
       " Document(page_content='matrix vector(BaseType M[SIZE][SIZE], BaseType V In[SIZE], BaseType V Out[SIZE]) {\\nBaseType i, j;\\ndata loop:\\nfor (i = 0; i < SIZE; i++) {\\nBaseType sum = 0;\\nV Out[i] = V In[0] ∗ M[i][0] + V In[1] ∗ M[i][1] + V In[2] ∗ M[i][2] +\\nV In[3] ∗ M[i][3] + V In[4] ∗ M[i][4] + V In[5] ∗ M[i][5] +\\nV In[6] ∗ M[i][6] + V In[7] ∗ M[i][7];\\n}\\n}\\nFigure 4.6: The matrix-vector multiplication example with a manually unrolled inner loop.\\niteration of the loop. The variable sum, which is keeping a running tally of the multiplications,\\nis being reused in each iteration and takes on a new value. This inner loop can be rewritten as\\nshown Figure 4.6. In this case, the sum variable has been completely eliminated and replaced with\\nmultiple intermediate values in the larger expression.\\nLoop unrolling is performed automatically by Vivado(cid:13)R HLS in a pipelined context. Loop\\nunrollingcanalsoberequestedbyusing#pragma HLS unrollortheequivalentdirectiveoutside\\nof a pipelined context.\\nIt should be clear that the new'),\n",
       " Document(page_content='by Vivado(cid:13)R HLS in a pipelined context. Loop\\nunrollingcanalsoberequestedbyusing#pragma HLS unrollortheequivalentdirectiveoutside\\nof a pipelined context.\\nIt should be clear that the new expression replacing the inner loop has significant amount of\\nparallelism. Each one of the multiplications can be performed simultaneously, and the summation\\ncan be performed using an adder tree. The data flow graph of this computation is shown in Figure\\n4.7.\\nIf we wish to achieve the minimum task latency for the expression resulting from the unrolled\\ninner loop, all eight of the multiplications should be executed in parallel. Assuming that the\\nmultiplication has a latency of 3 cycles and addition has a latency of 1 cycle, then all of the\\nV In[j] ∗ M[i][j] operations are completed by the third time step. The summation of these eight\\nintermediate results using an adder tree takes log8 = 3 cycles. Hence, the body of data loop now\\nhas a latency of 6 cycles for each iteration and requires 8'),\n",
       " Document(page_content='time step. The summation of these eight\\nintermediate results using an adder tree takes log8 = 3 cycles. Hence, the body of data loop now\\nhas a latency of 6 cycles for each iteration and requires 8 multipliers and 7 adders. This behavior\\nis shown in the left side of Figure 4.8. Note that the adders could be reused across Cycle 4-6,\\nwhich would reduce the number of adders to 4. However, adders are typically not shared when\\ntargeting FPGAs since an adder and a multiplexer require the same amount of FPGA resources\\n(approximately 1 LUT per bit for a 2-input operator).\\nIf we are not willing to use 8 multipliers, there is an opportunity to reduce resource usage\\nin exchange for increasing the number of cycles to execute the function. For example, using 4\\nmultipliers would result in a latency of 6 cycles for the multiplication of the eight V In[j] ∗ M[i][j]\\noperations, and an overall latency of 9 cycles to finish the body of data loop. This behavior is\\nshown in the right side of Figure 4.8. You'),\n",
       " Document(page_content='cycles for the multiplication of the eight V In[j] ∗ M[i][j]\\noperations, and an overall latency of 9 cycles to finish the body of data loop. This behavior is\\nshown in the right side of Figure 4.8. You could even use fewer multipliers at the cost of taking\\nmore cycles to complete the inner loop.\\nLooking at Figure 4.8, it is apparent that there are significant periods where the operators\\n84DISCRETE FOURIER TRANSFORM\\nV In[0]\\nM[i][0]\\nV In[1]\\nM[i][1]\\nV In[2]\\nM[i][2]\\nV In[3]\\nM[i][3]\\nV Out[i]\\nV In[4]\\nM[i][4]\\nV In[5]\\nM[i][5]\\nV In[6]\\nM[i][6]\\nV In[7]\\nM[i][7]\\nFigure 4.7: A data flow graph of the expression resulting from the unrolled inner loop from Figure\\n4.6.\\nInterval = 6\\nInterval = 9\\nV in[]\\nV in[]\\nV out[i]\\nV out[i]\\nLatency = 9\\nLatency = 6\\nFigure 4.8: Possible sequential implementations resulting from the unrolled inner loop from Figure\\n4.6.\\n85DISCRETE FOURIER TRANSFORM\\nInterval = 3\\nInterval = 6\\nV in[]\\nV in[]\\nV out[i]\\nV out[i]\\nLatency = 9\\nLatency = 6\\nFigure 4.9: Possible pipelined'),\n",
       " Document(page_content='resulting from the unrolled inner loop from Figure\\n4.6.\\n85DISCRETE FOURIER TRANSFORM\\nInterval = 3\\nInterval = 6\\nV in[]\\nV in[]\\nV out[i]\\nV out[i]\\nLatency = 9\\nLatency = 6\\nFigure 4.9: Possible pipelined implementations resulting from the unrolled inner loop from Figure\\n4.6.\\nare not performing useful work, reducing the overall efficiency of the design. It would be nice if\\nwe could reduce these periods. In this case we can observe that each iteration of data loop is,\\nin fact completely independent, which means that they can be executed concurrently. Just as we\\nunrolleddot product loop,it’salsopossibletounrolldata loopandperformallofthemultiplications\\nconcurrently. However, this would require a very large amount of FPGA resources. A better choice\\nis to enable each iteration of the loop to start as soon as possible, while the previous execution of\\nthe loop is still executing. This process is called loop pipelining and is achieved in Vivado(cid:13)R HLS\\nusing #pragma HLS pipeline. In most cases,'),\n",
       " Document(page_content='soon as possible, while the previous execution of\\nthe loop is still executing. This process is called loop pipelining and is achieved in Vivado(cid:13)R HLS\\nusing #pragma HLS pipeline. In most cases, loop pipelining reduces the interval of a loop to be\\nreduced, but does not affect the latency. Loop pipelined behavior of this design is shown in Figure\\n4.9.\\nUntil now, we have only focused on operator latency. It is common for functional units to be\\nalso be pipelined and most functional units in Vivado(cid:13)R HLS are fully pipelined with an interval\\nof one. Even though it might take 3 cycles for a single multiply operation to complete, a new\\nmultiply operation could start every clock cycle on a pipelined multiplier. In this way, a single\\nfunctional unit may be able to simultaneously execute many multiply operations at the same time.\\nFor instance, a multiplier with a latency of 3 and an interval of 1 could be simultaneously executing\\nthree multiply operations.\\nBy taking advantage of'),\n",
       " Document(page_content='many multiply operations at the same time.\\nFor instance, a multiplier with a latency of 3 and an interval of 1 could be simultaneously executing\\nthree multiply operations.\\nBy taking advantage of pipelined multipliers, we can reduce the latency of the unrolled inner\\nloop without adding additional operators. One possible implementation using three pipelined mul-\\ntipliers is shown on the left in Figure 4.10. In this case, the multiplication operations can execute\\nconcurrently (because they have no data dependencies), while the addition operations cannot be-\\ngin until the first multiplication has completed. In the figure on the right, a pipelined version\\nof this design is shown, with an interval of 3, which is similar to the results of Vivado(cid:13)R HLS if\\n#pragma HLS pipeline II=3 is applied to the data loop. In this case, not only are individual opera-\\ntions executing concurrently on the same operators, but those operations may come from different\\niterations of data loop.\\nAt this point'),\n",
       " Document(page_content='to the data loop. In this case, not only are individual opera-\\ntions executing concurrently on the same operators, but those operations may come from different\\niterations of data loop.\\nAt this point you may have observed that pipelining is possible at different levels of hierarchy,\\nincluding the operator level, loop level, and function level. Furthermore, pipelining at different\\nlevels are largely independent! We can use pipelined operators in a sequential loop, or we can use\\nsequential operators to build a pipelined loop. It’s also possible to build pipelined implementations\\n86DISCRETE FOURIER TRANSFORM\\nInterval = 8 Interval = 3\\nV in[] V in[]\\nV out[i] V out[i]\\nLatency = 8 Latency = 8\\nFigure 4.10: Possible implementations resulting from the unrolled inner loop from Figure 4.6 using\\npipelined multipliers.\\nof large functions which can be shared in Vivado(cid:13)R HLS just like primitive operators. In the end,\\nwhat matters most is how many operators are being instantiated, their'),\n",
       " Document(page_content='multipliers.\\nof large functions which can be shared in Vivado(cid:13)R HLS just like primitive operators. In the end,\\nwhat matters most is how many operators are being instantiated, their individual costs, and how\\noften they are used.\\n4.5 Storage Tradeoffs and Array Partitioning\\nUpuntilthispoint,wehaveassumedthatthedatainarrays(V In[],M[][],andV Out[]areaccessible\\nat anytime. In practice, however, the placement of the data plays a crucial role in the performance\\nand resource usage. In most processor systems, the memory architecture is fixed and we can only\\nadapt the program to attempt to best make use of the available memory hierarchy, taking care to\\nminimize register spills and cache misses, for instance. In HLS designs, we can also explore and\\nleverage different memory structures and often try to find the memory structure that best matches\\na particular algorithm. Typically large amounts of data are stored in off-chip memory, such as\\nDRAM, flash, or even network-attached storage.'),\n",
       " Document(page_content='often try to find the memory structure that best matches\\na particular algorithm. Typically large amounts of data are stored in off-chip memory, such as\\nDRAM, flash, or even network-attached storage. However, data access times are typically long, on\\nthe order of tens to hundreds (or more) of cycles. Off-chip storage also relatively large amounts of\\nenergy to access, because large amounts of current must flow through long wires. On-chip storage,\\nin contrast can be accessed quickly and is much lower power. I contrast it is more limited in the\\namount of data that can be stored. A common pattern is to load data into on-chip memory in a\\nblock, where it can then be operated on repeatedly. This is similar to the effect of caches in the\\nmemory hierarchy of general purpose CPUs.\\nThe primary choices for on-chip storage on in embedded memories (e.g., block RAMs) or in\\nflip-flops (FFs). These two options have their own tradeoffs. Flip-flop based memories allow for\\nmultiple reads at different'),\n",
       " Document(page_content='for on-chip storage on in embedded memories (e.g., block RAMs) or in\\nflip-flops (FFs). These two options have their own tradeoffs. Flip-flop based memories allow for\\nmultiple reads at different addresses in a single clock. It is also possible to read, modify, and write\\naFlip-flopbasedmemoryinasingleclockcycle. However, thenumberofFFsistypicallylimitedto\\naround100Kbytes,eveninthelargestdevices. Inpractice,mostflip-flopbasedmemoriesshouldbe\\nmuch smaller in order to make effective use of other FPGA resources. Block RAMs (BRAMs) offer\\nhigher capacity, on the order Mbytes of storage, at the cost of limited accessibility. For example,\\na single BRAM can store more than 1-4 Kbytes of data, but access to that data is limited to two\\ndifferentaddresseseachclockcycle. Furthermore, BRAMsarerequiredtohaveaminimumamount\\nof pipelining (i.e. the read operation must have a latency of at least one cycle). Therefore, the\\nfundamental tradeoff boils down to the required bandwidth versus the capacity.\\nIf'),\n",
       " Document(page_content='pipelining (i.e. the read operation must have a latency of at least one cycle). Therefore, the\\nfundamental tradeoff boils down to the required bandwidth versus the capacity.\\nIf throughput is the number one concern, all of the data would be stored in FFs. This would\\n87DISCRETE FOURIER TRANSFORM\\nallowanyelementtobeaccessedasmanytimesasitisneededeachclockcycle. However,asthesize\\nof arrays grows large, this is not feasible. In the case of matrix-vector multiplication, storing a 1024\\nby 1024 matrix of 32-bit integers would require about 4 MBytes of memory. Even using BRAM,\\nthis storage would require about 1024 BRAM blocks, since each BRAM stores around 4KBytes.\\nOn the other hand, using a single large BRAM-based memory means that we can only access two\\nelements at a time. This obviously prevents higher performance implementations, such as in Figure\\n4.7, which require accessing multiple array elements each clock cycle (all eight elements of V In[]\\nalong with 8 elements of M[][]). In practice,'),\n",
       " Document(page_content='higher performance implementations, such as in Figure\\n4.7, which require accessing multiple array elements each clock cycle (all eight elements of V In[]\\nalong with 8 elements of M[][]). In practice, most designs require larger arrays to be strategically\\ndivided into smaller BRAM memories, a process called array partitioning. Smaller arrays (often\\nused for indexing into larger arrays) can be partitioned completely into individual scalar variables\\nandmappedintoFFs. Matchingpipeliningchoicesandarraypartitioningtomaximizetheefficiency\\nof operator usage and memory usage is an important aspect of design space exploration in HLS.\\nVivado(cid:13)R HLS will perform some array partitioning automatically, but as array partitioning\\ntends to be rather design-specific it is often necessary to guide the tool for best results. Global\\nconfiguration of array partitioning is available in the config array partition project option. In-\\ndividual arrays can be explicitly partitioned using the array partition'),\n",
       " Document(page_content='tool for best results. Global\\nconfiguration of array partitioning is available in the config array partition project option. In-\\ndividual arrays can be explicitly partitioned using the array partition directive. The directive\\narray partition complete will split each element of an array into its own register, resulting in\\na flip-flop based memory. As with many other directive-based optimizations, the same result\\ncan also be achieved by rewriting the code manually. In general, it is preferred to use the tool\\ndirectives since it avoids introducing bugs and keeps the code easy to maintain.\\nReturningtothematrix-vectormultiplicationcodeinFigure4.4,wecanachieveahighlyparallel\\nimplementation with the addition of only a few directives, as shown in Figure 4.11. The resulting\\narchitecture is shown in Figure 4.12. Notice that the inner j loop is automatically unrolled by\\nVivado(cid:13)R HLS and hence every use of j is replaced with constants in the implementation. This\\ndesign demonstrates the most'),\n",
       " Document(page_content='Figure 4.12. Notice that the inner j loop is automatically unrolled by\\nVivado(cid:13)R HLS and hence every use of j is replaced with constants in the implementation. This\\ndesign demonstrates the most common use of array partitioning where the array dimensions that\\narepartitioned(inthiscase,V In[]andtheseconddimensionofM[][])areindexedwiththeconstants\\n(in this case the loop index j of the unrolled loop). This enables an architecture where multiplexers\\nare not required to access the partitioned arrays.\\nIt’s also possible to achieve other designs which use fewer multipliers and have lower perfor-\\nmance. For instance, in Figure 4.10, these designs use only three multipliers, hence we need only\\nneed to read three elements of matrix M[][] and vector V in[] each clock cycle. Completely parti-\\ntioning these arrays would result in extra multiplexing as shown in Figure 4.13. In actuality the\\narrays only need to be partitioned into three physical memories. Again, this partitioning could\\nbe'),\n",
       " Document(page_content='these arrays would result in extra multiplexing as shown in Figure 4.13. In actuality the\\narrays only need to be partitioned into three physical memories. Again, this partitioning could\\nbe implemented manually by rewriting code or in Vivado(cid:13)R HLS using the array partition cyclic\\ndirective.\\nBeginning with an array x containing the values\\n(cid:2) (cid:3)\\n1 2 3 4 5 6 7 8 9\\nThe directive array partition variable=x factor=2 cyclic on the array would split it into two\\narrays which are\\n(cid:2) (cid:3) (cid:2) (cid:3)\\n1 3 5 7 9 and 2 4 6 8\\n88DISCRETE FOURIER TRANSFORM\\n#define SIZE 8\\ntypedef int BaseType;\\nvoid matrix vector(BaseType M[SIZE][SIZE], BaseType V In[SIZE], BaseType V Out[SIZE]) {\\n#pragma HLS array partition variable=M dim=2 complete\\n#pragma HLS array partition variable=V In complete\\nBaseType i, j;\\ndata loop:\\nfor (i = 0; i < SIZE; i++) {\\n#pragma HLS pipeline II=1\\nBaseType sum = 0;\\ndot product loop:\\nfor (j = 0; j < SIZE; j++) {\\nsum += V In[j] ∗ M[i][j];\\n}\\nV Out[i] ='),\n",
       " Document(page_content='In complete\\nBaseType i, j;\\ndata loop:\\nfor (i = 0; i < SIZE; i++) {\\n#pragma HLS pipeline II=1\\nBaseType sum = 0;\\ndot product loop:\\nfor (j = 0; j < SIZE; j++) {\\nsum += V In[j] ∗ M[i][j];\\n}\\nV Out[i] = sum;\\n}\\n}\\nFigure4.11: Matrix-vectormultiplicationwithaparticularchoiceofarraypartitioningandpipelin-\\ning.\\ni\\nV In[0] Function Interval = 13\\nLoop Interval = 1\\nV In[1] M[][0]\\nV in[]\\nV In[2] M[][1]\\nV In[3] M[][2]\\nV Out[]\\nV In[4] M[][3]\\nV In[5] M[][4]\\nV In[6] M[][5]\\nV out[i]\\nV In[7] M[][6] Loop Latency = 6\\nM[][7] Function Latency = 13\\nFigure4.12: Matrix-vectormultiplicationarchitecturewithaparticularchoiceofarraypartitioning\\nand pipelining. The pipelining registers have been elided and the behavior is shown at right.\\n89DISCRETE FOURIER TRANSFORM\\nV In[0]\\ni\\nM[][0]\\nV In[1]\\nM[][1]\\nV In[2]\\nM[][2]\\nV In[3]\\nM[][3]\\nV In[4]\\nM[][4]\\nV In[5]\\nM[][5]\\nV In[6]\\nM[][6]\\nV In[7]\\nM[][7]\\nsreddA\\noT\\nsreddA\\noT\\nsreddA\\noT\\ni,j%3\\nM[][0-2]\\nM[][3-5]\\nM[][6-7]\\nsreddA\\noT\\nj%3\\nV In[0-2]\\nV In[3-5]\\nV In[6-7]\\nFigure 4.13: Matrix-vector'),\n",
       " Document(page_content='In[4]\\nM[][4]\\nV In[5]\\nM[][5]\\nV In[6]\\nM[][6]\\nV In[7]\\nM[][7]\\nsreddA\\noT\\nsreddA\\noT\\nsreddA\\noT\\ni,j%3\\nM[][0-2]\\nM[][3-5]\\nM[][6-7]\\nsreddA\\noT\\nj%3\\nV In[0-2]\\nV In[3-5]\\nV In[6-7]\\nFigure 4.13: Matrix-vector multiplication architectures at II=3 with a particular choices of array\\npartitioning. On the left, the arrays have been partitioned more than necessary, resulting in multi-\\nplexers. On the right, the arrays are partitioned with factor=3. In this case, multiplexing has been\\nreduced, but the j loop index becomes a part of the address computations.\\nSimilarly, the directive array partition variable=x factor=2 block would split it into two arrays\\n(cid:2) (cid:3) (cid:2) (cid:3)\\n1 2 3 4 5 and 6 7 8 9\\nStudy the effects of varying pipeline II and array partitioning on the performance and area.\\nPlot the performance in terms of number of matrix vector multiply operations per second\\n(throughput) versus the unroll and array partitioning factor. Plot the same trend for area\\n(showing LUTs, FFs, DSP blocks,'),\n",
       " Document(page_content='in terms of number of matrix vector multiply operations per second\\n(throughput) versus the unroll and array partitioning factor. Plot the same trend for area\\n(showing LUTs, FFs, DSP blocks, BRAMs). What is the general trend in both cases? Which\\ndesign would you select? Why?\\nAlternatively, similar results can be achieved by pipelining and applying partial loop unrolling\\ntotheinnerdot product loop. Figure4.14showstheresultofunrollingtheinnerloopofthematrix-\\nvector multiplication code by a factor of 2. You can see that the loop bounds now increment by 2.\\nEach loop iteration requires 2 elements of matrix M[][] and vector V in[] each iteration and perform\\ntwo multiplies instead of one. In this case after loop unrolling Vivado(cid:13)R HLS can implement the\\noperations in both expressions in parallel, corresponding to two iterations of the original loop.\\nNote that without appropriate array partitioning, unrolling the inner loop may offer no increase\\nin performance, as the number of concurrent'),\n",
       " Document(page_content='corresponding to two iterations of the original loop.\\nNote that without appropriate array partitioning, unrolling the inner loop may offer no increase\\nin performance, as the number of concurrent read operations is limited by the number of ports to\\nthe memory. In this case, we can store the data from the even columns in one BRAM and the\\ndata from the odd columns in the other. This is due to the fact that the unrolled loop is always\\nperforming one even iteration and one odd iteration.\\n90DISCRETE FOURIER TRANSFORM\\n#define SIZE 8\\ntypedef int BaseType;\\nvoid matrix vector(BaseType M[SIZE][SIZE], BaseType V In[SIZE], BaseType V Out[SIZE]) {\\n#pragma HLS array partition variable=M dim=2 cyclic factor=2\\n#pragma HLS array partition variable=V In cyclic factor=2\\nBaseType i, j;\\ndata loop:\\nfor (i = 0; i < SIZE; i++) {\\nBaseType sum = 0;\\ndot product loop:\\nfor (j = 0; j < SIZE; j+=2) {\\n#pragma HLS pipeline II=1\\nsum += V In[j] ∗ M[i][j];\\nsum += V In[j+1] ∗ M[i][j+1];\\n}\\nV Out[i] = sum;\\n}\\n}\\nFigure 4.14:'),\n",
       " Document(page_content='i < SIZE; i++) {\\nBaseType sum = 0;\\ndot product loop:\\nfor (j = 0; j < SIZE; j+=2) {\\n#pragma HLS pipeline II=1\\nsum += V In[j] ∗ M[i][j];\\nsum += V In[j+1] ∗ M[i][j+1];\\n}\\nV Out[i] = sum;\\n}\\n}\\nFigure 4.14: The inner loop of matrix-vector multiply manually unrolled by a factor of two.\\nThe HLS tool can automatically unroll loops using the unroll directive. The directive\\ntakes a factor argument which is a positive integer denoting the number of times that the\\nloop body should be unrolled.\\nManually divide M[][] and vector V in[] into separate arrays in the same manner as the\\ndirective array partition cyclic factor=2. How do you have to modify the code in order to\\nchange the access patterns? Now manually unroll the loop by a factor of two. How do the\\nperformance results vary between the original code (no array partitioning and no unrolling),\\nonly performing array partitioning, and performing array partitioning and loop unrolling?\\nFinally, use the directives to perform array partitioning and loop'),\n",
       " Document(page_content='(no array partitioning and no unrolling),\\nonly performing array partitioning, and performing array partitioning and loop unrolling?\\nFinally, use the directives to perform array partitioning and loop unrolling. How do those\\nresults compare to your manual results?\\nInthiscode,weseethatarraypartitioningoftengoeshandinhandwithourchoicesofpipelining.\\nArraypartitioningbyafactorof2enablesanincreaseinperformancebyafactorof2, whichcanbe\\nachievedeitherbypartiallyunrollingtheinnerloopbyafactorof2orbyreducingtheIIoftheouter\\nloopbyafactorof2. Increasingperformancerequiresacorrespondingamountofarraypartitioning.\\nInthecaseofmatrixvectormultiplication, thisrelationshipisrelativelystraightforwardsincethere\\nis only one access to each variable in the inner loop. In other code, the relationship might be more\\ncomplicated. Regardless, the goal of a designer is usually to ensure that the instantiated FPGA\\nresources are used efficiently. Increasing performance by a factor of 2 should use approximately\\ntwice as'),\n",
       " Document(page_content='Regardless, the goal of a designer is usually to ensure that the instantiated FPGA\\nresources are used efficiently. Increasing performance by a factor of 2 should use approximately\\ntwice as many resources. Decreasing performance by a factor of 2 should use approximately half\\nas many resources.\\n91DISCRETE FOURIER TRANSFORM\\nStudy the effects of loop unrolling and array partitioning on the performance and area.\\nPlot the performance in terms of number of matrix vector multiply operations per second\\n(throughput) versus the unroll and array partitioning factor. Plot the same trend for area\\n(showing LUTs, FFs, DSP blocks, BRAMs). What is the general trend in both cases? Which\\ndesign would you select? Why?\\n4.6 Baseline Implementation\\nWe just discussed some optimizations for matrix-vector multiplication. This is a core computation\\nin performing a DFT. However, there are some additionally intricacies that we must consider to\\nmove from the matrix-vector multiplication in the previous section to a'),\n",
       " Document(page_content='This is a core computation\\nin performing a DFT. However, there are some additionally intricacies that we must consider to\\nmove from the matrix-vector multiplication in the previous section to a functionally complete DFT\\nhardware implementation. We move our focus to the DFT in this section, and describe how to\\noptimize it to make it execute most efficiently.\\nOne significant change that is required is that we must be able to handle complex numbers.\\nAs noted in Section 4.2 because the elements of the S matrix are complex numbers, the DFT of\\na real-valued signal is almost always a complex-valued signal. It is also common to perform the\\nDFT of a complex-valued signal, to produce a complex-valued result. Additionally, we need to\\nhandle fractional or possibly floating point data, rather than integers. This can increase the cost\\nof the implementation, particularly if floating point operations need to be performed. In'),\n",
       " Document(page_content='we need to\\nhandle fractional or possibly floating point data, rather than integers. This can increase the cost\\nof the implementation, particularly if floating point operations need to be performed. In addition,\\nfloatingpointoperators,particularlyaddition,havemuchlargerlatencythanintegeraddition. This\\ncan make it more difficult to achieve II=1 loops. A second change is that we’d like to be able to\\nscale our design up to large input vector sizes, perhaps N=1024 input samples. Unfortunately, if\\nwe directly use matrix-vector multiplication, then we must store the entire S matrix. Since this\\nmatrix is the square of the input size, it becomes prohibitive to store for large input sizes. In the\\nfollowing sections, we’ll discuss techniques to address both of these complexities.\\nAs is typical when creating a hardware implementation using high level synthesis, we start with\\na straightforward or naive implementation. This provides us with a baseline code that we can\\ninsure has the correct'),\n",
       " Document(page_content='when creating a hardware implementation using high level synthesis, we start with\\na straightforward or naive implementation. This provides us with a baseline code that we can\\ninsure has the correct functionality. Typically, this code runs in a very sequential manner; it is\\nnot highly optimized and therefore may not meet the desired performance metrics. However, it is\\na necessary step to insure that the designer understand the functionality of the algorithm, and it\\nserves as starting point for future optimizations.\\nFigure 4.15 shows a baseline implementation of the DFT. This uses a doubly nested for loop.\\nThe inner loop multiplies one row of the S matrix with the input signal. Instead of reading the S\\nmatrix as an input, this code computes an element of S in each each iteration of the inner loop,\\nbased on the current loop indices. This phasor is converted to Cartesian coordinates (a real part\\nand an imaginary part) using the cos() and sin() functions. The code then performs a'),\n",
       " Document(page_content='the inner loop,\\nbased on the current loop indices. This phasor is converted to Cartesian coordinates (a real part\\nand an imaginary part) using the cos() and sin() functions. The code then performs a complex\\nmultiplication of the phasor with the appropriate sample of the input signal and accumulates the\\nresult. AfterN iterationsofthisinnerloop, oneforeachcolumnofS, onefrequencydomainsample\\nis calculated. The outer loop also iterates N times, once for each row of S. As a result, the code\\ncomputes an expression for w N times, but computes the cos() and sin() functions and a complex\\nmultiply-add N2 times.\\nThis code uses a function call to calculate cos() and sin() values. Vivado(cid:13)R HLS is capable of\\nsynthesizing these functions using its built-in math library. There are several possible algorithms\\n[22] for implementing trigonometric functions including CORDIC, covered in Chapter 3. However,\\ngenerating precise results for these functions can be expensive. There are several'),\n",
       " Document(page_content='possible algorithms\\n[22] for implementing trigonometric functions including CORDIC, covered in Chapter 3. However,\\ngenerating precise results for these functions can be expensive. There are several possibilities for\\n92DISCRETE FOURIER TRANSFORM\\n#include <math.h> //Required for cos and sin functions\\ntypedef double IN TYPE; // Data type for the input signal\\ntypedef double TEMP TYPE; // Data type for the temporary variables\\n#define N 256 // DFT Size\\nvoid dft(IN TYPE sample real[N], IN TYPE sample imag[N]) {\\nint i, j;\\nTEMP TYPE w;\\nTEMP TYPE c, s;\\n// Temporary arrays to hold the intermediate frequency domain results\\nTEMP TYPE temp real[N];\\nTEMP TYPE temp imag[N];\\n// Calculate each frequency domain sample iteratively\\nfor (i = 0; i < N; i += 1) {\\ntemp real[i] = 0;\\ntemp imag[i] = 0;\\n// (2 ∗ pi ∗ i)/N\\nw = (2.0 ∗ 3.141592653589 / N) ∗ (TEMP TYPE)i;\\n// Calculate the jth frequency sample sequentially\\nfor (j = 0; j < N; j += 1) {\\n// Utilize HLS tool to calculate sine and cosine values\\nc = cos(j ∗'),\n",
       " Document(page_content='i)/N\\nw = (2.0 ∗ 3.141592653589 / N) ∗ (TEMP TYPE)i;\\n// Calculate the jth frequency sample sequentially\\nfor (j = 0; j < N; j += 1) {\\n// Utilize HLS tool to calculate sine and cosine values\\nc = cos(j ∗ w);\\ns = sin(j ∗ w);\\n// Multiply the current phasor with the appropriate input sample and keep\\n// running sum\\ntemp real[i] += (sample real[j] ∗ c − sample imag[j] ∗ s);\\ntemp imag[i] += (sample real[j] ∗ s + sample imag[j] ∗ c);\\n}\\n}\\n// Perform an inplace DFT, i.e., copy result into the input arrays\\nfor (i = 0; i < N; i += 1) {\\nsample real[i] = temp real[i];\\nsample imag[i] = temp imag[i];\\n}\\n}\\nFigure 4.15: Baseline code for the DFT.\\n93DISCRETE FOURIER TRANSFORM\\neliminating these function calls, since the inputs aren’t arbitrary. We will discuss these tradeoffs\\nin more detail later. A sequential implementation of this code is show in Figure 4.16.\\nWhat changes would this code require if you were to use a CORDIC that you designed, for\\nexample, from Chapter 3? Would changing the accuracy of the'),\n",
       " Document(page_content='implementation of this code is show in Figure 4.16.\\nWhat changes would this code require if you were to use a CORDIC that you designed, for\\nexample, from Chapter 3? Would changing the accuracy of the CORDIC core make the DFT\\nhardware resource usage change? How would it effect the performance?\\nImplement the baseline code for the DFT using HLS. Looking at the reports, what is the\\nrelative cost of the implementation of the trignometric functions, compared to multiplication\\nandaddition? Whichoperationsdoesitmakemoresensetotrytooptimize? Whatperformance\\ncan be achieved by pipelining the inner loop?\\n4.7 DFT optimization\\nThe baseline DFT implementation of the previous section uses relatively high precision double\\ndatatypes. Implementing floating point operations is typically very expensive and requires many\\npipeline stages, particularly for double precision. We can see in Figure 4.16 that this significantly\\naffects the performance of the loop. With pipelining, the affect of these high-latency'),\n",
       " Document(page_content='many\\npipeline stages, particularly for double precision. We can see in Figure 4.16 that this significantly\\naffects the performance of the loop. With pipelining, the affect of these high-latency operations is\\nless critical, since multiple executions of the loop can execute concurrently. The exception in this\\ncode are the temp real[] and temp imag[] variables, which are used to accumulate the result. This\\naccumulation is a recurrence and limits the achievable II in this design when pipelining the inner\\nloop. This operator dependence is shown in Figure 4.17.\\nOne possible solution is to reduce the precision of the computation. This is always a valuable\\ntechnique when it can be applied, since it reduces the resources required for each operation, the\\nmemory required to store any values, and often reduces the latency of operations as well. For\\ninstance we could use the 32-bit float type or the 16-bit half types rather than double. Many signal\\nprocessing systems avoid floating point data types'),\n",
       " Document(page_content='reduces the latency of operations as well. For\\ninstance we could use the 32-bit float type or the 16-bit half types rather than double. Many signal\\nprocessing systems avoid floating point data types entirely and use fixed point data types3.5. For\\ncommonly used integer and fixed-point precisions, each addition can be completed in a single cycle,\\nenabling the loop to be pipelined at II=1.\\nWhat happens to the synthesis result of the code in Figure 4.15 if you change all of the\\ndata types from double to float? Or from double to half? Or to a fixed point value? How does\\nthis change the performance (interval and latency) and the resource usage? Does it change the\\nvalues of the output frequency domain samples?\\nA more general solution to achieve II=1 with floating point accumulations is to process the\\ndata in a different order. Looking at Figure 4.17 we see that the recurrence exists (represented\\nby the arrow) because the j loop is the inner loop. If the inner loop were the i loop instead,'),\n",
       " Document(page_content='the\\ndata in a different order. Looking at Figure 4.17 we see that the recurrence exists (represented\\nby the arrow) because the j loop is the inner loop. If the inner loop were the i loop instead, then\\nwe wouldn’t need the result of the accumulation before the next iteration starts. We can achieve\\nthis in the code by interchanging the order of the two loops. This optimization is often called\\nloop interchange or pipeline-interleaved processing[40]. In this case, it may not be obvious that we\\ncan rearrange the loops because of the extra code inside the outer i loop. Fortunately, the S matrix\\nis diagonally symmetric, and hence i and j can be exchanged in the computation of w. The result\\n94DISCRETE FOURIER TRANSFORM\\nRun remaining\\nCompute w Run inner loop Compute w iterations Copy Result\\nsample[i]\\nint to double\\ndouble\\nint to double\\ndouble\\nsin/cos\\ndouble\\ndouble\\ndouble\\ndouble\\nsample[i]\\nN\\ni\\n/ x\\n2 ∏\\nw\\nx j\\nCORDIC x +\\nsample[0] temp[0]\\nsample[1] temp[1]\\nsample[2] temp[2]\\n. .\\n. .\\n. .\\nsample[N-1]'),\n",
       " Document(page_content='to double\\ndouble\\nint to double\\ndouble\\nsin/cos\\ndouble\\ndouble\\ndouble\\ndouble\\nsample[i]\\nN\\ni\\n/ x\\n2 ∏\\nw\\nx j\\nCORDIC x +\\nsample[0] temp[0]\\nsample[1] temp[1]\\nsample[2] temp[2]\\n. .\\n. .\\n. .\\nsample[N-1] temp[N-1]\\nFigure 4.16: A high level architectural diagram of the DFT as specified in the code from Figure\\n4.15. This is not a comprehensive view of the architecture, e.g., it is missing components related\\nto updating the loop counters i and j. It is meant to provide an approximate notion of how this\\narchitecture will be synthesized. Here we’ve assumed that floating point operators take 4 clock\\ncycles.\\n95DISCRETE FOURIER TRANSFORM\\nj\\nLoop Interval = 4 i temp real[0]\\ntemp real[1]\\nsample[i]\\nint to double temp real[2]\\ndouble temp real[3]\\nsin/cos\\ndouble temp real[4]\\ndouble temp real[5]\\ndouble\\ndouble temp real[6]\\nsample[i] temp real[7]\\nFigure 4.17: Pipelined version of the behavior in Figure 4.16. In this case, the initiation interval\\nof the loop is limited to 4, since each floating point addition takes 4'),\n",
       " Document(page_content='temp real[7]\\nFigure 4.17: Pipelined version of the behavior in Figure 4.16. In this case, the initiation interval\\nof the loop is limited to 4, since each floating point addition takes 4 clock cycles to complete and\\nthe result is required before the next loop iteration begins (the dependence shown in red). The\\ndependencies for all iterations are summarized in the diagram on the right.\\nis that we can now achieve an II of 1 for the inner loop. The tradeoff is additional storage for the\\ntemp real and temp imag arrays to store the intermediate values until they are needed again.\\nReorder the loops of the code in Figure 4.15 and show that you can pipeline the inner loop\\nwith an II of 1.\\nThere are other optimizations that we can apply based on the structure of the S matrix in\\nthe DFT to eliminate the trigonometric operations entirely. Recall that the complex vectors for\\neach element of the S matrix are calculated based upon a fixed integer rotation around the unit\\ncircle. Row S[0][] of the S'),\n",
       " Document(page_content='the trigonometric operations entirely. Recall that the complex vectors for\\neach element of the S matrix are calculated based upon a fixed integer rotation around the unit\\ncircle. Row S[0][] of the S matrix corresponds to zero rotations around the unit circle, row S[1][]\\ncorresponds to a single rotation, and the following rows correspond to more rotations around the\\nunitcircle. ItturnsoutthatthevectorscorrespondingtothesecondrowS[1][],whichisonerotation\\naround the unit circle (divided into 360/8 = 45◦ individual rotations), cover all of the vectors from\\nevery other row. This can be visually confirmed by studying Figure 9.11. Thus it is possible to\\nstore only the sine and cosine values from this one rotation, and then index into this memory to\\ncalculatetherequisitevaluesforthecorrespondingrows. Thisrequiresonly2×N = O(N)elements\\nofstorage. ThisresultsinaO(N)reductioninstorage, whichforthe1024pointDFTwouldreduce\\nthememorystoragerequirementsto1024×2entries.'),\n",
       " Document(page_content='Thisrequiresonly2×N = O(N)elements\\nofstorage. ThisresultsinaO(N)reductioninstorage, whichforthe1024pointDFTwouldreduce\\nthememorystoragerequirementsto1024×2entries. Assuming32bitfixedorfloatingpointvalues,\\nthiswouldrequireonly8KBofon-chipmemory. Obviously,thisisasignificantreductioncompared\\nto storing the entire S matrix explicitly. We denote this one dimensional storage of the matrix S\\nas S(cid:48) where\\nS(cid:48) = S[1][·] = (1 s s2 ··· sN−1) (4.14)\\nDerive a formula for the access pattern for the 1D array S(cid:48) given as input the row number\\ni and column element j corresponding to the array S. That is, how do we index into the 1D S\\narray to access element S(i,j) from the 2D S array.\\nTo increase performance further we can apply techniques that are very similar to the matrix-\\nvector multiply. Previously, we observed that increasing performance of matrix-vector multiply\\nrequired partitioning the M[][] array. Unfortunately, representing the S matrix using the S(cid:48) means\\nthat there'),\n",
       " Document(page_content='Previously, we observed that increasing performance of matrix-vector multiply\\nrequired partitioning the M[][] array. Unfortunately, representing the S matrix using the S(cid:48) means\\nthat there is no longer an effective way to partition S(cid:48) to increase the amount of data that we can\\n96DISCRETE FOURIER TRANSFORM\\nread on each clock. Every odd row and column of S includes every element of S(cid:48). As a result,\\nthere is no way to partition the values of S(cid:48) like were able to do with S. The only way to increase\\nthe number of read ports from the memory that stores S(cid:48) is to replicate the storage. Fortunately,\\nunlike with a memory that must be read and written, it is relatively easy to replicate the storage\\nfor an array that is only read. In fact, Vivado(cid:13)R HLS will perform this optimization automatically\\nwhen instantiates a Read-only Memory (ROM) for an array which is initialized and then never\\nmodified. One advantage of this capability is that we can simply move'),\n",
       " Document(page_content='this optimization automatically\\nwhen instantiates a Read-only Memory (ROM) for an array which is initialized and then never\\nmodified. One advantage of this capability is that we can simply move the sin() and cos() calls\\ninto an array initialization. In most cases, if this code is at the beginning of a function and only\\ninitializes the array, then Vivado(cid:13)R HLS is able to optimize away the trigonometric computation\\nentirely and compute the contents of the ROM automatically.\\nDevise an architecture that utilizes S(cid:48) – the 1D version of the S matrix. How does this\\naffect the required storage space? Does this change the logic utilization compared to an imple-\\nmentation using the 2D S matrix?\\nIn order to effectively optimize the design, we must consider every part of the code. The\\nperformance can only as good as the “weakest link” meaning that if there is a bottleneck the\\nperformance will take a significant hit. The current version of the DFT function performs an in-\\nplace'),\n",
       " Document(page_content='The\\nperformance can only as good as the “weakest link” meaning that if there is a bottleneck the\\nperformance will take a significant hit. The current version of the DFT function performs an in-\\nplace operation on the input and output data, i.e., it stores the results in the same array as the\\ninput data. The input array arguments sample real and sample imag effectively act as a memory\\nport. That is, you can think of these arguments arrays as stored in the same memory location.\\nThus, we can only grab one piece of data from each of these arrays on any given cycle. This can\\ncreateabottleneckintermsofparallelizingthemultiplicationandsummationoperationswithinthe\\nfunction. This also explains the reason why we must store all of the output results in a temporary\\narray, and then copy all of those results into the “sample” arrays at the end of the function. We\\nwould not have to do this if we did not perform an in-place operation.\\nModify the DFT function interface so that the input and outputs are'),\n",
       " Document(page_content='into the “sample” arrays at the end of the function. We\\nwould not have to do this if we did not perform an in-place operation.\\nModify the DFT function interface so that the input and outputs are stored in separate\\narrays. How does this effect the optimizations that you can perform? How does it change the\\nperformance? What about the area results?\\n4.8 Conclusion\\nInthischapter, welookedatthehardwareimplementationandoptimizationoftheDiscreteFourier\\nTransform (DFT). The DFT is a fundamental operation in digital signal processing. It takes a\\nsignal sampled in the time domain and converts it into the frequency domain. At the beginning\\nof this chapter, we describe the mathematical background for the DFT. This is important for\\nunderstanding the optimizations done in the next chapter (FFT). The remainder of the chapter\\nwas focused on specifying and optimizing the DFT for an efficient implementation on an FPGA.\\nAt its core, the DFT performs a matrix-vector multiplication. Thus, we spend some'),\n",
       " Document(page_content='remainder of the chapter\\nwas focused on specifying and optimizing the DFT for an efficient implementation on an FPGA.\\nAt its core, the DFT performs a matrix-vector multiplication. Thus, we spend some time\\ninitially to describe instruction level optimizations on a simplified code performing matrix-vector\\nmultiplication. These instruction level optimizations are done by the HLS tool. We use this as an\\nopportunity to shed some light into the process that the HLS tool performs in the hopes that it\\nwill provide some better intuition about the results the tool outputs.\\nAfterthat,weprovideanfunctionallycorrectimplementationfortheDFT.Wediscussanumber\\nof optimizations that can be done to improve the performance. In particular, we focus on the\\n97DISCRETE FOURIER TRANSFORM\\nproblemofdividingthecoefficientarrayintodifferentmemoriesinordertoincreasethethroughput.\\nArray partitioning optimization are often key to achieving the highest performing architectures.\\n98Chapter 5\\nFast Fourier'),\n",
       " Document(page_content='partitioning optimization are often key to achieving the highest performing architectures.\\n98Chapter 5\\nFast Fourier Transform\\nPerforming the Discrete Fourier Transform (DFT) directly using matrix-vector multiply requires\\nO(n2) multiply and add operations, for an input signal with n samples. It is possible to reduce the\\ncomplexity by exploiting the structure of the constant coefficients in the matrix. This S matrix\\nencodes the coefficients of the DFT; each row of this matrix corresponds to a fixed number of\\nrotationsaroundthecomplexunitcircle(pleaserefertoChapter??formoredetailedinformation).\\nThese values have a significant amount of redundancy, and that can be exploited to reduce the\\ncomplexity of the algorithm.\\nThe ’Big O’ notation used here describes the general order of complexity of an algorithm\\nbased on the size of the input data. For a complete description of Big O notation and its use\\nin analyzing algorithms, see [17].\\nThe Fast Fourier Transform (FFT) uses a divide-and-conquer'),\n",
       " Document(page_content='an algorithm\\nbased on the size of the input data. For a complete description of Big O notation and its use\\nin analyzing algorithms, see [17].\\nThe Fast Fourier Transform (FFT) uses a divide-and-conquer approach based on the symmetry\\nof the S matrix. The FFT was made popular by the Cooley-Tukey algorithm [16], which requires\\nO(nlogn) operations to compute the same function as the DFT. This can provide a substantial\\nspeedup, especially when performing the Fourier transform on large signals.\\nThe divide-and-conquer approach to computing the DFT was initially developed by Karl\\nFriedrichGaussintheearly19thcentury. However,sinceGauss’workonthiswasnotpublished\\nduring his lifetime and only appeared in a collected works after his death, it was relegated to\\nobscurity. Heideman et al. [32] provide a nice background on the history of the FFT.\\nThe focus of this chapter is to provide the reader with a good understanding of the FFT\\nalgorithm since that is an important part of creating an optimized'),\n",
       " Document(page_content='a nice background on the history of the FFT.\\nThe focus of this chapter is to provide the reader with a good understanding of the FFT\\nalgorithm since that is an important part of creating an optimized hardware design. Thus, we start\\nby giving a mathematical treatment of the FFT. This discussion focuses on small FFT sizes to give\\nsome basic intuition on the core ideas. After that, we focus on different hardware implementation\\nstrategies.\\n5.1 Background\\nThe FFT brings about a reduction in complexity by taking advantage of symmetries in the DFT\\ncalculation. Tobetterunderstandhowtodothis, letuslookatDFTwithasmallnumberofpoints,\\n99FAST FOURIER TRANSFORM\\nstarting with the 2 point DFT. Recall that the DFT performs a matrix vector multiplication, i.e.,\\nG[] = S[][]·g[], where g[] is the input data, G[] is the frequency domain output data, and S[][] are\\nthe DFT coefficients. We follow the same notation for the coefficient matrix, and the input and\\noutput vectors as described in Chapter ??.\\nFor a'),\n",
       " Document(page_content='G[] is the frequency domain output data, and S[][] are\\nthe DFT coefficients. We follow the same notation for the coefficient matrix, and the input and\\noutput vectors as described in Chapter ??.\\nFor a 2 point DFT, the values of S are:\\n(cid:20) W00 W01(cid:21)\\nS = 2 2 (5.1)\\nW10 W11\\n2 2\\nHere we use the notation W = e−j2π. The superscript on W denotes values that are added to\\nthe numerator and the subscript on the W indicates those values added in the denominator of the\\ncomplex exponential. For example, W 423 = e−j2 4π·2·3 . This is similar to the s value used in the DFT\\n−j2π\\ndiscussion (Chapter ??) where s = e N . The relationship between s and W is s = W N.\\nThe e−j2π or W terms are often called twiddle factors. This term has its origin in the 1966\\npaper by Gentleman and Sande [27].\\n(cid:20) G[0](cid:21) (cid:20) W00 W01(cid:21) (cid:20) g[0](cid:21)\\n= 2 2 · (5.2)\\nG[1] W10 W11 g[1]\\n2 2\\nExpanding the two equations for a 2 point DFT gives us:\\n−j2π·0·0 −j2π·0·1\\nG[0] = g[0]·e 2 +g[1]·e 2'),\n",
       " Document(page_content='G[0](cid:21) (cid:20) W00 W01(cid:21) (cid:20) g[0](cid:21)\\n= 2 2 · (5.2)\\nG[1] W10 W11 g[1]\\n2 2\\nExpanding the two equations for a 2 point DFT gives us:\\n−j2π·0·0 −j2π·0·1\\nG[0] = g[0]·e 2 +g[1]·e 2 (5.3)\\n= g[0]+g[1]\\ndue to the fact that since e0 = 1. The second frequency term\\n−j2π·1·0 −j2π·1·1\\nG[1] = g[0]·e 2 +g[1]·e 2 (5.4)\\n= g[0]−g[1]\\nsince e−j2 2π·1·1 = e−jπ = −1.\\nFigure 5.1 provides two different representations for this computation. Part a) is the data flow\\ngraph for the 2 point DFT. It is the familiar view that we have used to represent computation\\nthroughout this book. Part b) shows a butterfly structure for the same computation. This is a\\ntypically structure used in digital signal processing, in particular, to represent the computations in\\nan FFT.\\nThe butterfly structure is a more compact representation that is useful to represent large data\\nflow graphs. When two lines come together this indicates an addition operation. Any label on the\\nline itself indicates a multiplication of'),\n",
       " Document(page_content='compact representation that is useful to represent large data\\nflow graphs. When two lines come together this indicates an addition operation. Any label on the\\nline itself indicates a multiplication of that label by the value on that line. There are two labels in\\nthis figure. The ‘−’ sign on the bottom horizontal line indicates that this value should be negated.\\nThis followed by the addition denoted by the two lines intersecting is the same as subtraction. The\\nsecond label is W0. While this is a multiplication is unnecessary (since W0 = 1 this means it is\\n2 2\\nmultiplying by the value ‘1’), we show it here since it is a common structure that appears in higher\\npoint FFTs.\\nNowletusconsideraslightlylargerDFT–a4pointDFT,i.e., onethathas4inputs, 4outputs,\\nand a 4×4 S matrix. The values of S for a 4 point DFT are:\\n\\uf8ee W00 W01 W02 W03\\uf8f9\\n4 4 4 4\\n\\uf8efW10 W11 W12 W13\\n\\uf8fa\\nS = \\uf8ef 4 4 4 4 \\uf8fa (5.5)\\n\\uf8f0W20 W21 W22 W23\\n\\uf8fb\\n4 4 4 4\\nW30 W31 W32 W33\\n4 4 4 4\\n100FAST FOURIER TRANSFORM\\na) b)\\ng[0] G[0]\\ng[0]\\n+ G[0] 0\\nW\\ng[1]'),\n",
       " Document(page_content='a 4 point DFT are:\\n\\uf8ee W00 W01 W02 W03\\uf8f9\\n4 4 4 4\\n\\uf8efW10 W11 W12 W13\\n\\uf8fa\\nS = \\uf8ef 4 4 4 4 \\uf8fa (5.5)\\n\\uf8f0W20 W21 W22 W23\\n\\uf8fb\\n4 4 4 4\\nW30 W31 W32 W33\\n4 4 4 4\\n100FAST FOURIER TRANSFORM\\na) b)\\ng[0] G[0]\\ng[0]\\n+ G[0] 0\\nW\\ng[1] 2 G[1]\\ng[1] -\\n- G[1]\\nFigure 5.1: Part a) is a data flow graph for a 2 point DFT/FFT. Part b) shows the same compu-\\ntation, but viewed as a butterfly structure. This is a common representation for the computation\\nof an FFT in the digital signal processing domain.\\nAnd the DFT equation to compute the frequency output terms are:\\n\\uf8ee G[0]\\uf8f9 \\uf8ee W00 W01 W02 W03\\uf8f9 \\uf8ee g[0]\\uf8f9\\n4 4 4 4\\n\\uf8efG[1]\\uf8fa \\uf8efW10 W11 W12 W13 \\uf8fa \\uf8efg[1]\\uf8fa\\n\\uf8ef \\uf8fa = \\uf8ef 4 4 4 4 \\uf8fa·\\uf8ef \\uf8fa (5.6)\\n\\uf8f0G[2]\\uf8fb \\uf8f0W20 W21 W22 W23 \\uf8fb \\uf8f0g[2]\\uf8fb\\n4 4 4 4\\nG[3] W30 W31 W32 W33 g[3]\\n4 4 4 4\\nNow we write out the equations for each of the frequency domain values in G[] one-by-one. The\\nequation for G[0] is:\\n−j2π·0·0 −j2π·0·1 −j2π·0·2 −j2π·0·3\\nG[0] = g[0]·e 4 +g[1]·e 4 +g[2]·e 4 +g[3]·e 4 (5.7)\\n= g[0]+g[1]+g[2]+g[3]\\nsince e0 = 1.\\nThe equation for G[1] is:\\n−j2π·1·0 −j2π·1·1 −j2π·1·2'),\n",
       " Document(page_content='for G[0] is:\\n−j2π·0·0 −j2π·0·1 −j2π·0·2 −j2π·0·3\\nG[0] = g[0]·e 4 +g[1]·e 4 +g[2]·e 4 +g[3]·e 4 (5.7)\\n= g[0]+g[1]+g[2]+g[3]\\nsince e0 = 1.\\nThe equation for G[1] is:\\n−j2π·1·0 −j2π·1·1 −j2π·1·2 −j2π·1·3\\nG[1] = g[0]·e 4 +g[1]·e 4 +g[2]·e 4 +g[3]·e 4\\n−j2π −j4π −j6π\\n= g[0]+g[1]·e 4 +g[2]·e 4 +g[3]·e 4\\n(5.8)\\n= g[0]+g[1]·e−j 42π +g[2]·e−jπ +g[3]·e−j 42π e−jπ\\n−j2π −j2π\\n= g[0]+g[1]·e 4 −g[2]−g[3]·e 4\\nThe reductions were done based upon the fact that e−jπ = −1.\\nThe equation for G[2] is:\\n−j2π·2·0 −j2π·2·1 −j2π·2·2 −j2π·2·3\\nG[2] = g[0]·e 4 +g[1]·e 4 +g[2]·e 4 +g[3]·e 4\\n= g[0]+g[1]·e−j 44π +g[2]·e−j 48π +g[3]·e−j 412π (5.9)\\n= g[0]−g[1]+g[2]−g[3]\\n−j8π −12jπ\\nThe reductions were done by simplifications based upon rotations. E.g., e 4 = 1 and e 4 = −1\\nsince in both cases use the fact that e−j2π is equal to 1. In other words, any complex exponential\\nwith a rotation by 2π is equal.\\n101FAST FOURIER TRANSFORM\\nFinally, the equation for G[3] is:\\n−j2π·3·0 −j2π·3·1 −j2π·3·2 −j2π·3·3\\nG[3] = g[0]·e 4 +g[1]·e 4'),\n",
       " Document(page_content='to 1. In other words, any complex exponential\\nwith a rotation by 2π is equal.\\n101FAST FOURIER TRANSFORM\\nFinally, the equation for G[3] is:\\n−j2π·3·0 −j2π·3·1 −j2π·3·2 −j2π·3·3\\nG[3] = g[0]·e 4 +g[1]·e 4 +g[2]·e 4 +g[3]·e 4\\n−j6π −j12π −j18π\\n= g[0]+g[1]·e 4 +g[2]·e 4 +g[3]·e 4\\n(5.10)\\n−j6π −j10π\\n= g[0]+g[1]·e 4 −g[2]+g[3]·e 4\\n−j6π −j6π\\n= g[0]+g[1]·e 4 −g[2]−g[3]·e 4\\n−j18π\\nMostofthereductionsthatwehavenotseenyetdealwiththelastterm. Itstartsoutase 4 . Itis\\n−j10π −j10π −j8π\\nreducedtoe 4 sincetheseareequivalentbasedupona2π rotation, or, equivalently, e 4 ·e 4\\n−j8π −j6π\\nand the second term e 4 = 1. Finally, a rotation of π, which is equal to −1, brings it to e 4 .\\n−j6π −j4π −j4π\\nAnother way of viewing this is e 4 ·e 4 and e 4 = −1. We leave this term in this unreduced\\nstate in order to demonstrate symmetries in the following equations.\\nWith a bit of reordering, we can view these four equations as:\\n−j2π0\\nG[0] = (g[0]+g[2])+e 4 (g[1]+g[3])\\n−j2π1\\nG[1] = (g[0]−g[2])+e 4 (g[1]−g[3])\\n(5.11)\\n−j2π2\\nG[2]'),\n",
       " Document(page_content='in the following equations.\\nWith a bit of reordering, we can view these four equations as:\\n−j2π0\\nG[0] = (g[0]+g[2])+e 4 (g[1]+g[3])\\n−j2π1\\nG[1] = (g[0]−g[2])+e 4 (g[1]−g[3])\\n(5.11)\\n−j2π2\\nG[2] = (g[0]+g[2])+e 4 (g[1]+g[3])\\n−j2π3\\nG[3] = (g[0]−g[2])+e 4 (g[1]−g[3])\\nSeveral different symmetries are starting to emerge. First, the input data can be partitioned\\ninto even and odd elements, i.e., similar operations are done on the elements g[0] and g[2], and the\\nsame is true for the odd elements g[1] and g[3]. Furthermore we can see that there are addition\\nand subtraction symmetries on these even and odd elements. During the calculations of the output\\nfrequencies G[0] and G[2], the even and odd elements are summed together. The even and odd\\ninput elements are subtracted when calculating the frequencies G[1] and G[3]. Finally, the odd\\nelements in every frequency term are multiplied by a constant complex exponential Wi where i\\n4\\ndenotes the index for the frequency output, i.e., G[i].\\nLooking at'),\n",
       " Document(page_content='G[1] and G[3]. Finally, the odd\\nelements in every frequency term are multiplied by a constant complex exponential Wi where i\\n4\\ndenotes the index for the frequency output, i.e., G[i].\\nLooking at the terms in the parentheses, we see that they are 2 point FFT. For example,\\nconsider the terms corresponding to the even input values g[0] and g[2]. If we perform a 2 point\\nFFT on these even terms, the lower frequency (DC value) is g[0]+g[2] (see Equation 5.3), and the\\nhigher frequency is calculated as g[0]−g[2] (see Equation 5.4). The same is true for the odd input\\nvalues g[1] and g[3].\\nWe perform one more transformation on these equations.\\n−j2π0\\nG[0] = (g[0]+g[2])+e 4 (g[1]+g[3])\\n−j2π1\\nG[1] = (g[0]−g[2])+e 4 (g[1]−g[3])\\n(5.12)\\n−j2π0\\nG[2] = (g[0]+g[2])−e 4 (g[1]+g[3])\\n−j2π1\\nG[3] = (g[0]−g[2])−e 4 (g[1]−g[3])\\n−j2π2 −j2π0 −j2π3\\nThe twiddle factors in the last two equations are modified from e 4 = −e 4 and e 4 =\\n−j2π1\\n−e 4 . This allows for a reduction in the complexity of the multiplications'),\n",
       " Document(page_content='4 (g[1]−g[3])\\n−j2π2 −j2π0 −j2π3\\nThe twiddle factors in the last two equations are modified from e 4 = −e 4 and e 4 =\\n−j2π1\\n−e 4 . This allows for a reduction in the complexity of the multiplications since we can share\\nmultiplications across two terms.\\nFigure 5.2 shows the butterfly diagram for the four point FFT. We can see that the first stage\\nis two 2 point FFT operations performed on the even (top butterfly) and odd (bottom butterfly)\\ninput values. The output of the odd 2 point FFTs are multiplied by the appropriate twiddle factor.\\nWe can use two twiddle factors for all four output terms by using the reduction shown in Equation\\n5.12.\\n102FAST FOURIER TRANSFORM\\n2 point FFT\\ng[0] G[0]\\ng[2] G[1]\\n-\\nW0\\ng[1] 4 G[2]\\n-\\nW1\\ng[3] 4 G[3]\\n- -\\n2 point FFT\\nStage 1 Stage 2\\nFigure 5.2: A four point FFT divided into two stages. Stage 1 has uses two 2 point FFTs – one 2\\npoint FFT for the even input values and the other 2 point FFT for the odd input values. Stage 2\\nperforms the remaining operations to'),\n",
       " Document(page_content='divided into two stages. Stage 1 has uses two 2 point FFTs – one 2\\npoint FFT for the even input values and the other 2 point FFT for the odd input values. Stage 2\\nperforms the remaining operations to complete the FFT computation as detailed in Equation 5.12.\\nWe are seeing the beginning of trend that allows the a reduction in complexity from O(n2)\\noperations for the DFT to O(nlogn) operations for the FFT. The key idea is building the compu-\\ntation through recursion. The 4 point FFT uses two 2 point FFTs. This extends to larger FFT\\nsizes. For example, an 8 point FFT uses two 4 point FFTs, which in turn each use two 2 point\\nFFTs (for a total of four 2 point FFTs). An 16 point FFT uses two 8 point FFTs, and so on.\\nHow many 2 point FFTs are used in a 32 point FFT? How many are there in a 64 point\\nFFT? How many 4 point FFTs are required for a 64 point FFT? How about a 128 point FFT?\\nWhat is the general formula for 2 point, 4 point, and 8 point FFTs in an N point FFT (where\\nN > 8)?\\nNow let us'),\n",
       " Document(page_content='How many 4 point FFTs are required for a 64 point FFT? How about a 128 point FFT?\\nWhat is the general formula for 2 point, 4 point, and 8 point FFTs in an N point FFT (where\\nN > 8)?\\nNow let us formally derive the relationship, which provides a general way to describe the re-\\ncursive structure of the FFT. Assume that we are calculating an N point FFT. The formula for\\ncalculating the frequency domain values G[] given the input values g[] is:\\nN−1\\n(cid:88) −j2πkn\\nG[k] = g[n]·e N for k = 0,...,N −1 (5.13)\\nn=0\\nWe can divide this equation into two parts, one that sums the even components and one that\\nsums the odd components.\\nN/2−1 N/2−1\\n(cid:88) −j2πk(2n) (cid:88) −j2πk(2n+1)\\nG[k] = g[2n]·e N + g[2n+1]·e N (5.14)\\nn=0 n=0\\nThe first part of this equation deals with the even inputs, hence the 2n terms in both g[] and in\\nthe exponent of e. The second part corresponds to the odd inputs with 2n+1 in both places. Also\\nnote that the sums now go to N/2−1 in both cases which should make sense since we'),\n",
       " Document(page_content='in both g[] and in\\nthe exponent of e. The second part corresponds to the odd inputs with 2n+1 in both places. Also\\nnote that the sums now go to N/2−1 in both cases which should make sense since we have divided\\nthem into two halves.\\nWe transform Equation 5.14 to the following:\\nN/2−1 N/2−1\\n(cid:88) −j2πkn (cid:88) −j2πk(2n) −j2πk\\nG[k] = g[2n]·e N/2 + g[2n+1]·e N ·e N (5.15)\\nn=0 n=0\\n103FAST FOURIER TRANSFORM\\nIn the first summation (even inputs), we simply move the 2 into the denominator so that it is\\nnow N/2. The second summation (odd inputs) uses the power rule to separate the +1 leaving two\\ncomplex exponentials. We can further modify this equation to\\nN/2−1 N/2−1\\n(cid:88) −j2πkn −j2πk (cid:88) −j2πkn\\nG[k] = g[2n]·e N/2 +e N · g[2n+1]·e N/2 (5.16)\\nn=0 n=0\\nHere we only modify the second summation. First we pull one of the complex exponentials outside\\nof the summation since it does not depend upon n. And we also move the 2 into the denominator\\nas we did before in the first summation. Note'),\n",
       " Document(page_content='First we pull one of the complex exponentials outside\\nof the summation since it does not depend upon n. And we also move the 2 into the denominator\\nas we did before in the first summation. Note that both summations now have the same complex\\n−j2πkn\\nexponential e N/2 . Finally, we simplify this to\\nG[k] = A +WkB (5.17)\\nk N k\\nwhere A and B are the first and second summations, respectively. And recall that W = e−j2π.\\nk k\\nThis completely describes an N point FFT by separating even and odd terms into two summations.\\nFor reasons that will become clear soon, let us assume that we only want to use Equation 5.17\\nto calculate the first N/2 terms, i.e., G[0] through G[N/2−1]. And we will derive the remaining\\nN/2 terms, i.e., those from G[N/2] to G[N −1] using a different equation. While this may seem\\ncounterintuitive or even foolish (why do more math than necessary?), you will see that this will\\nallow us to take advantage of even more symmetry, and derive a pattern as we have seen in the 4\\npoint'),\n",
       " Document(page_content='or even foolish (why do more math than necessary?), you will see that this will\\nallow us to take advantage of even more symmetry, and derive a pattern as we have seen in the 4\\npoint FFT.\\nInordertocalculatethehigherfrequenciesG[N/2]toG[N−1], letusderivethesameequations\\nbut this time using k = N/2,N/2+1,...,N/2−1. Thus, we wish to calculate\\nN−1\\n(cid:88) −j2π(k+N/2)n\\nG[k+N/2] = g[n]·e N for k = 0,...,N/2−1 (5.18)\\nn=0\\nThis is similar to Equation 5.13 with different indices, i.e., we replace k from Equation 5.13 with\\nk +N/2. Using the same set of transformations that we did previously, we can move directly to\\nthe equivalent to Equation 5.16, but replacing all instances of k with k+N/2 which yields\\nN/2−1 N/2−1\\n(cid:88) −j2π(k+N/2)n −j2π(k+N/2) (cid:88) −j2π(k+N/2)n\\nG[k+N/2] = g[2n]·e N/2 +e N · g[2n+1]·e N/2 (5.19)\\nn=0 n=0\\nWe can reduce the complex exponential in the summations as follows:\\n−j2π(k+N/2)n −j2πkn −j2π(N/2)n −j2πkn −j2πkn\\ne N/2 = e N/2 ·e N/2 = e N/2 ·e−j2πn = e N/2 ·1 (5.20)\\nThe'),\n",
       " Document(page_content='g[2n+1]·e N/2 (5.19)\\nn=0 n=0\\nWe can reduce the complex exponential in the summations as follows:\\n−j2π(k+N/2)n −j2πkn −j2π(N/2)n −j2πkn −j2πkn\\ne N/2 = e N/2 ·e N/2 = e N/2 ·e−j2πn = e N/2 ·1 (5.20)\\nThe first reduction uses the power rule to split the exponential. The second reduction cancels the\\nterm N/2 in the second exponential. The final reduction uses that fact that n is a non-negative\\ninteger, and thus e−j2πn will always be a rotation of multiple of 2π. This means that this term is\\nalways equal to 1.\\nNow let us tackle the second complex exponential\\ne−j2π( Nk+N/2) = e−j N2πk ·e−j2 NπN/2 = e−j N2πk ·e−jπ = −e−j N2πk (5.21)\\nThe first reduction splits the exponential using the power rule. The second reduction does some\\nsimplifications on the second exponential. We get the final term by realizing that e−jπ = −1.\\n104FAST FOURIER TRANSFORM\\ng[0] G[0]\\ng[2] N/2 G[1]\\nPoint\\ng[4] FFT G[2]\\n.... . .\\n.... . .\\n.... . .\\ng[N - 2] G[N/2 -1]\\nW0\\ng[1] N G[N/2]\\n-\\nW1\\ng[3] N/2 N G[N/2 +1]\\n-\\nPoint\\nW2\\ng[5]'),\n",
       " Document(page_content='realizing that e−jπ = −1.\\n104FAST FOURIER TRANSFORM\\ng[0] G[0]\\ng[2] N/2 G[1]\\nPoint\\ng[4] FFT G[2]\\n.... . .\\n.... . .\\n.... . .\\ng[N - 2] G[N/2 -1]\\nW0\\ng[1] N G[N/2]\\n-\\nW1\\ng[3] N/2 N G[N/2 +1]\\n-\\nPoint\\nW2\\ng[5] FFT N G[N/2 +2]\\n. . - .\\n. . WN/2-1 .\\ng[N -1] . . N . G[N -1]\\n-\\nFigure 5.3: Building an N point FFT from two N/2 point FFTs. The upper N/2 point FFT is\\nperformed on the even inputs; the lower N/2 FFT uses the odd inputs.\\nBy substituting Equations 5.20 and 5.21 into Equation 5.19, we get\\nN/2−1 N/2−1\\n(cid:88) −j2πkn −j2πk (cid:88) −j2πkn\\nG[k+N/2] = g[2n]·e N/2 −e N · g[2n+1]·e N/2 (5.22)\\nn=0 n=0\\nNote the similarity to Equation 5.16. We can put it in terms of Equation 5.17 as\\nG[k+N/2] = A −WkB (5.23)\\nk N k\\nWe can use Equations 5.17 and 5.23 to create an N point FFT from two N/2 point FFTs.\\nRemember that A corresponds to the even input values, and B is a function of the odd input\\nk k\\nvalues. Equation 5.17 covers the first N/2 terms, and Equation 5.23 corresponds to the higher'),\n",
       " Document(page_content='point FFTs.\\nRemember that A corresponds to the even input values, and B is a function of the odd input\\nk k\\nvalues. Equation 5.17 covers the first N/2 terms, and Equation 5.23 corresponds to the higher N/2\\nfrequencies.\\nFigure 5.3 shows an N point FFT derived from two N/2 point FFTs. A corresponds to the\\nk\\ntop N/2 FFT, and B is the bottom N/2 FFT. The output terms G[0] through G[N/2−1] are\\nk\\nmultiplied by W0 while the output terms G[N/2] through G[N −1] are multiplied by −W0. Note\\nN N\\nthat the inputs g[] are divided into even and odd elements feeding into the top and bottom n/2\\npoint FFTs, respectively.\\nWe can use the general formula for creating the FFT that was just derived to recursively create\\nthe N/2 point FFT. That is, each of the N/2 point FFTs can be implemented using two N/4 point\\nFFTs. And each N/4 point FFT uses two N/8 point FFTs, and so on until we reach the base case,\\na 2 point FFT.\\nFigure 5.4 shows an 8 point FFT and highlights this recursive structure. The boxes with'),\n",
       " Document(page_content='And each N/4 point FFT uses two N/8 point FFTs, and so on until we reach the base case,\\na 2 point FFT.\\nFigure 5.4 shows an 8 point FFT and highlights this recursive structure. The boxes with the\\ndotted lines indicate different sizes of FFT. The outermost box indicates an 8 point FFT. This is\\ncomposed by two 4 point FFTs. Each of these 4 point FFTs have two 2 point FFTs for a total of\\nfour 2 point FFTs.\\n105FAST FOURIER TRANSFORM\\ng[0] g[0] G[0]\\nW0\\ng[1] g[4] 8 G[1]\\n-\\nW0\\ng[2] g[2] 8 G[2]\\n-\\nW0 W2\\ng[3] g[6] 8 8 G[3]\\nBit - -\\nReverse\\nW0\\ng[4] g[1] 8 G[4]\\n-\\nW0 W1\\ng[5] g[5] 8 8 G[5]\\n- -\\nW0 W2\\ng[6] g[3] 8 8 G[6]\\n- -\\nW0 W2 W3\\ng[7] g[7] 8 8 8 G[7]\\n- - -\\nLegend\\n2 point FFT Stage 1 Stage 2 Stage 3\\n4 point FFT\\n8 point FFT\\nFigure 5.4: An 8 point FFT built recursively. There are two 4 point FFTs, which each use two\\n2 point FFTs. The inputs must be reordered to even and odd elements twice. This results in\\nreordering based upon the bit reversal of the indices.\\n106FAST FOURIER TRANSFORM\\nAlso note that the'),\n",
       " Document(page_content='use two\\n2 point FFTs. The inputs must be reordered to even and odd elements twice. This results in\\nreordering based upon the bit reversal of the indices.\\n106FAST FOURIER TRANSFORM\\nAlso note that the inputs must be reordered before they are feed into the 8 point FFT. This is\\ndue to the fact that the different N/2 point FFTs take even and odd inputs. The upper four inputs\\ncorrespond to even inputs and the lower four inputs have odd indices. However, they are reordered\\ntwice. If we separate the even and odd inputs once we have the even set {g[0],g[2],g[4],g[6]} and\\nthe odd set {g[1],g[3],g[5],g[7]}. Now let us reorder the even set once again. In the even set g[0]\\nand g[4] are the even elements, and g[2] and g[6] are the odd elements. Thus reordering it results\\nin the set {g[0],g[4],g[2],g[6]}. The same can be done for the initial odd set yielding the reordered\\nset {g[1],g[5],g[3],g[7]}.\\nThe final reordering is done by swapping values whose indices are in bit reversed order. Table\\n5.1'),\n",
       " Document(page_content='The same can be done for the initial odd set yielding the reordered\\nset {g[1],g[5],g[3],g[7]}.\\nThe final reordering is done by swapping values whose indices are in bit reversed order. Table\\n5.1 shows the indices and their three bit binary values. The table shows the eight indices for the\\n8 point FFT, and the corresponding binary value for each of those indices in the second column.\\nThe third column is the bit reversed binary value of the second column. And the last column is\\nthe decimal number corresponding the reversed binary number.\\nTable 5.1: The index, three bit binary value for that index, bit reversed binary value, and the\\nresulting bit reversed index.\\nIndex Binary Reversed Reversed\\nBinary Index\\n0 000 000 0\\n1 001 100 4\\n2 010 010 2\\n3 011 110 6\\n4 100 001 1\\n5 101 101 5\\n6 110 011 3\\n7 111 111 7\\nLooking at the first row, the initial index 0, has a binary value of 000, which when reversed\\nremains 000. Thus this index does not need to be swapped. Looking at Figure 5.4 we see that this\\nis'),\n",
       " Document(page_content='111 7\\nLooking at the first row, the initial index 0, has a binary value of 000, which when reversed\\nremains 000. Thus this index does not need to be swapped. Looking at Figure 5.4 we see that this\\nis true. g[0] remains in the same location. In the second row, the index 1 has a binary value 001.\\nWhen reversed this is 100 or 4. Thus, the data that initially started at index 1, i.e., g[1] should\\nend up in the fourth location. And looking at index 4, we see the bit reversed value is 1. Thus g[1]\\nand g[4] are swapped.\\nThis bit reversal process works regardless of the input size of the FFT, assuming that the FFT\\nis a power of two. FFT are commonly a power of two since this allows them to be recursively\\nimplemented.\\nIn an 32 point FFT, index 1 is swapped with which index? Which index is index 2 is\\nswapped with?\\nThis completes our mathematical treatment of the FFT. There are plenty of more details about\\ntheFFT,andhowtooptimizeit.'),\n",
       " Document(page_content='index 1 is swapped with which index? Which index is index 2 is\\nswapped with?\\nThis completes our mathematical treatment of the FFT. There are plenty of more details about\\ntheFFT,andhowtooptimizeit. Youmaythinkthatwespenttoomuchtimealreadydiscussingthe\\nfinerdetailsoftheFFT;thisisabookonparallelprogrammingforFPGAsandnotondigitalsignal\\nprocessing. This highlights an important part of creating an optimum hardware implementation –\\nthe designer must have a good understanding of the algorithm under development. Without that,\\n107FAST FOURIER TRANSFORM\\nit is difficult to create a good implementation. The next section deals with how to create a good\\nFFT implementation.\\n5.2 Baseline Implementation\\nIn the remainder of this chapter, we discuss different methods to implement the Cooley-Tukey FFT\\n[16] algorithm using the Vivado(cid:13)R HLS tool. This is the same algorithm that we described in the\\nprevioussection. Westartwithacommonversionofthecode, andthendescribehowtorestructure\\nit to achieve a'),\n",
       " Document(page_content='algorithm using the Vivado(cid:13)R HLS tool. This is the same algorithm that we described in the\\nprevioussection. Westartwithacommonversionofthecode, andthendescribehowtorestructure\\nit to achieve a better hardware design.\\nWhen performed sequentially, the O(nlogn) operations in the FFT require O(nlogn) time\\nsteps. Typically, a parallel implementation will perform some portion of the FFT in parallel. One\\ncommon way of parallelizing the FFT is to organize the computation into logn stages, as shown\\nin Figure 5.8. The operations in each stage are dependent on the operations of the previous stage,\\nnaturally leading to a pipelining across the tasks. Such an architecture allows logn FFTs to be\\ncomputed simultaneously with a task interval determined by the architecture of each stage. We\\ndiscuss task pipelining using the dataflow directive in Section 5.4.\\nEach stage in the FFT also contains significant parallelism, since each butterfly computation\\nis independent of other butterfly computations'),\n",
       " Document(page_content='pipelining using the dataflow directive in Section 5.4.\\nEach stage in the FFT also contains significant parallelism, since each butterfly computation\\nis independent of other butterfly computations in the same stage. In the limit, performing n/2\\nbutterfly computations every clock cycle with a Task Interval of 1 can allow the entire stage to\\nbe computed with a Task Interval of 1. When combined with a dataflow architecture, all of\\nthe parallelism in the FFT algorithm can be exploited. Note, however that although such an\\narchitecture can be constructed, it is almost never used except for very small signals, since an\\nentire new block of SIZE samples must be provided every clock cycle to keep the pipeline fully\\nutilized. For instance, a 1024-point FFT of complex 32-bit floating point values, running at 250\\nMHz would require 1024 points*(8 bytes/point)*250*109 Hz = 1Terabyte/second of data into the\\nFPGA. In practice, a designer must match the computation architecture to the data rate'),\n",
       " Document(page_content='running at 250\\nMHz would require 1024 points*(8 bytes/point)*250*109 Hz = 1Terabyte/second of data into the\\nFPGA. In practice, a designer must match the computation architecture to the data rate required\\nin a system.\\nAssumingaclockrateof250MHzandonesamplereceivedeveryclockcycle,approximately\\nhow many butterfly computations must be implemented to process every sample with a 1024-\\npoint FFT? What about for a 16384-point FFT?\\nIn the remainder of this section, we describe the optimization of an FFT with the function\\nprototype void fft(DTYPE X R[SIZE], DTYPE X I[SIZE]) where DTYPE is a user customizable data\\ntype for the representation of the input data. This may be int, float, or a fixed point type. For\\nexample, #define DTYPE int defines DTYPE as an int. Note that we choose to implement the real\\nand imaginary parts of the complex numbers in two separate arrays. The X R array holds the real\\ninput values, and the X I array holds the imaginary values. X R[i] and X I[i] hold the ith'),\n",
       " Document(page_content='the real\\nand imaginary parts of the complex numbers in two separate arrays. The X R array holds the real\\ninput values, and the X I array holds the imaginary values. X R[i] and X I[i] hold the ith complex\\nnumber in separate real and imaginary parts.\\nThere is one change in the FFT implementation that we describe in this section. Here we\\nperform an FFT on complex numbers. The previous section uses only real numbers. While\\nthis may seem like a major change, the core ideas stay the same. The only differences are that\\nthe data has two values (corresponding to the real and imaginary part of the complex number),\\nand the operations (add, multiply, etc.) are complex operations.\\n108FAST FOURIER TRANSFORM\\nThis function prototype forces an in-place implementation. That is, the output data is stored\\nin the same array as the input data. This eliminates the need for additional arrays for the output\\ndata, which reduces the amount of memory that is required for the implementation. However,\\nthis may'),\n",
       " Document(page_content='the same array as the input data. This eliminates the need for additional arrays for the output\\ndata, which reduces the amount of memory that is required for the implementation. However,\\nthis may limit the performance due to the fact that we must read the input data and write the\\noutput data to the same arrays. Using separate arrays for the output data is reasonable if it can\\nincrease the performance. There is always a tradeoff between resource usage and performance; the\\nsame is true here. The best implementation depends upon the application requirements (e.g., high\\nthroughput, low power, size of FPGA, size of the FFT, etc.).\\nWe start with code for an FFT that would be typical for a software implementation. Figure\\n5.5 shows a nested three for loop structure. The outer for loop, labeled stage loop implements one\\nstage of the FFT during each iteration. There are log (N) stages where N is the number of input\\n2\\nsamples. The stages are clearly labeled in Figure 5.4; this 8 point FFT has log'),\n",
       " Document(page_content='loop implements one\\nstage of the FFT during each iteration. There are log (N) stages where N is the number of input\\n2\\nsamples. The stages are clearly labeled in Figure 5.4; this 8 point FFT has log (8) = 3 stages. You\\n2\\ncan see that each stage performs the same amount of computation, or the same number of butterfly\\noperations. In the 8 point FFT, each stage has four butterfly operations.\\nFor an N point FFT, how many butterfly operations are there in each stage? How many\\ntotal butterfly operations are there for the entire FFT?\\nThe second for loop, labeled butterfly loop, performs all of the butterfly operations for the\\ncurrent stage. butterfly loop has another nested for loop, labeled dft loop. Each iteration of dft loop\\nperforms one butterfly operation. Remember that we are dealing with complex numbers and must\\nperform complex additions and multiplications.\\nThe first line in dft loop determines the offset of the butterfly. Note that the “width” of the\\nbutterfly operations changes'),\n",
       " Document(page_content='complex numbers and must\\nperform complex additions and multiplications.\\nThe first line in dft loop determines the offset of the butterfly. Note that the “width” of the\\nbutterfly operations changes depending upon the stage. Looking at Figure 5.4, Stage 1 performs\\nbutterfly operations on adjacent elements, Stage 2 performs butterfly operations on elements with\\nindex differing by two, and Stage 3 performs butterfly operations on elements with index differing\\nby four. This difference is computed and stored in the i lower variable. Notice that this offset,\\nstored in the variable numBF, is different in every stage.\\nThe remaining operations in dft loop perform multiplication by the twiddle factor and an ad-\\ndition or subtraction operation. The variables temp R and temp I hold the real and imaginary\\nportions of the data after multiplication by the twiddle factor W. The variables c and s are the\\nreal and imaginary parts of W, which is calculated using the sin() and cos() builtin functions.'),\n",
       " Document(page_content='of the data after multiplication by the twiddle factor W. The variables c and s are the\\nreal and imaginary parts of W, which is calculated using the sin() and cos() builtin functions. We\\ncould also use the CORDIC, such as the one developed in Chapter 3, to have more control over the\\nimplementation. Lastly, elements of the X R[] and X I[] arrays are updated with the result of the\\nbutterfly computation.\\ndft loop and butterfly loop each execute a different number of times depending upon the stage.\\nHowever the total number of times that the body of dft loop is executed in one stage is constant.\\nThe number of iterations for the butterfly for loop depends upon the number of unique W twiddle\\nfactors in that stage. Referring again to Figure 5.4, we can see that Stage 1 uses only one twiddle\\nfactor, in this case W0. Stage 2 uses two unique twiddle factors and Stage 3 uses four different\\n8\\nW values. Thus, butterfly loop has only one iteration in Stage 1, 2 iterations in stage 2, and'),\n",
       " Document(page_content='in this case W0. Stage 2 uses two unique twiddle factors and Stage 3 uses four different\\n8\\nW values. Thus, butterfly loop has only one iteration in Stage 1, 2 iterations in stage 2, and four\\niterations in stage 3. Similarly, the number of iterations of dft loop changes. It iterates four times\\nfor an 8 point FFT in Stage 1, two times in Stage 2, and only one time in stage 3. However in\\nevery stage, the body of dft loop is executed the same number of times in total, executing a total\\nof four butterfly operations for each stage an 8 point FFT.\\n109FAST FOURIER TRANSFORM\\nvoid fft(DTYPE X R[SIZE], DTYPE X I[SIZE]) {\\nDTYPE temp R; // temporary storage complex variable\\nDTYPE temp I; // temporary storage complex variable\\nint i, j, k; // loop indexes\\nint i lower; // Index of lower point in butterfly\\nint step, stage, DFTpts;\\nint numBF; // Butterfly Width\\nint N2 = SIZE2; // N2=N>>1\\nbit reverse(X R, X I);\\nstep = N2;\\nDTYPE a, e, c, s;\\nstage loop:\\nfor (stage = 1; stage <= M; stage++) {\\n// Do M stages'),\n",
       " Document(page_content='step, stage, DFTpts;\\nint numBF; // Butterfly Width\\nint N2 = SIZE2; // N2=N>>1\\nbit reverse(X R, X I);\\nstep = N2;\\nDTYPE a, e, c, s;\\nstage loop:\\nfor (stage = 1; stage <= M; stage++) {\\n// Do M stages of butterflies\\nDFTpts = 1 << stage; // DFT = 2ˆstage = points in sub DFT\\nnumBF = DFTpts / 2; // Butterfly WIDTHS in sub−DFT\\nk = 0;\\ne = −6.283185307178 / DFTpts;\\na = 0.0;\\n// Perform butterflies for j−th stage\\nbutterfly loop:\\nfor (j = 0; j < numBF; j++) {\\nc = cos(a);\\ns = sin(a);\\na = a + e;\\n// Compute butterflies that use same W∗∗k\\ndft loop:\\nfor (i = j; i < SIZE; i += DFTpts) {\\ni lower = i + numBF; // index of lower point in butterfly\\ntemp R = X R[i lower] ∗ c − X I[i lower] ∗ s;\\ntemp I = X I[i lower] ∗ c + X R[i lower] ∗ s;\\nX R[i lower] = X R[i] − temp R;\\nX I[i lower] = X I[i] − temp I;\\nX R[i] = X R[i] + temp R;\\nX I[i] = X I[i] + temp I;\\n}\\nk += step;\\n}\\nstep = step / 2;\\n}\\n}\\nFigure 5.5: A common implementation for the FFT using three nested for loops. While this may\\nwork well running as software'),\n",
       " Document(page_content='R[i] + temp R;\\nX I[i] = X I[i] + temp I;\\n}\\nk += step;\\n}\\nstep = step / 2;\\n}\\n}\\nFigure 5.5: A common implementation for the FFT using three nested for loops. While this may\\nwork well running as software on a processor, it is far from optimal for a hardware implementation.\\n110FAST FOURIER TRANSFORM\\nVivado(cid:13)R HLS performs significant static analysis on each synthesized function, including\\ncomputingboundsonthenumberoftimeseachloopcanexecute. Thisinformationcomesfrom\\nmany sources, including variable bitwidths, ranges, and assert() functions in the code. When\\ncombined with the loop II, Vivado(cid:13)R HLS can compute bounds on the latency or interval of the\\nFFT function. In some cases (usually when loop bounds are variable or contain conditional\\nconstructs), the tool is unable to compute the latency or interval of the code and returns\\n‘’?’. When synthesizing the code in Figure 5.5, Vivado(cid:13)R HLS may not be able to determine\\nthe number of times that butterfly loop and dft loop'),\n",
       " Document(page_content='the latency or interval of the code and returns\\n‘’?’. When synthesizing the code in Figure 5.5, Vivado(cid:13)R HLS may not be able to determine\\nthe number of times that butterfly loop and dft loop iterate because these loops have variable\\nbounds.\\nThe tripcount directive enables the user to specify to the Vivado(cid:13)R HLS tool more informa-\\ntion about the number of times a loop is executed which can be used by the tool to analyze the\\nperformance of the design. It takes three optional arguments min, max, and average. In this\\ncode, we could add a directive to dft loop. By applying this directive, the Vivado(cid:13)R HLS tool\\ncancalculateboundsonthelatencyandintervalvaluefortheloopandtheoveralldesign. Note\\nthat since the Vivado(cid:13)R HLS tool uses the numbers that you provide, if you give the tool an\\nincorrect tripcount then the reported task latency and task interval will be incorrect – garbage\\nin, garbage out.\\nWhat is the appropriate way to use the trip count directive for the FFT'),\n",
       " Document(page_content='the tool an\\nincorrect tripcount then the reported task latency and task interval will be incorrect – garbage\\nin, garbage out.\\nWhat is the appropriate way to use the trip count directive for the FFT in Figure 5.5?\\nShould you set the max, min, and/or average arguments? Would you need to modify the\\ntripcount arguments if the size of the FFT changes?\\n5.3 Bit Reversal\\nWe have not talked about the bit reverse function, which swaps the input data values so that we\\ncan perform an in-place FFT. This means that the inputs values are mixed, and the result is that\\nthe output data is in the correct order. We discuss that function in some detail now.\\nFigure 5.6 shows one possible implementation of the bit reverse function. It divides the code\\ninto two functions. The first is the bit reversal function (bit reverse), which reorders data in the\\ngiven arrays so that each data is in located at a different index in the array. This function calls\\nanother function, reverse bits, which takes an input integer'),\n",
       " Document(page_content='reverse), which reorders data in the\\ngiven arrays so that each data is in located at a different index in the array. This function calls\\nanother function, reverse bits, which takes an input integer and returns the bit reversed value of\\nthat input.\\nLet us start with a brief overview of the reverse bits function. The function goes bit by bit\\nthrough the input variable and shifts it into the rev variable. The for loop body consists of a few\\nbitwiseoperationsthatreorderthebitsoftheinput. Althoughtheseoperationsareindividuallynot\\nterribly complex, the intention of this code is that the for loop is completely unrolled and Vivado(cid:13)R\\nHLS can identify that the bits of the input can simply be wired to the output. As a result,\\nthe implementation of the reverse bits function should require no logic resources at all, but only\\nwires. Thisisacasewhereunrollingloopsgreatlysimplifiestheoperationsthatmustbeperformed.\\nWithoutunrollingtheloop,theindividual‘or’operationsmustbeperformedsequentially.'),\n",
       " Document(page_content='logic resources at all, but only\\nwires. Thisisacasewhereunrollingloopsgreatlysimplifiestheoperationsthatmustbeperformed.\\nWithoutunrollingtheloop,theindividual‘or’operationsmustbeperformedsequentially. Although\\nthis loop can be pipelined, the ‘or’ operation would still be implemented in logic resources in the\\nFPGA and executing the the loop would have a latency determined by the number of bits being\\nreversed (\\\\gls{fft} BITS in this case).\\n111FAST FOURIER TRANSFORM\\n#define FFT BITS 10 // Number of bits of FFT, i.e., log(1024)\\n#define SIZE 1024 // SIZE OF FFT\\n#define SIZE2 SIZE >> 1 // SIZE/2\\n#define DTYPE int\\nunsigned int reverse bits(unsigned int input) {\\nint i, rev = 0;\\nfor (i = 0; i < FFT BITS; i++) {\\nrev = (rev << 1) | (input & 1);\\ninput = input >> 1;\\n}\\nreturn rev;\\n}\\nvoid bit reverse(DTYPE X R[SIZE], DTYPE X I[SIZE]) {\\nunsigned int reversed;\\nunsigned int i;\\nDTYPE temp;\\nfor (i = 0; i < SIZE; i++) {\\nreversed = reverse bits(i); // Find the bit reversed index\\nif (i < reversed) {\\n// Swap'),\n",
       " Document(page_content='X R[SIZE], DTYPE X I[SIZE]) {\\nunsigned int reversed;\\nunsigned int i;\\nDTYPE temp;\\nfor (i = 0; i < SIZE; i++) {\\nreversed = reverse bits(i); // Find the bit reversed index\\nif (i < reversed) {\\n// Swap the real values\\ntemp = X R[i];\\nX R[i] = X R[reversed];\\nX R[reversed] = temp;\\n// Swap the imaginary values\\ntemp = X I[i];\\nX I[i] = X I[reversed];\\nX I[reversed] = temp;\\n}\\n}\\n}\\nFigure 5.6: The first stage in our FFT implementation reorders the input data. This is done\\nby swapping the value at index i in the input array with the value at the bit reversed index\\ncorresponding to i. The function reverse bits gives the bit reversed value corresponding to the input\\nargument. And the function bit reverse swaps the values in the input array.\\n112FAST FOURIER TRANSFORM\\nvoid fft(DTYPE X R[SIZE], DTYPE X I[SIZE], DTYPE OUT R[SIZE], DTYPE OUT I[SIZE])\\n{\\n#pragma HLS dataflow\\nDTYPE Stage1 R[SIZE], Stage1 I[SIZE];\\nDTYPE Stage2 R[SIZE], Stage2 I[SIZE];\\nDTYPE Stage3 R[SIZE], Stage3 I[SIZE];\\nbit reverse(X R, X I,'),\n",
       " Document(page_content='DTYPE OUT R[SIZE], DTYPE OUT I[SIZE])\\n{\\n#pragma HLS dataflow\\nDTYPE Stage1 R[SIZE], Stage1 I[SIZE];\\nDTYPE Stage2 R[SIZE], Stage2 I[SIZE];\\nDTYPE Stage3 R[SIZE], Stage3 I[SIZE];\\nbit reverse(X R, X I, Stage1 R, Stage1 I);\\nfft stage one(Stage1 R, Stage1 I, Stage2 R, Stage2 I);\\nfft stages two(Stage2 R, Stage2 I, Stage3 R, Stage3 R);\\nfft stage three(Stage3 R, Stage3 I, OUT R, OUT I);\\n}\\nFigure 5.7: The code divides an 8 point FFT into four stages, each of which is a separate function.\\nThe bit reverse function is the first stages. And there are three stages for the 8 point FFT.\\nWhat is the latency of the reverse bits function when no directives are applied? What is the\\nlatency when the loop is pipelined? What is the latency when the whole function is pipelined?\\nItistemptingto“blindly”applydirectivesinordertoachieveabetterdesign. However,this\\ncan be counterproductive. The best designer has a deep understanding of both the'),\n",
       " Document(page_content='when the whole function is pipelined?\\nItistemptingto“blindly”applydirectivesinordertoachieveabetterdesign. However,this\\ncan be counterproductive. The best designer has a deep understanding of both the application\\nandtheavailableoptimizationsandcarefullyconsidersthesetogethertoachievethebestresults.\\nNow let us optimize the parent bit reverse function. This function has a single for loop that\\niterates through each index of the input arrays. Note that there are two input arrays X R[] and\\nX I[]. Since we are dealing with complex numbers, we must store both the real portion (in the\\narray X R[]), and the imaginary portion (in the array X I[]). X R[i] and X I[i] holds the real and\\nimaginary values of the i-th input. In each iteration of the for loop, we find the index reversed\\nvalue by calling the reverse bits function. Then we swap both the real and imaginary values stored\\nin the index i and the index returned by the function reverse bits. Note that as we go through all\\nSIZE indices, we will'),\n",
       " Document(page_content='reverse bits function. Then we swap both the real and imaginary values stored\\nin the index i and the index returned by the function reverse bits. Note that as we go through all\\nSIZE indices, we will eventually hit the reversed index for every value. Thus, the code only swaps\\nvalues the first time based on the condition if(i < reversed).\\n5.4 Task Pipelining\\nDividingtheFFTalgorithmintostagesenablesVivado(cid:13)R HLStogenerateanimplementationwhere\\ndifferent stages of the algorithm are operating on different data sets. This optimization, called\\ntask pipelining is enabled using the dataflow directive. This is a common hardware optimization,\\nand thus is relevant across a range of applications.\\nWe can naturally divide the FFT algorithm into log (N +1) stages where N is the number of\\n2\\npoints of the FFT. The first stage swaps each element in the input array with the element located\\nat the bit reversed address in the array. After this bit reverse stage, we perform log (N) stages of\\n2\\nbutterfly'),\n",
       " Document(page_content='the FFT. The first stage swaps each element in the input array with the element located\\nat the bit reversed address in the array. After this bit reverse stage, we perform log (N) stages of\\n2\\nbutterfly operations. Each of these butterfly stages has the same computational complexity. Figure\\n113FAST FOURIER TRANSFORM\\nX R[] Stage1 R[] Stage2 R[] Stage3 R[] OUT R[]\\nbit reverse fft stage one fft stage two fft stage three\\nX I[] Stage1 I[] Stage2 I[] Stage3 I[] OUT I[]\\ninput\\nbit reverse\\nfft stage one\\nfft stage two\\nfft stage three\\noutput\\nFigure 5.8: Dividing the FFT into different stages allows for task pipelining across each of these\\nstages. The figure shows an example with three FFT stages (i.e., an 8 point FFT). The figure\\nshows four 8 point FFT executing at the same time.\\n5.7describeshowtodevidean8pointFFTintofourseparatetasks. Thecodehasseparatefunctions\\nfor each of the tasks: bit reverse, fft stage one, fft stage two, and fft stage three. Each stage has two\\ninput arrays and two output'),\n",
       " Document(page_content='Thecodehasseparatefunctions\\nfor each of the tasks: bit reverse, fft stage one, fft stage two, and fft stage three. Each stage has two\\ninput arrays and two output arrays: one for the real portion and one for the imaginary portion of\\nthe complex numbers. Assume that the DTYPE is defined elsewhere, e.g., as an int, float or a fixed\\npoint data type.\\nRefactoring the FFT code allows us to perform task pipelining. Figure 5.8 gives an example\\nof this. In this execution, rather than wait for the first task to complete all four four functions in\\nthe code before the second task can begin, we allow the second task to start after the first task\\nhas only completed the first function bit reverse. The first task continues to execute each stage\\nin the pipeline in order, followed by the remaining tasks in order. Once the pipeline is full, all\\nfour subfunctions are executing concurrently, but each one is operating on different input data.\\nSimilarly, there are four 8 point FFTs being computed'),\n",
       " Document(page_content='tasks in order. Once the pipeline is full, all\\nfour subfunctions are executing concurrently, but each one is operating on different input data.\\nSimilarly, there are four 8 point FFTs being computed simultaneously, each one executing on a\\ndifferent component of the hardware. This shown in the middle portion of Figure 5.8. Each of the\\nvertical four stages represents one 8 point FFT. And the horizontal denotes increasing time. Thus,\\nonce we start the fourth 8 point FFT, we have four FFTs running simultaneously.\\nThedataflowdirectivecan constructseparatepipelinestages(oftencalled processes)fromboth\\nfunctions and loops. The code in Figure 5.7 uses functions only, but we could achieve a similar\\nresult with four loops instead of four functions. In fact, this result could be achieved by unrolling\\nthe outer stage loop in the original code either explicitly or using #pragma HLS unroll.\\nThe dataflow directive and the pipeline directive both generate circuits capable of pipelined\\nexecution.'),\n",
       " Document(page_content='outer stage loop in the original code either explicitly or using #pragma HLS unroll.\\nThe dataflow directive and the pipeline directive both generate circuits capable of pipelined\\nexecution. Thekeydifference isin thegranularityof thepipeline. Thepipeline directive constructs\\nan architecture that is efficiently pipelined at the cycle level and is characterized by the II of the\\npipeline. OperatorsarestaticallyscheduledandiftheIIisgreaterthanone, thenoperationscanbe\\nshared on the same operator. The dataflow directive constructs an architecture that is efficiently\\npipelined for operations that take a (possibly unknown) number of clock cycles, such as the the\\n114FAST FOURIER TRANSFORM\\nbehavior of a loop operating on a block of data. These coarse-grained operations are not statically\\nscheduledandthebehavioriscontrolleddyanmicallybythehandshakeofdatathroughthepipeline.\\nIn the case of the FFT, each stage is an operation on a block of data (the whole array) which takes\\na large number of cycles.'),\n",
       " Document(page_content='the case of the FFT, each stage is an operation on a block of data (the whole array) which takes\\na large number of cycles. Within each stage, loops execute individual operations on the data in a\\nblock. Hence, this is a case where it often makes sense to use the dataflow directive at the toplevel\\nto form a coarse-grained pipeline, combined with the pipeline directive within each loop to form\\nfine-grained pipelines of the operations on each individual data element.\\nThe dataflow directive must implement memories to pass data between different processes.\\nIn the case when Vivado(cid:13)R HLS can determine that processes access data in sequential order, it\\nimplements the memory using a FIFO. This requires that data is written into an array in the same\\norder that it is read from the array. If the is not the case, or if Vivado(cid:13)R HLS can not determine\\nif this streaming condition is met, then the memory can be implemented using a ping-pong buffer\\ninstead. The ping-pong buffer consists of'),\n",
       " Document(page_content='is not the case, or if Vivado(cid:13)R HLS can not determine\\nif this streaming condition is met, then the memory can be implemented using a ping-pong buffer\\ninstead. The ping-pong buffer consists of two (or more) conceptual blocks of data, each the size of\\nthe original array. One of the blocks can be written by the source process while another block is\\nread by the destination process. The term “ping-pong” comes from the fact that the reading and\\nwriting to each block of data alternates in every execution of the task. That is, the source process\\nwill write to one block and then switch to the other block before beginning the next task. The\\ndestination process reads from the block that the producer is not writing to. As a result, the source\\nand destination processes can never writing and reading from the same block at the same time.\\nA ping-pong buffer requires enough memory to store each communication array at least twice.\\nFIFOscanoftenbesignificantlysmaller,'),\n",
       " Document(page_content='can never writing and reading from the same block at the same time.\\nA ping-pong buffer requires enough memory to store each communication array at least twice.\\nFIFOscanoftenbesignificantlysmaller, althoughdeterminingaminimalsizeforeachfifoisoftena\\ndifficult design problem. Unlike a FIFO, however, the data in a ping-pong buffer can be written to\\nand read from in any order. Thus, FIFOs are generally the best choice when the data is produced\\nand consumed in sequential order and ping-pong buffers are a better choice when there is not such\\nregular data access patterns.\\nUsing the dataflow directive effectively still requires the behavior of each individual process\\nto be optimized. Each individual process in the pipeline can still be optimized using techniques\\nwe have seen previously such as code restructuring, pipelining, and unrolling. For example, we\\nhave already discussed some optimizations for the bit reverse function in Section 5.3. In general,\\nit is important to optimize the individual'),\n",
       " Document(page_content='restructuring, pipelining, and unrolling. For example, we\\nhave already discussed some optimizations for the bit reverse function in Section 5.3. In general,\\nit is important to optimize the individual tasks while considering overall toplevel performance\\ngoals. Many times it is best to start with small functions and understand how to optimize them in\\nisolation. As a designer, it is often easier to comprehend what is going on in a small piece of code\\nand hopefully determine the best optimizations quickly. After optimizing each individual function,\\nthen you can move up the hierarchy considering larger functions given particular implementations\\nof low level functions, eventually reaching the toplevel function.\\nHowever, the local optimizations must be considered in the overall scope of the goals. In\\nparticularfordataflowdesignstheachievedintervalfortheoverallpipelinecanneverbesmallerthan\\nthe interval of each individual process. Looking again at Figure 5.7, assume that bit reverse has'),\n",
       " Document(page_content='goals. In\\nparticularfordataflowdesignstheachievedintervalfortheoverallpipelinecanneverbesmallerthan\\nthe interval of each individual process. Looking again at Figure 5.7, assume that bit reverse has an\\ninterval of 8 cycles, fft stage one takes 12 cycles, fft stage two requires 12 cycles, and fft stage three\\ntakes14cycles. Whenusingdataflow, theoveraltaskintervalis14, determinedbythemaximumof\\nall of the tasks/functions. This means that you should be careful in balancing optimizations across\\ndifferentprocesseswiththegoalofcreatingabalancedpipelinewheretheintervalofeachprocessis\\napproximately the same. In this example, improving the interval of the bit reverse function cannot\\nimprove the overall interval of the fft function. In fact, it might be beneficial to increase the latency\\nof the bit reverse function, if it can be achieved with significantly fewer resources.\\n115FAST FOURIER TRANSFORM\\n5.5 Conclusion\\nThe overall goal is to create the most optimal design, which is a function of your'),\n",
       " Document(page_content='reverse function, if it can be achieved with significantly fewer resources.\\n115FAST FOURIER TRANSFORM\\n5.5 Conclusion\\nThe overall goal is to create the most optimal design, which is a function of your application needs.\\nThis may be to create the smallest implementation. Or the goal could be creating something that\\ncan perform the highest throughput implementation regardless of the size of the FPGA or the\\npower/energy constraints. Or the latency of delivering the results may matter if the application\\nhas real-time constraints. All of the optimizations change these factors in different ways.\\nIn general, there is no one algorithm on how to optimize your design. It is a complex function\\nof the application, design constraints, and the inherent abilities of the designer himself. Yet, it\\nis important that the designer have a deep understanding of the application itself, the design\\nconstraints, and the abilities of the synthesis tool.\\nWeattemptedtoillustratethesebitsofwisdominthischapter.'),\n",
       " Document(page_content='important that the designer have a deep understanding of the application itself, the design\\nconstraints, and the abilities of the synthesis tool.\\nWeattemptedtoillustratethesebitsofwisdominthischapter. WhiletheFFTisawellstudied\\nalgorithm, with a large number of known hardware implementation tricks, it still serves as a good\\nexemplar for high-level synthesis. We certainly did not give all of the tricks for optimization; we\\nleave that as an exercise in Chapter ?? where we task the designer to create an simple orthogonal\\nfrequency-division multiplexing receiver. The core of this an FFT. Regardless, we attempted to\\nprovide some insight into the key optimizations here, which we hope serve as a guide to how to\\noptimize the FFT using the Vivado(cid:13)R HLS tool.\\nFirst and foremost, understand the algorithm. We spent a lot of time explaining the basics\\nof the FFT, and how it relates to the DFT. We hope that the reader understands that this is\\nthe most important part of building optimal'),\n",
       " Document(page_content='the algorithm. We spent a lot of time explaining the basics\\nof the FFT, and how it relates to the DFT. We hope that the reader understands that this is\\nthe most important part of building optimal hardware. Certainly, the designer could translate\\nC/MATLAB/Java/Python code into Vivado(cid:13)R HLS and get an working implementation. And that\\nsame designer could somewhat blindly apply directives to achieve better results. But that designer\\nis not going to get anywhere close to optimal results without a deep understanding of the algorithm\\nitself.\\nSecond, we provide an introduction to task level pipelining using the dataflow directive. This\\nis a powerful optimization that is not possible through code restructuring. I.e., the designer must\\nuse this optimization to get such a design. Thus, it is important that the designer understand its\\npower, drawbacks, and usage.\\nAdditionally, we give build upon some of the optimizations from previous chapters, e.g., loop\\nunrolling and pipelining. All of'),\n",
       " Document(page_content='important that the designer understand its\\npower, drawbacks, and usage.\\nAdditionally, we give build upon some of the optimizations from previous chapters, e.g., loop\\nunrolling and pipelining. All of these are important to get an optimized FFT hardware design.\\nWhile we did not spend too much time on these optimizations, they are extremely important.\\nFinally, we tried to impress on the reader that these optimizations cannot be done in isolation.\\nSometimes the optimizations are independent, and they can be done in isolation. For example,\\nwe can focus on one of the tasks (e.g., in the bit reverse function as we did in Section 5.3). But\\nmany times different optimizations will effect another. For example, the inline directive will effect\\nthe way the pipelining of a function. And in particular, the way that we optimize tasks/functions\\ncan propagate itself up through the hierarchy of functions. The takeaway is that it is extremely\\nimportant that the designer understand the effects of the'),\n",
       " Document(page_content='the way that we optimize tasks/functions\\ncan propagate itself up through the hierarchy of functions. The takeaway is that it is extremely\\nimportant that the designer understand the effects of the optimizations on the algorithm, both\\nlocally and globally.\\n116Chapter 6\\nSparse Matrix Vector Multiplication\\nSparse matrix vector multiplication (SpMV) takes a sparse matrix, i.e., one in which most of its\\nelements are zero, and multiplies it by a vector. The vector itself may be sparse as well, but often\\nit is dense. This is a common operation in scientific applications, economic modeling, data mining,\\nand information retrieval. For example, it is used as an iterative method for solving sparse linear\\nsystems and eigenvalue problems. It is an operation in PageRank and it is also used in computer\\nvision, e.g., image reconstruction.\\nThis chapter introduces several new HLS concepts, and reinforces some previously discussed\\noptimization. One goal of the chapter is to introduce a more complex data'),\n",
       " Document(page_content='e.g., image reconstruction.\\nThis chapter introduces several new HLS concepts, and reinforces some previously discussed\\noptimization. One goal of the chapter is to introduce a more complex data structure. We use a\\ncompressed row storage (CRS) representation to hold the sparse matrix. Another goal is to show\\nhow to perform testing. We build a simple structure for a testbench that can be used to help\\ndetermine if the code is functionally correct. This is an important aspect of hardware design, and\\nVivado(cid:13)R HLS makes it easy to test many aspects of the generated RTL with the same high-level\\nC testbench. This is one of the big advantages of HLS over RTL design. We also show how you\\ncan perform C/RTL cosimulation using the testbench and Vivado(cid:13)R HLS tool. This is necessary\\nto derive the performance characteristics for the different SpMV designs. Since the execution time\\ndepends upon the number of entries in the sparse matrix, we must use input data in order to\\ndetermine the'),\n",
       " Document(page_content='the performance characteristics for the different SpMV designs. Since the execution time\\ndepends upon the number of entries in the sparse matrix, we must use input data in order to\\ndetermine the clock cycles for the task interval and task latency.\\n6.1 Background\\nFigure 6.1 shows an example of a 4×4 matrix M represented in two different ways. Figure 6.1\\na) shows the normal representation of the matrix as a two-dimensional array of 16 elements. Each\\nelement is store in its own location in the array. Figure 6.1 b) shows the same matrix represented\\nin CRS format. The CRS representation is a data structure consisting of three arrays. The values\\narray holds the value of each non-zero element in the matrix. The columnIndex and rowPtr arrays\\nencode information about the location of these non-zero elements in the matrix. columnIndex\\nstores the column of each element, while rowPtr contains the index in values of the first element\\nin each row. The CRS format avoids storing values in the matrix'),\n",
       " Document(page_content='elements in the matrix. columnIndex\\nstores the column of each element, while rowPtr contains the index in values of the first element\\nin each row. The CRS format avoids storing values in the matrix that are zero, although there is\\nnothing to prevent a zero from being explicitly represented in the values array. In the example,\\nhowever, we see that the values array does not, in fact, contain any zero elements. The tradeoff\\nis that some additional book-keeping information (the columnIndex and rowPtr arrays) in order\\nto properly interpret and manipulate the matrix. The CRS form is commonly used when large\\nmatrices contain only a small number of non-zero elements (typically 10 percent or less), enabling\\nthesematricestobestoredwithlessmemoryandmanipulatedwithfeweroperations. However, the\\n117SPARSE MATRIX VECTOR MULTIPLICATION\\na) b)\\nMatrix M\\nvalues\\n3 4 0 0 3 4 5 9 2 3 1 4 6\\n0 5 9 0\\ncolumnIndex\\n2 0 3 1 0 1 1 2 0 2 3 1 3\\n0 4 0 6\\nrowPtr\\n0 2 4 7 9\\nFigure 6.1: A 4×4 matrix M represented in two'),\n",
       " Document(page_content='MATRIX VECTOR MULTIPLICATION\\na) b)\\nMatrix M\\nvalues\\n3 4 0 0 3 4 5 9 2 3 1 4 6\\n0 5 9 0\\ncolumnIndex\\n2 0 3 1 0 1 1 2 0 2 3 1 3\\n0 4 0 6\\nrowPtr\\n0 2 4 7 9\\nFigure 6.1: A 4×4 matrix M represented in two different ways: as a ‘dense’ matrix stored in a\\ntwo-dimensional array, and as a sparse matrix stored in the compressed row storage (CRS) form,\\na data structure consisting of three arrays.\\nCRS form has no requirements about the sparsity of the matrix and can be used for any matrix.\\nThis makes it a general approach that can be used for any matrix, but not necessarily the most\\nefficient. The CRS form is also not the only efficient representation of sparse matrices. Depending\\non the characteristics of the matrix and the types of operations to be performed, other sparse\\nrepresentations can also be used.\\nMoreprecisely, theCRSformatusesadatastructureconsistingofthreearrays: values, colIndex,\\nand rowPtr. The values array and the columnIndex has an entry for each of the non-zero elements\\nin the sparse'),\n",
       " Document(page_content='theCRSformatusesadatastructureconsistingofthreearrays: values, colIndex,\\nand rowPtr. The values array and the columnIndex has an entry for each of the non-zero elements\\nin the sparse matrix M. These arrays represent the matrix M stored in a row-wise fashion, i.e.,\\nleft to right, and top to bottom. The data in the matrix is stored in the values array, while the\\ncolumnIndex array contains the horizontal location of the data in the matrix. If values[k] represents\\nM then colIndex[k] = j. The array rowPtr has size n+1 for an n-row matrix. rowPtr[k] contains\\nij\\nthe total number of elements in all the rows in the matrix prior to row k, with the first element\\nrowPtr[0] = 0 and the last element rowPtr[n] always giving the total number of non-zero elements\\nthe matrix. As a result, if values[k] represents M , then rowPtr[i] ≤ k <rowPtr[i+1]. If row k\\nij\\ncontains any non-zero elements, then rowPtr[k] will contain the index of the first element in the\\nrow.'),\n",
       " Document(page_content='matrix. As a result, if values[k] represents M , then rowPtr[i] ≤ k <rowPtr[i+1]. If row k\\nij\\ncontains any non-zero elements, then rowPtr[k] will contain the index of the first element in the\\nrow. Notethatiftherearerowsinthematrixwithoutanon-zeroelement, thenvaluesintherowPtr\\narray will repeat.\\nLooking at Figure 6.1 a), we can scan the matrix in row-major order to determine the values\\narray in CRS form. Whenever we find a non-zero element, its value is stored at the next available\\nindex i in the values array and its column is stored at columnIndex[i]. In addition, whenever we\\nstart scanning a new row, we store the next available index i in the rowPtr array. As a result, the\\nfirst element in the rowPtr array is always zero. Looking at Figure 6.1 b), we can also convert the\\nmatrix back to a two-dimensional array representation. The first step is to determine the number\\nof elements in each row of the matrix from the rowPtr array. The number of elements in row i is\\nthe difference'),\n",
       " Document(page_content='back to a two-dimensional array representation. The first step is to determine the number\\nof elements in each row of the matrix from the rowPtr array. The number of elements in row i is\\nthe difference rowPtr[i]−rowPtr[i+1]. Then the row can be reconstructed by iterating through the\\nvalues array starting at values[rowPtr[i]]. In our example matrix, the because the first two elements\\nof the rowPtr array are 0 and 2, then we know that there are 2 elements in the first row, i.e.,\\nvalues[0] and values[1]. The first non-zero element in the values data structure, values[0], is 3. This\\nvalue is in column 0, since columnIndex[0] = 0. Similarly, the second non-zero value is the value 4\\nin column 1. The second row of the matrix has elements with k ∈ [2,4), the third row has elements\\nwith k ∈ [4,7), and so on. In this case, there are 9 non-zero entries, thus that last entry in the\\nrowPtr data structure is 9.\\n118SPARSE MATRIX VECTOR MULTIPLICATION\\n#include ”spmv.h”\\nvoid spmv(int rowPtr[NUM ROWS+1],'),\n",
       " Document(page_content='so on. In this case, there are 9 non-zero entries, thus that last entry in the\\nrowPtr data structure is 9.\\n118SPARSE MATRIX VECTOR MULTIPLICATION\\n#include ”spmv.h”\\nvoid spmv(int rowPtr[NUM ROWS+1], int columnIndex[NNZ],\\nDTYPE values[NNZ], DTYPE y[SIZE], DTYPE x[SIZE])\\n{\\nL1: for (int i = 0; i < NUM ROWS; i++) {\\nDTYPE y0 = 0;\\nL2: for (int k = rowPtr[i]; k < rowPtr[i+1]; k++) {\\n#pragma HLS unroll factor=8\\n#pragma HLS pipeline\\ny0 += values[k] ∗ x[columnIndex[k]];\\n}\\ny[i] = y0;\\n}\\n}\\nFigure 6.2: The baseline code for sparse matrix vector (SpMV) multiplication, which performs the\\noperation y = M·x. The variables rowPtr, columnIndex, and values hold M in CRS format. The\\nfirst for loop iterates across the rows while the second nested for loop iterates across the columns\\nof M by multiplying each non-zero element by the corresponding element in the vector x which\\nresults in one element in the resulting vector y.\\nGiven a 2-dimensional array representing a matrix, write the C code to convert the'),\n",
       " Document(page_content='non-zero element by the corresponding element in the vector x which\\nresults in one element in the resulting vector y.\\nGiven a 2-dimensional array representing a matrix, write the C code to convert the matrix\\nto CRS form. Write the corresponding C code to convert the matrix in CRS form back to a\\n2-dimensional array.\\nIt turns out that using the CRS form, we can multiply a sparse matrix with a vector relatively\\nefficiently without explicitly converting the matrix back to a 2-dimensional array. In fact, for large\\nmatrices with a small number of non-zero elements, sparse matrix-vector multiply is much more\\nefficient than the dense matrix-vector multiply we discussed in chapter 4. This is because we\\ncan compute the non-zero elements of the result by only looking at the non-zero elements of the\\noperands.\\n6.2 Baseline Implementation\\nFigure 6.2 provides a baseline code for sparse matrix vector multiplication. The spmv function has\\nfive arguments. The arguments rowPtr, columnIndex, and values'),\n",
       " Document(page_content='Baseline Implementation\\nFigure 6.2 provides a baseline code for sparse matrix vector multiplication. The spmv function has\\nfive arguments. The arguments rowPtr, columnIndex, and values correspond to the input matrix M\\nin CRS format. These are equivalent to the data structures shown in Figure 6.1. The argument\\ny holds the output result y and the argument x holds the input vector x to be multiplied by the\\nmatrix. The variable NUM ROWS indicates the number of rows in the matrix M. The variable\\nNNZ is the number of non-zero elements in the matrix M. Finally, the variable SIZE is the number\\nof elements in the arrays x and y.\\nThe outer for loop, labeled L1, iterates across each row of the matrix. Multiplying this row of\\nthe matrix with the vector x will produce one element of y. The inner loop labeled L2 loop across\\nthe elements in the columns of the matrix M. The L2 loop iterates rowPtr[i+1] − rowPtr[i] times,\\n119SPARSE MATRIX VECTOR MULTIPLICATION\\n#ifndef SPMV H\\n#define SPMV H\\nconst static'),\n",
       " Document(page_content='L2 loop across\\nthe elements in the columns of the matrix M. The L2 loop iterates rowPtr[i+1] − rowPtr[i] times,\\n119SPARSE MATRIX VECTOR MULTIPLICATION\\n#ifndef SPMV H\\n#define SPMV H\\nconst static int SIZE = 4; // SIZE of square matrix\\nconst static int NNZ = 9; //Number of non−zero elements\\nconst static int NUM ROWS = 4; // SIZE;\\ntypedef float DTYPE;\\nvoid spmv(int rowPtr[NUM ROWS+1], int columnIndex[NNZ],\\nDTYPE values[NNZ], DTYPE y[SIZE], DTYPE x[SIZE]);\\n#endif // MATRIXMUL H not defined\\nFigure 6.3: The header file for spmv function and testbench.\\ncorresponding to the number of non-zero entries in that row. For each entry, we read the value of\\nthe non-zero element of the M matrix from the values array and multiply it by the corresponding\\nvalue of the vector x read from the x array. That value is located at columnIndex[k] since the data\\nstructure columnIndex holds the column for the value k.\\n6.3 Testbench\\nFigure 6.4 shows a simple testbench for the spmv function. The testbench starts by'),\n",
       " Document(page_content='is located at columnIndex[k] since the data\\nstructure columnIndex holds the column for the value k.\\n6.3 Testbench\\nFigure 6.4 shows a simple testbench for the spmv function. The testbench starts by defining the\\nmatrixvector function. This is a straightforward implementation of matrix vector multiplication.\\nThis does not assume a sparse matrix and does not use the CRS format. We will compare the\\noutput results from this function with the results from our spmv function.\\nA common testbench will implement a “golden” reference implementation of the function\\nthat the designer wishes to synthesis. The testbench will then compare the results of the\\ngolden reference with those generated from the code that is synthesized by the Vivado(cid:13)R HLS\\ncode. A best practice for the testbench is to use alternative implementations for the golden\\nreference and the synthesizable code. This provides more assurance that both implementations\\nare correct.\\nThetestbenchcontinuesinthemainfunction.'),\n",
       " Document(page_content='is to use alternative implementations for the golden\\nreference and the synthesizable code. This provides more assurance that both implementations\\nare correct.\\nThetestbenchcontinuesinthemainfunction. Herewesetthefailvariableequalto0(latercode\\nsets this to 1 if the output data from spmv does not match that from the function matrixvector).\\nThen we define a set of variables that correspond to the matrix M, the input vector x and the\\noutput vector y. In case of M, we have both the “normal” form and the CSR form (stored in the\\nvariables values, columnIndex, and rowPtr). The values of the M matrix are the same as shown in\\nFigure 6.1. We have two versions of the output vector y. The y sw array stores the output from\\nthe function matrixvector and the y array has the output from the function spmv.\\nAfter defining all of the input and output variables, we call the spmv and matrixvector functions\\nusing the appropriate data. The following for loop compares the output results from both of'),\n",
       " Document(page_content='spmv.\\nAfter defining all of the input and output variables, we call the spmv and matrixvector functions\\nusing the appropriate data. The following for loop compares the output results from both of the\\nfunctions by comparing the elements from y sw with those in y. If any of them are different, we set\\nthe fail flag equal to 1. Lastly, we print out the results of the test and then return the fail variable.\\nThis testbench is relatively simple and probably insufficient to ensure that the implementation\\nis correct. Primarily, it only tests one example matrix, whereas a better testbench would test\\n120SPARSE MATRIX VECTOR MULTIPLICATION\\n#include ”spmv.h”\\n#include <stdio.h>\\nvoid matrixvector(DTYPE A[SIZE][SIZE], DTYPE ∗y, DTYPE ∗x)\\n{\\nfor (int i = 0; i < SIZE; i++) {\\nDTYPE y0 = 0;\\nfor (int j = 0; j < SIZE; j++)\\ny0 += A[i][j] ∗ x[j];\\ny[i] = y0;\\n}\\n}\\nint main(){\\nint fail = 0;\\nDTYPE M[SIZE][SIZE] = {{3,4,0,0},{0,5,9,0},{2,0,3,1},{0,4,0,6}};\\nDTYPE x[SIZE] = {1,2,3,4};\\nDTYPE y sw[SIZE];\\nDTYPE values[]'),\n",
       " Document(page_content='< SIZE; j++)\\ny0 += A[i][j] ∗ x[j];\\ny[i] = y0;\\n}\\n}\\nint main(){\\nint fail = 0;\\nDTYPE M[SIZE][SIZE] = {{3,4,0,0},{0,5,9,0},{2,0,3,1},{0,4,0,6}};\\nDTYPE x[SIZE] = {1,2,3,4};\\nDTYPE y sw[SIZE];\\nDTYPE values[] = {3,4,5,9,2,3,1,4,6};\\nint columnIndex[] = {0,1,1,2,0,2,3,1,3};\\nint rowPtr[] = {0,2,4,7,9};\\nDTYPE y[SIZE];\\nspmv(rowPtr, columnIndex, values, y, x);\\nmatrixvector(M, y sw, x);\\nfor(int i = 0; i < SIZE; i++)\\nif(y sw[i] != y[i])\\nfail = 1;\\nif(fail == 1)\\nprintf(”FAILED\\\\n”);\\nelse\\nprintf(”PASS\\\\n”);\\nreturn fail;\\n}\\nFigure 6.4: A simple testbench for our spmv function. The testbench generates one example\\nand computes the matrix vector multiplication using a sparse (spmv) and non-sparse function\\n(matrixvector).\\n121SPARSE MATRIX VECTOR MULTIPLICATION\\nmultiple matrices. It is common, for instance, to randomly generate inputs for testing, in addition\\nto explicitly verifying important corner-cases. In this case, we need to make sure to vary not only\\nthe values being operated on, which which will be passed'),\n",
       " Document(page_content='generate inputs for testing, in addition\\nto explicitly verifying important corner-cases. In this case, we need to make sure to vary not only\\nthe values being operated on, which which will be passed to our accelerator when it is executing,\\nbut also to vary the compile-time parameters which might be used to create different accelerators\\nwith different tradeoffs. The key difference is that we can randomly generate multiple data values\\nto operate on and test them all in the same execution of the program, using multiple function\\ncalls. Compile-time parameters, on the other hand, require the code to be recompiled every time\\nparameters change.\\nCreate a more sophisticated testbench which generates multiple sets of test data using\\na random number generator. The compile-time parameters of the sparse matrix should be\\nmodifiable (e.g., SIZE, NNZ, etc.). Create an HLS synthesis script which executes the same\\ncode multiple times for different reasonable compile-time parameters.\\n6.4 Specifying Loop'),\n",
       " Document(page_content='matrix should be\\nmodifiable (e.g., SIZE, NNZ, etc.). Create an HLS synthesis script which executes the same\\ncode multiple times for different reasonable compile-time parameters.\\n6.4 Specifying Loop Properties\\nIfyoudirectlysynthesizethiscode,youwillgetresultsfortheclockperiodandutilization. However,\\nyouwillnotgetthenumberofclockcycleseitherintermsoftasklatencyorinitiationinterval. This\\nisbecausethisdependsupontheinputdata,whichisexternaltothespmvfunctionitself. Primarily,\\nthe performance depends on the number of times the body of the inner loop is executed, which is\\nequal to the number of non-zero elements in M. We know that the number of non-zero elements is\\nlimitedbytheconstantNNZinthecode, butitispossibletocallthecodewithmatricesofdifferent\\nsizes, so the actual number of iterations is data-dependent. In addition, the performance may vary\\ndepending on the location of the non-zero elements and the optimization directives utilized during\\nsynthesis. To make matters worse, the number of'),\n",
       " Document(page_content='data-dependent. In addition, the performance may vary\\ndepending on the location of the non-zero elements and the optimization directives utilized during\\nsynthesis. To make matters worse, the number of iterations depends on the input in a complex way\\nand many potential inputs don’t actually represent valid matrices. Thus, it is very difficult for a\\ntool to determine the total number of clock cycles for the spmv function without complex analysis\\nand additional information. Vivado(cid:13)R HLS is unable to perform this analysis.\\nWhatarethepreconditionsforthespmvfunctiontoworkcorrectly? Provethatgiventhese\\npreconditions, the body of the inner loop does, in fact, execute exactly once for each non-zero\\nelement in the matrix.\\nThereareseveralwaystoleveragethetooltoderivesomeperformanceestimates, however. One\\nmethod is to provide the Vivado(cid:13)R HLS tool additional information about the loop bounds. This\\ncan be done using the loop tripcount directive, which enables the designer to specify a'),\n",
       " Document(page_content='One\\nmethod is to provide the Vivado(cid:13)R HLS tool additional information about the loop bounds. This\\ncan be done using the loop tripcount directive, which enables the designer to specify a minimum,\\nmaximum, and/or average number of iterations for each particular loop. By providing these values,\\nthe Vivado(cid:13)R HLS tool is capable of providing an estimate on the number of clock cycles.\\nUse the loop tripcount directive to specify minimum, maximum, and/or average number\\nof iterations for a loop with a variable bound. This enables the Vivado(cid:13)R HLS tool to provide\\nan estimate on the number of clock cycles for the design. This does not impact the results of\\nthe synthesis; it only effects the synthesis report.\\n122SPARSE MATRIX VECTOR MULTIPLICATION\\nAdd a loop tripcount directive to the spmv function. The syntax for the pragma form\\nof the directive is #pragma HLS loop tripcount min=X, max=Y, avg=Z where X, Y, and Z are\\nconstant positive integers. Which loops require this'),\n",
       " Document(page_content='to the spmv function. The syntax for the pragma form\\nof the directive is #pragma HLS loop tripcount min=X, max=Y, avg=Z where X, Y, and Z are\\nconstant positive integers. Which loops require this directive? What happens to the synthesis\\nreport when you change the different parameters (min, max, and avg)? How does this effect\\nthe clock period? How does it change the utilization results?\\nThe loop tripcount directive enables the designer to get a rough idea about the performance\\nof a function. This can enable comparison between different implementations of the same function\\neither by applying different optimization directives or by restructuring the code itself. However,\\nit may be difficult or impossible to determine the min, max, and avg parameters. It can also be\\ndifficult to provide tight bounds on the min and max parameters. If there is a testbench, there is\\nanother more accurate method to calculate the total number of clock cycles required for the spmv\\nfunction. This is done by'),\n",
       " Document(page_content='tight bounds on the min and max parameters. If there is a testbench, there is\\nanother more accurate method to calculate the total number of clock cycles required for the spmv\\nfunction. This is done by performing C/RTL cosimulation.\\n6.5 C/RTL Cosimulation\\nC/RTL cosimulation performs automatic testing of the register-transfer level (RTL) designs that\\nare generated by the Vivado(cid:13)R HLS tool. It does this by executing the synthesized code together\\nwith the provided testbench. The execution is instrumented to record the input and output values\\nfor each execution of the synthesized code. The input values are converted to cycle-by-cycle input\\nvectors. TheinputvectorsareusedinanRTL-levelsimulationofthegeneratedRTLdesignandthe\\nresultingoutput vectors arecaptured. Thetestbenchcodecanthenbeexecutedagainreplacingthe\\nsynthesized code with the captured input and output values. The testbench code can then return\\na zero value (indicating success) or a non-zero value (indicating failure).\\nThe'),\n",
       " Document(page_content='code with the captured input and output values. The testbench code can then return\\na zero value (indicating success) or a non-zero value (indicating failure).\\nThe C/RTL cosimulation flow combines the cycle-accurate RTL design generated from the\\nVivado(cid:13)R HLS tool with input values provided from the C testbench. As a result, it can gen-\\nerate accurate estimates of the performance of the generated RTL design which reflect any HLS\\noptimizations, even in the presence of data-dependent behavior. The minimum, maximum, and av-\\nerage latency and interval of the synthesized function are automatically extracted after simulation\\ncompletes.\\nNote that these numbers only correspond to the clock cycles derived from the input data used\\nby the testbench. Thus, they are only as good as the testbench itself. To put it another way, if the\\ntestbench does not exercise the function in a manner that is consistent with how it will be used\\nupon deployment, the results will not be accurate. In addition,'),\n",
       " Document(page_content='itself. To put it another way, if the\\ntestbench does not exercise the function in a manner that is consistent with how it will be used\\nupon deployment, the results will not be accurate. In addition, the input testvectors are generated\\nwith idealized timing that does not accurately model the behavior of external interfaces. The\\nactual performance may be lower if execution stalls waiting for input data, or if there is contention\\nwaiting for external memory access to complete. Nevertheless, it provides a convenient method\\nfor determining clock cycles that does not require the designer to estimate the loop bounds for a\\nvariable loop.\\nC/RTLcosimulationprovidesthelatencyforfunctionswithvariableloopbounds. Itreports\\nthe minimum, maximum, and average clock cycles for function latency and function interval.\\nThese latency values are directly dependent upon the input data from the C testbench.\\n123SPARSE MATRIX VECTOR MULTIPLICATION\\nL1 Iteration 1 L1 Iteration 2 L1 Iteration 3 L1 Iteration 4\\nL2'),\n",
       " Document(page_content='interval.\\nThese latency values are directly dependent upon the input data from the C testbench.\\n123SPARSE MATRIX VECTOR MULTIPLICATION\\nL1 Iteration 1 L1 Iteration 2 L1 Iteration 3 L1 Iteration 4\\nL2 (II = 3)\\ncolumnIndex[]\\nvalues[]\\nx[]\\ny[]\\nk values[]\\ni\\ncolumnIndex[]\\ny[]\\nx[]\\nFigure 6.5: Architecture and behavior of the spmv code with a pipelined inner loop.\\nWhat are the minimum, maximum, and average clock cycles for the spmv function latency\\nand function interval when using the testbench provided in Figure 6.4?\\n6.6 Loop Optimizations and Array Partitioning\\nNow that we have a method to gather all of the performance and utilization estimates from the\\nVivado(cid:13)R HLS tool, let us consider how to best optimize the function. Pipelining, loop unrolling,\\nanddatapartitioningarethemostcommonfirstapproachesinoptimizingadesign. Andthetypical\\napproach is to start with the innermost loop, and then move outwards as necessary.\\nIn this example, pipelining the inner L2 loop is perhaps the first and'),\n",
       " Document(page_content='Andthetypical\\napproach is to start with the innermost loop, and then move outwards as necessary.\\nIn this example, pipelining the inner L2 loop is perhaps the first and easiest optimization to\\nconsider. This overlaps the execution of the consecutive iterations of this loop, which can result is a\\nfasteroverallimplementation. Withoutpipelining, eachiterationoftheL2loopoccurssequentially.\\nNote that the iterations of the L1 loop are still done sequentially.\\nFigure6.5illustratestheapproximatemannerinwhichthespmvfunctionexecuteswhenpipelin-\\ning the L2 for loop. Each iteration of the inner L2 loop is pipelined with II=3. Pipelining allows\\nmultiple iterations of the inner loop from the same iteration of the outer loop execute concurrently.\\nIn this case, the II of the inner loop is limited by a recurrence through the accumulation. II=3 is\\nachieved because we’ve assumed that the adder has a latency of 3 clock cycles. Iterations of the\\nouter loop are not pipelined, so the inner loop must'),\n",
       " Document(page_content='by a recurrence through the accumulation. II=3 is\\nachieved because we’ve assumed that the adder has a latency of 3 clock cycles. Iterations of the\\nouter loop are not pipelined, so the inner loop must completely finish and flush the pipeline before\\nthe next iteration of the outer L2 loop begins.\\nPipeline the innermost L2 for loop. This can be done by adding a pipeline directive to the\\nspmv code from Figure 6.2. What is the achieved initiation interval (II)? What happens to the\\nresults as you specify an II argument, and increase or decrease the target II?\\nLooking at this behavior, we see that there are several factors limiting the performance of the\\n124SPARSE MATRIX VECTOR MULTIPLICATION\\nloop. One factor is the recurrence through the adder that limits the achieved loop II. A second\\nfactor is that iterations of the outer loop are not pipelined. An efficient solution for sparse matrix-\\nvector multiply would likely come close to using each multiplier and adder every clock cycle. This\\ndesign'),\n",
       " Document(page_content='that iterations of the outer loop are not pipelined. An efficient solution for sparse matrix-\\nvector multiply would likely come close to using each multiplier and adder every clock cycle. This\\ndesign is far from that.\\nIn Section 4.3 we explored several design optimization techniques, including pipelining different\\nloops, loopunrolling, andarraypartitioning. Understandingthetradeoffsbetweenthesetechniques\\ncanbesomewhatchallenging,sincetheyareoftendependentonwhatanother. Wemustoftenapply\\nthesetechniquestogetherwithacarefullychosengoalinmindinordertogetabenefitandapplying\\none technique without applying another technique can actually make matters worse. For instance,\\nwhenperformingloopunrolling, thedesignermustbecarefultounderstandtheeffectsthatthishas\\nupon memory accesses. Increasing the number of operations that can execute concurrently doesn’t\\nhelp if performance is limited by available memory ports. Similarly, providing more memory'),\n",
       " Document(page_content='memory accesses. Increasing the number of operations that can execute concurrently doesn’t\\nhelp if performance is limited by available memory ports. Similarly, providing more memory ports\\nifthereareinsufficientoperationstoutilizethosememoryports(oriftheaddressesofeachmemory\\noperationcan’tbeeasilypartitioned)canalsoincuraresourcecostwithoutincreasingperformance.\\nTo see some of the complexity in applying these combinations of transforms, we encourage you\\nto perform the following exercise:\\nSynthesize the spmv design using the directives specified in each of the ten cases from Table\\n6.1. Each case has different pipeline, unroll, and partitioning directives for the different loops\\nand arrays. These partitionings should be done across the three arrays (values, columnIndex,\\nand x). What sort of trends do you see? Does increasing the unroll and partitioning factors\\nhelp or hurt when it comes to utilization? How about performance? Why?\\nTable 6.1: Potential optimizations for sparse matrix-vector'),\n",
       " Document(page_content='do you see? Does increasing the unroll and partitioning factors\\nhelp or hurt when it comes to utilization? How about performance? Why?\\nTable 6.1: Potential optimizations for sparse matrix-vector multiplication.\\nL1 L2\\nCase 1 - -\\nCase 2 - pipeline\\nCase 3 pipeline -\\nCase 4 unroll=2 -\\nCase 5 - pipeline, unroll=2\\nCase 6 - pipeline, unroll=2, cyclic=2\\nCase 7 - pipeline, unroll=4\\nCase 8 - pipeline, unroll=4, cyclic=4\\nCase 9 - pipeline, unroll=8\\nCase 10 - pipeline, unroll=8, cyclic=8\\nCase 11 - pipeline, unroll=8, block=8\\nIf you performed the previous exercise, you should have seen that blindly applying optimization\\ndirectives may not always provide you with the expected results. It is usually more effective to\\nconsider the propertiesof anapplication underdesign, and to selectoptimizations witha particular\\ndesign goal in mind. Of course, this requires some intuition behind the capabilities and limitations\\nof a particular tool being used. While it is certainly difficult (perhaps impossible?) to'),\n",
       " Document(page_content='goal in mind. Of course, this requires some intuition behind the capabilities and limitations\\nof a particular tool being used. While it is certainly difficult (perhaps impossible?) to understand\\nevery detail of a complex tool like Vivado(cid:13)R HLS, we can build a mental model of the most critical\\naspects.\\n125SPARSE MATRIX VECTOR MULTIPLICATION\\nOne of the options we considered in cases 3 and 4 above was to increase pipeline outer loops,\\nsuch as the L1 loop in this code, rather than inner loops. This transformation has the effect of\\nincreasing the potential parallelism within one task. In order to perform this optimization, the\\nVivado(cid:13)R HLS tool must fully unroll inner loops, like the L2 loop in this code. If full unrolling is\\npossible, this can reduce the cost of calculating the loop bounds and can also eliminate recurrences\\nin the code. However, in this code, the inner loop cannot be unrolled by Vivado(cid:13)R HLS because\\nthe loop bound is not constant.\\nAdd a directive to'),\n",
       " Document(page_content='loop bounds and can also eliminate recurrences\\nin the code. However, in this code, the inner loop cannot be unrolled by Vivado(cid:13)R HLS because\\nthe loop bound is not constant.\\nAdd a directive to pipeline the outermost L1 loop, i.e., implement case 3 above. What is\\nthe initiation interval (II) when you do not set a target II? What happens to the utilization?\\nHow does explicitly increasing the II change the utilization results? How does this compare to\\npipelining the L2 loop? How does this compare to the baseline design (no directives)? What\\nis happening when you attempt to pipeline this outer loop? (hint: check the synthesis log)\\nAnother option to increase parallelism is partial loop unrolling of the inner loop, as in cases\\n5 through 10. This transformation exposes more parallelism by allowing more operations from\\nthe same loop iteration to be executed concurrently. In some cases, more operations can increase\\nperformance by enabling Vivado(cid:13)R HLS to instantiate more operators'),\n",
       " Document(page_content='allowing more operations from\\nthe same loop iteration to be executed concurrently. In some cases, more operations can increase\\nperformance by enabling Vivado(cid:13)R HLS to instantiate more operators when pipelining the inner\\nloop. However, in this case it is still difficult to improve the II of the inner loop because of the\\nrecurrence through the inner loop. However, in this case, because we have an II greater than 1,\\nmany of those operations can be shared on the same operators.\\nAn partially unrolled version of the code is shown in Figure 6.6. In this code, the L2 loop has\\nbeen split into two loops labeled L2 1 and L2 2. The innermost L2 2 executes a parameterized\\nnumber of times, given by the compile-time parameter S. The body of the inner loop contains\\nthe body of the original L2 loop, along with a condition that arises from the loop bound of the\\noriginal L2 loop. In this code, we now have an arbitrary number of multiply and add operations to\\nexecute in the body of the L2 1 loop,'),\n",
       " Document(page_content='along with a condition that arises from the loop bound of the\\noriginal L2 loop. In this code, we now have an arbitrary number of multiply and add operations to\\nexecute in the body of the L2 1 loop, given by the parameter S, and a single recurrence through\\nthe accumulation y0 += yt.\\nNote that the code in Figure 6.6 is slightly different from the code that is generated from\\nautomatic loop unrolling. Automatic loop unrolling duplicates operations, but must also preserve\\nthe order of each operation (additions in this case). This results in a long chain of operation\\ndependencies in the inner loop shown on the left side of Figure 6.7. Reordering the operations\\nresults in operation dependencies show on the right side of the figure. In this case, only the final\\naccumulation results in a recurrence. When using floating-point data types, this reordering of\\noperations can slightly change the behavior of the program, so Vivado(cid:13)R HLS does not apply this\\nkind of operation reordering'),\n",
       " Document(page_content='recurrence. When using floating-point data types, this reordering of\\noperations can slightly change the behavior of the program, so Vivado(cid:13)R HLS does not apply this\\nkind of operation reordering automatically.\\nA possible implementation of this design is shown in Figure 6.8. In this case, S = 3 to match\\nthe best achievable II where there is a latency of 3 through the adder. In this case, we see that\\nall the operations have been successfully shared on a single multiplier and adder. Comparing this\\nbehavior to the behavior in Figure 6.5, we see that there are some disadvantages. In particular,\\nthe depth of the pipeline of the inner loop is much longer, which implies that the number of cycles\\nto flush the pipeline to start a new iterations of the outer L1 loop is much larger. Processing of\\nthe non-zero elements in a row also occurs in blocks of size S. A row with 3 elements takes exactly\\nthe same time to compute as a row with one element. The remaining operations which are'),\n",
       " Document(page_content='Processing of\\nthe non-zero elements in a row also occurs in blocks of size S. A row with 3 elements takes exactly\\nthe same time to compute as a row with one element. The remaining operations which are still\\nscheduled in the loop pipeline must still ‘execute’ even though their results are discarded. In order\\nto rigorously compare the characteristics of the two designs, we need to understand the expected\\nnumber of non-zero elements in each row of the matrix. Fewer non-zero elements in each line would\\n126SPARSE MATRIX VECTOR MULTIPLICATION\\n#include ”spmv.h”\\nconst static int S = 7;\\nvoid spmv(int rowPtr[NUM ROWS+1], int columnIndex[NNZ],\\nDTYPE values[NNZ], DTYPE y[SIZE], DTYPE x[SIZE])\\n{\\nL1: for (int i = 0; i < NUM ROWS; i++) {\\nDTYPE y0 = 0;\\nL2 1: for (int k = rowPtr[i]; k < rowPtr[i+1]; k += S) {\\n#pragma HLS pipeline II=S\\nDTYPE yt = values[k] ∗ x[columnIndex[k]];\\nL2 2: for(int j = 1; j < S; j++) {\\nif(k+j < rowPtr[i+1]) {\\nyt += values[k+j] ∗ x[columnIndex[k+j]];\\n}\\n}\\ny0 += yt;\\n}\\ny[i] ='),\n",
       " Document(page_content='k += S) {\\n#pragma HLS pipeline II=S\\nDTYPE yt = values[k] ∗ x[columnIndex[k]];\\nL2 2: for(int j = 1; j < S; j++) {\\nif(k+j < rowPtr[i+1]) {\\nyt += values[k+j] ∗ x[columnIndex[k+j]];\\n}\\n}\\ny0 += yt;\\n}\\ny[i] = y0;\\n}\\n}\\nFigure 6.6: A partially unrolled version of the spmv code from Figure 6.2.\\n0\\n0\\nFigure 6.7: Two different partially unrolled versions of an accumulation. The version on the left\\nhas a recurrence with three additions, whereas the version on right only has one addition in the\\nrecurrence.\\n127SPARSE MATRIX VECTOR MULTIPLICATION\\nL1 Iteration 1 L1 Iteration 2\\nL2 1 II = 3\\ncolumnIndex[]\\nvalues[]\\nx[]\\ny[]\\nFigure6.8: Architectureandbehaviorofthespmvcodebasedonthepartiallyunrolledandpipelined\\ninner loop shown in Figure 6.6.\\nfavor the first implementation, while more non-zero elements in each line would favor the second\\nimplementation.\\nNotice that there is, to some extent a chicken-and-egg problem here. We need to know the\\ntarget device and clock period to determine the number of pipeline'),\n",
       " Document(page_content='line would favor the second\\nimplementation.\\nNotice that there is, to some extent a chicken-and-egg problem here. We need to know the\\ntarget device and clock period to determine the number of pipeline stages required for the adder to\\nmeet timing. Only after we know the number of pipeline stages (perhaps by running with S=1 and\\ninvestigating the Vivado(cid:13)R HLS logs to identify the adder recurrence) can we select an appropriate\\nversion of the parameter S that achieves II=1. Once we’ve determined S, we can run C/RTL\\ncosimulation to determine the achieved performance on a set of benchmark test data. Because of\\nthe variable loop bounds, the achieved performance is data-dependent so we might have to explore\\ndifferentvaluesofStodeterminethevaluethatmaximizesperformance. Changingthetargetdevice\\nor clock period might affect all of these decisions! Although it may seem like high-level synthesis\\nprovides little assistance in solving this problem, it’s still much faster (and possible to easily'),\n",
       " Document(page_content='clock period might affect all of these decisions! Although it may seem like high-level synthesis\\nprovides little assistance in solving this problem, it’s still much faster (and possible to easily script)\\ncompared to evaluating each new version with a new RTL design that must be verified!\\nThe behavior in Figure 6.8 is achieved when S is the same as the number of pipeline stages\\nfor the adder. What happens to the behavior when S is set larger? What happens to the\\nbehavior when is it set smaller? What happens when the target II is smaller than S? What\\nhappens when the target II is larger?\\n6.7 Conclusion\\nIn this chapter, we looked at sparse matrix-vector multiplication (SpMV). This continues our\\nstudy of matrix operations. This operation is particularly interesting because it uses a unique data\\nstructure. In order to reduce the amount of storage, the matrix is stored in a compressed row\\nstorage format. This requires a design that uses some indirect references to find the appropriate\\nentry'),\n",
       " Document(page_content='In order to reduce the amount of storage, the matrix is stored in a compressed row\\nstorage format. This requires a design that uses some indirect references to find the appropriate\\nentry in the matrix.\\nThis chapter is the first to discuss at length the testing and simulation abilities of the Vivado(cid:13)R\\nHLS tool. We provide a simple testbench for SpMV and describe how it can be integrated into\\nthe HLS work-flow. Additionally, we describe the C/RTL cosimulation features of the Vivado(cid:13)R\\nHLS tool. This is particularly important for us in order to get precise performance results. The\\ntask interval and task latency depends upon the input data. The less sparse the matrix, the more\\ncomputation that must be performed. The cosimulation provides a precise trace of execution using\\nthe given testbench. This allows the tool to compute the clock cycles to include in the performance\\nresults. Finally, we discuss optimizing the code using loop optimizations and array partitioning.\\n128Chapter'),\n",
       " Document(page_content='testbench. This allows the tool to compute the clock cycles to include in the performance\\nresults. Finally, we discuss optimizing the code using loop optimizations and array partitioning.\\n128Chapter 7\\nMatrix Multiplication\\nThis chapter looks at a bit more complex design – matrix multiplication. We consider two different\\nversions. We start with a “straightforward” implementation, i.e., one that takes two matrices as\\ninputs and outputs the result of their multiplication. We call this complete matrix multiplication.\\nThen, we look at a block matrix multiplication. Here the input matrices are feed into the function\\nin portions, and the function computes partial results.\\n7.1 Background\\nMatrix multiplication is a binary operation that combines two matrices into a third. The operation\\nitself can be described as a linear operation on the vectors that compose the two matrices. The\\nmost common form of matrix multiplication is call the matrix product. The matrix product AB\\ncreates an n×p matrix'),\n",
       " Document(page_content='be described as a linear operation on the vectors that compose the two matrices. The\\nmost common form of matrix multiplication is call the matrix product. The matrix product AB\\ncreates an n×p matrix when matrix A has dimensions n×m and matrix B has dimensions m×p.\\nMore precisely, we define the following:\\n\\uf8ee \\uf8f9 \\uf8ee \\uf8f9\\nA A ··· A B B ··· B\\n11 12 1m 11 12 1p\\n\\uf8efA 21 A 22 ··· A 2m\\uf8fa \\uf8efB 21 B 22 ··· B 2p\\uf8fa\\nA = \\uf8ef \\uf8ef\\n\\uf8f0\\n. .\\n.\\n. .\\n.\\n... . .\\n.\\n\\uf8fa \\uf8fa \\uf8fb, B = \\uf8ef \\uf8ef\\n\\uf8f0\\n. .\\n.\\n. .\\n.\\n... . .\\n.\\n\\uf8fa \\uf8fa\\n\\uf8fb\\n(7.1)\\nA A ··· A B B ··· B\\nn1 n2 nm m1 m2 mp\\n\\uf8ee \\uf8f9\\n(AB) (AB) ··· (AB)\\n11 12 1p\\n\\uf8ef(AB) (AB) ··· (AB) \\uf8fa\\nAB = \\uf8ef \\uf8ef\\n\\uf8f0\\n. .\\n.\\n21 . .\\n.\\n22 ... . .\\n.\\n2p\\uf8fa \\uf8fa\\n\\uf8fb\\n(7.2)\\n(AB) (AB) ··· (AB)\\nn1 n2 np\\nwhere the operation (AB) is defined as (AB) =\\n(cid:80)m\\nA B .\\nij ij k=1 ik kj\\nNow we provide a simple example. Let\\n\\uf8ee \\uf8f9\\n(cid:20) A A A (cid:21) B 11 B 12\\nA = A11 A12 A13 , B = \\uf8f0B 21 B 22\\uf8fb (7.3)\\n21 22 23 B B\\n31 32\\nThe result of the matrix product is\\n(cid:20) (cid:21)\\nA B +A B +A B A B +A B +A B\\nAB = 11 11 12 21 13 31 11 12 12 22 13 32 (7.4)\\nA B +A B'),\n",
       " Document(page_content='12\\nA = A11 A12 A13 , B = \\uf8f0B 21 B 22\\uf8fb (7.3)\\n21 22 23 B B\\n31 32\\nThe result of the matrix product is\\n(cid:20) (cid:21)\\nA B +A B +A B A B +A B +A B\\nAB = 11 11 12 21 13 31 11 12 12 22 13 32 (7.4)\\nA B +A B +A B A B +A B +A B\\n21 11 22 21 23 31 21 12 22 22 23 32\\n129MATRIX MULTIPLICATION\\nvoid matrixmul(int A[N][M], int B[M][P], int AB[N][P]) {\\n#pragma HLS ARRAY RESHAPE variable=A complete dim=2\\n#pragma HLS ARRAY RESHAPE variable=B complete dim=1\\n/∗ for each row and column of AB ∗/\\nrow: for(int i = 0; i < N; ++i) {\\ncol: for(int j = 0; j < P; ++j) {\\n#pragma HLS PIPELINE II=1\\n/∗ compute (AB)i,j ∗/\\nint ABij = 0;\\nproduct: for(int k = 0; k < M; ++k) {\\nABij += A[i][k] ∗ B[k][j];\\n}\\nAB[i][j] = ABij;\\n}\\n}\\n}\\nFigure 7.1: A common three for loop structure for matrix multiplication. The outer for loops,\\nlabeledrowsandcols, iterateacrosstherowsandcolumnsoftheoutputmatrixAB. Theinnermost\\nloop, labeled product multiplies the appropriate elements of one row of A and one column of B and\\naccumulates them until it'),\n",
       " Document(page_content='iterateacrosstherowsandcolumnsoftheoutputmatrixAB. Theinnermost\\nloop, labeled product multiplies the appropriate elements of one row of A and one column of B and\\naccumulates them until it has the result for the element in AB .\\nMatrix multiplication is a fundamental operation in numerical algorithms. Computing the\\nproduct between large matrices can take a significant amount of time. Therefore, it is critically\\nimportant part of many of problems in numerical computing. Fundamentally, matrices repre-\\nsent linear transforms between vector spaces; matrix multiplication provides way to compose the\\nlinear transforms. Applications include linearly changing coordinates (e.g., translation, rotation\\nin graphics), high dimensional problems in statistical physics (e.g., transfer-matrix method), and\\ngraph operations (e.g., determining if a path exists from one vertex to another). Thus it is a well\\nstudied problem, and there are many algorithms that aim to increase its performance, and reduce\\nthe'),\n",
       " Document(page_content='operations (e.g., determining if a path exists from one vertex to another). Thus it is a well\\nstudied problem, and there are many algorithms that aim to increase its performance, and reduce\\nthe memory usage.\\n7.2 Complete Matrix Multiplication\\nWe start our optimization process with perhaps the most common method to compute a matrix\\nmultiplication – using three nested for loops. Figure 7.1 provides the code for such an implementa-\\ntion. The outer for loops, labeled rows and cols, iterate across the rows and columns of the output\\nmatrix AB. The innermost for loop computes a dot product of one row of A and one column of\\nB. Each dot product is a completely independent set of computations that results in one element\\nof AB. Conceptually, we are performing P matrix-vector multiplications, one for each column of\\nB.\\nInthiscase,we’veappliedapipelinedirectivetothecolloopwithatargetinitiationintervalof1.\\nTheresultisthattheinnermostforloopisfullyunrolled,andweexpecttheresultingcircuitinclude\\nroughly'),\n",
       " Document(page_content='each column of\\nB.\\nInthiscase,we’veappliedapipelinedirectivetothecolloopwithatargetinitiationintervalof1.\\nTheresultisthattheinnermostforloopisfullyunrolled,andweexpecttheresultingcircuitinclude\\nroughly M multiply-add operators and to have an interval of roughly N ∗P cycles. As discussed\\nin Chapter 4, this is only one reasonable choice. We could choose to place the pipeline directive in\\n130MATRIX MULTIPLICATION\\ndifferentlocationsinthefunctionwiththegoalofachievingdifferentresource-throughputtradeoffs.\\nFor instance, placing the same directive at the top of the function (outside all of the for loops)\\nwill result in all of the loops being completely unrolled, which would take roughly N ∗ M ∗ P\\nmultiply-add operators and would have an interval of 1 cycle. Placing it inside the row loop would\\nresult in roughly M ∗P multiply-add operators and an interval of roughly N cycles. These design\\npointsarerelativelyeasytoachieve,giventhecorrespondingarraypartitioning.'),\n",
       " Document(page_content='it inside the row loop would\\nresult in roughly M ∗P multiply-add operators and an interval of roughly N cycles. These design\\npointsarerelativelyeasytoachieve,giventhecorrespondingarraypartitioning. It’salsopossibleto\\npipelinetheinnermostloopwiththegoalofachievingadesignwithonlyonemultiply-addoperator,\\nalthough achieving an II=1 implementation at high clock frequencies can be difficult because of\\nthe recurrence involved in the accumulation of variable ABij. We can also partially unroll different\\nloops to achieve yet more design points. The fundamental tradeoff here is between the complexity\\nof the resulting architecture, i.e., the number of multiply-add operators, and performance, i.e., the\\nnumber of cycles that the hardware is busy. In an ideal world, each doubling of the resource usage\\nshould result in exactly half the number of clock cycles being required, although in practice such\\n‘perfect scaling’ is difficult to achieve.\\nChange the location of the pipeline directive. How does the'),\n",
       " Document(page_content='result in exactly half the number of clock cycles being required, although in practice such\\n‘perfect scaling’ is difficult to achieve.\\nChange the location of the pipeline directive. How does the location effect the resource\\nusage? How does it change the performance? Which alternative provides the best performance\\nin terms of function interval? Which provides the smallest resource usage? Where do you\\nthink is the best place for the directive? Would increasing the size of the matrices change your\\ndecision?\\nExecuting large numbers of operations every cycle requires being able to supply all of the\\nrequired operands and to store the results of each operation. Previously we have used the\\narray partition directive to increase the number of accesses that can be performed on each memory.\\nAs long as the partition of the array that each memory access can be determined at compile time,\\nthen array partitioning is a simple and efficient way to increase the number of memory accesses\\nthat can be'),\n",
       " Document(page_content='as the partition of the array that each memory access can be determined at compile time,\\nthen array partitioning is a simple and efficient way to increase the number of memory accesses\\nthat can be performed each clock cycle. In this case, we use the slightly different array reshape\\ndirective to perform array partitioning. This directive not only partitions the address space of\\nthe memory into separate memory blocks, but then recombines the memory blocks into a single\\nmemory. This transformation increases the data width of the memory used to store the array, but\\ndoesn’t change the overall number of bits being stored. The difference is shown in Figure 7.2.\\nBoth array reshape and array partition increase the number of array elements that can be read\\neach clock cycle. They also support the same options, enabling cyclic and block partitions or parti-\\ntioning along different dimensions of a multi-dimensional array. In the case of array reshape, the\\nelements must each have the same address in'),\n",
       " Document(page_content='options, enabling cyclic and block partitions or parti-\\ntioning along different dimensions of a multi-dimensional array. In the case of array reshape, the\\nelements must each have the same address in the transformed array, whereas with array partition,\\nthe addresses in the transformed array can be unrelated. Although it may seem like one would\\nalways want to use array partition because it is more flexible, it makes each individual memory\\nsmaller, which can sometimes result in inefficient memory usage. The array reshape directive re-\\nsults in larger memory blocks, which can sometimes be mapped more efficiently into primitive\\nFPGA resources. In particular, the smallest granularity of block RAM (BRAM) blocks in Xilinx\\nVirtex Ultrascale+ devices is 18 Kbits with several different supported combination of depths and\\nwidths. When the partitions of an array become smaller than around 18Kbits, then BRAMs are\\nno longer used efficiently. If we start with an original array which is a 4-bit array'),\n",
       " Document(page_content='of depths and\\nwidths. When the partitions of an array become smaller than around 18Kbits, then BRAMs are\\nno longer used efficiently. If we start with an original array which is a 4-bit array with dimensions\\n[1024][4], this array can fit in a single BRAM resource configured as a 4Kbit x 4 memory. Parti-\\ntioning this array completely in the second dimension would result in 4 1Kbit x 4 memories, each\\nof which are much smaller than one one BRAM resource. Reshaping the array instead using the\\narray reshapedirective results inamemory whichis1Kbit x16, asupportedBRAMconfiguration.\\n131MATRIX MULTIPLICATION\\ni,k\\nA[i][k]\\nstnemele\\nM*N\\nlog (N) 32\\n2\\ni\\nlog (N ∗M) 32\\n2 A[i][0] N elements\\ni\\nA[i][1]\\ni\\nA[i][M-2]\\ni\\n32 bits\\nA[i][M-1]\\n32 bits\\nseiromem\\nM\\nlog (N) 32*M\\n2\\ni\\nA[i][]\\nN elements\\n32*M bits\\nFigure 7.2: Three different implementations of a two-dimensional array. On the left is the original\\narray consisting of N ∗ M elements. In the middle, the array has been transformed using the\\narray partition'),\n",
       " Document(page_content='7.2: Three different implementations of a two-dimensional array. On the left is the original\\narray consisting of N ∗ M elements. In the middle, the array has been transformed using the\\narray partition directive, resulting in M memories, each with N elements. On the right, the array\\nhas been transformed using the array reshape directive, resulting in one memory with N locations\\nand each location contains M elements of the original array.\\nNote that this partitioning is effective because the partitioned dimension (dimension 2 of\\nA or dimension 1 of B) is indexed by a constant. In addition, the non-partitioned dimension\\nis indexed by the same (variable) value. When partitioning multi-dimensional arrays, this is a\\ngood rule of thumb to identify which dimension should be partitioned.\\nRemove the array reshape directives. How does this effect the performance? How does it\\nchange the resource usage? Does it make sense to use any other array reshape directives (with\\ndifferent arguments) on these'),\n",
       " Document(page_content='array reshape directives. How does this effect the performance? How does it\\nchange the resource usage? Does it make sense to use any other array reshape directives (with\\ndifferent arguments) on these arrays? In this case, how does the result differ if you use the\\narray reshape directive instead?\\nThe size of the arrays can have a substantial effect on the optimizations that you wish to\\nperform. Some applications might use very small matrices, say 2×2 or 4×4. In this cases, it\\nmay be desirable to implement a design with the absolute highest performance, which is generally\\nachievedbyapplyingthepipelinedirectiveontheentirefunction. Asthesizeofthearraysincrease,\\nin the range of 32 × 32, this approach quickly become infeasible because of the resource limits\\navailable in a single device. There will simply not be enough DSP resources to implement that\\nmany multiplications every clock cycle or enough external bandwidth to get get data on and off the\\nchip. Many FPGA designs are often coupled to'),\n",
       " Document(page_content='simply not be enough DSP resources to implement that\\nmany multiplications every clock cycle or enough external bandwidth to get get data on and off the\\nchip. Many FPGA designs are often coupled to the data rates of other components in a system,\\nsuch as an Analog to Digital (A/D) converter, or the symbol rate in a communication system.\\nIn these designs it is common to instead apply the pipeline directive on inner loops with the\\ngoal of matching the interval of the computation with the data rate in a system. In such cases,\\nwe often need to explore different resource-throughput tradeoffs by moving the pipeline directive\\ninto an inner loop or partially unrolling loops. When dealing with very large matrices containing\\nthousands or millions of elements, we often need to take into account more complex architectural\\nconsiderations. The next section discusses a common approach to scaling matrix multiply to larger\\ndesigns, called blocking or tiling.\\n132MATRIX MULTIPLICATION\\nOptimize your design'),\n",
       " Document(page_content='architectural\\nconsiderations. The next section discusses a common approach to scaling matrix multiply to larger\\ndesigns, called blocking or tiling.\\n132MATRIX MULTIPLICATION\\nOptimize your design for the 128×128 matrix multiplication. Then start increasing the\\nsize of the matrices by a factor of two (to 512×512, 1024×1024, 2048×2048, etc. How does\\nthis effect the resource usage and performance? How about the runtime of the tool? What is\\nthe best way to optimize for large large matrix sizes?\\n7.3 Block Matrix Multiplication\\nAblock matrix isinterpretedasbeingpartitionedintodifferentsubmatrices. Thiscanbevisualized\\nby drawing different horizontal and vertical lines across the elements of the matrix. The resulting\\n“blocks” can be viewed as submatrices of the original matrix. Alternatively, we can view the\\noriginal matrix as a matrix of blocks. This naturally leads to many hierarchical algorithms in\\nLinear Algebra where we compute matrix operations, such as matrix multiply, on large'),\n",
       " Document(page_content='we can view the\\noriginal matrix as a matrix of blocks. This naturally leads to many hierarchical algorithms in\\nLinear Algebra where we compute matrix operations, such as matrix multiply, on large block\\nmatrices by decomposing them into smaller matrix operations on the blocks themselves.\\nFor instance, when we talk about the matrix multiplication operation between matrix A and\\nB in Equations 7.3 and 7.4, we might normally think of each element of the matrices A or B\\n11 23\\nas a single number or perhaps a complex number. Alternatively, we can consider each element in\\nthese matrix operations as a block of the original matrix. In this case, as long as the sizes of the\\nindividual blocks are compatible, we simply have to perform the correct matrix operations instead\\nof the original scalar operations. For instance, to compute AB , we would need to compute two\\n11\\nmatrix products and two matrix sums to compute A B +A B +A B .\\n11 11 12 21 13 31\\nMatrix blocking turns out to be a very useful'),\n",
       " Document(page_content='operations. For instance, to compute AB , we would need to compute two\\n11\\nmatrix products and two matrix sums to compute A B +A B +A B .\\n11 11 12 21 13 31\\nMatrix blocking turns out to be a very useful technique for a number of reasons. One reason\\nis that blocking is an easy way to find more structure in the algorithm that we can explore. In\\nfact, some of the optimizations that we have already seen as loop transformations, such as loop\\nunrolling, can be viewed as specific simple forms of blocking. Another reason is that we can choose\\nto block a matrix according to the natural structure of the matrix. If a matrix has a large block\\nof zeros, then many individual products may be zero. If we want to skip these individual products\\nthen this can be difficult in a statically schedule pipeline, whereas it may be easier to skip a large\\nblock of zeros. Many matrices are block-diagonal where the blocks on the diagonal are non-zero\\nand blocks off the diagonal are zero. Yet another reason is that'),\n",
       " Document(page_content='whereas it may be easier to skip a large\\nblock of zeros. Many matrices are block-diagonal where the blocks on the diagonal are non-zero\\nand blocks off the diagonal are zero. Yet another reason is that the blocked decomposition results\\nin lots of smaller problems operating on smaller sets of data. This increases the data locality of\\na computation. In processor systems, it is common to choose block sizes that conveniently match\\nthe memory hierarchy of a processor or the natural size of the vector data types supported by the\\nprocessor. Similarly, in FPGAs we can choose the blocking sizes to match the available on-chip\\nmemory size or to the number of multiply-add operators that we can budget to support.\\nUntil now, we have assumed that accelerators always have all of their data available before\\nthe start of a task. However in designs dealing with large datasets, such as large matrices, this\\ncan sometimes be an unneeded constraint. Since it is unlikely that our accelerator will be able'),\n",
       " Document(page_content='before\\nthe start of a task. However in designs dealing with large datasets, such as large matrices, this\\ncan sometimes be an unneeded constraint. Since it is unlikely that our accelerator will be able to\\nprocess all input data immediately, we can build an accelerator that receives input data only right\\nbefore it is needed. This allows an accelerator to more efficiently use the available on-chip memory.\\nWe call this a streaming architecture since we transfer the input data (and potentially the output\\ndata) one portion at a time rather than all at once.\\nStreaming architectures are common in many applications. In some cases this is because of\\na conscious design choice that we make to decompose a large computation into multiple smaller\\ncomputations. Forinstance,wemaydesignamatrixmultiplicationsystemthatreadsandprocesses\\none block of data at a time from external memory. In other cases, we might process a stream of\\ndata because the data is being sampled in real time from the physical world,'),\n",
       " Document(page_content='block of data at a time from external memory. In other cases, we might process a stream of\\ndata because the data is being sampled in real time from the physical world, for instance, from an\\n133MATRIX MULTIPLICATION\\nA/D converter. In other cases, the data we are processing may simply be created in sequence from\\na previous computation or accelerator. In fact, we’ve already seen an instance of this in Section\\n5.4.\\nOne potential advantage of streaming is a reduction in the memory that we can use to store\\nthe input and output data. The assumption here is that we can operate on the data in portions,\\ncreate partial results, and then we are done with that data, thus we do not need to store it. When\\nthe next data arrives, we can overwrite the old data resulting in smaller memories.\\nIn the following, we develop a streaming architecture for matrix multiplication. We divide the\\ninput arrays A and B into blocks, which are a contiguous set of rows and columns, respectively.\\nUsing these blocks, we'),\n",
       " Document(page_content='we develop a streaming architecture for matrix multiplication. We divide the\\ninput arrays A and B into blocks, which are a contiguous set of rows and columns, respectively.\\nUsing these blocks, we compute a portion of the product AB. Then we stream the next set of\\nblocks, compute another portion of AB until the entire matrix multiplication is complete.\\nFigure 7.3 provides the a description of the streaming architecture that we create. Our archi-\\ntecture has a variable BLOCK SIZE that indicates the number of rows that we take from the A\\nmatrix on each execution, the number of columns taken from the B matrix, and the BLOCK SIZE\\n× BLOCK SIZE result matrix corresponding to the data that we compute each time for the AB\\nmatrix.\\nThe example in Figure 7.3 uses a BLOCK SIZE = 2. Thus we take two rows from A, and two\\ncolumns from B on each execution of the streaming architecture that we define. The result of each\\ncall to the blockmatmul function is a 2×2 matrix for the AB architecture.\\nSince we'),\n",
       " Document(page_content='from A, and two\\ncolumns from B on each execution of the streaming architecture that we define. The result of each\\ncall to the blockmatmul function is a 2×2 matrix for the AB architecture.\\nSince we are dealing with 4×4 matrices in the example, we need to do this process four times.\\nEach time we get a 2×2 set of results for the AB matrix. The figure shows a progression of the\\nrows and columns that we send. In Figure 7.3 a) we send the first two rows of A and the first two\\ncolumns of B. The function will compute a 2×2 matrix corresponding to the first two elements in\\nthe rows and columns of the resulting matrix AB.\\nInFigure7.3b),weuseagainthefirsttworowsofA,butthistimewesendthelasttwocolumns\\nofB. WedonotneedtoresendthedatafromtherowsofAsincetheyarethesameastheprevious\\ndata from the previous execution. And we get the results for the 2×2 matrix corresponding to the\\ndata in “upper left” corner of AB.\\nFigure 7.3 c) sends different data for both the A and B matrices. This time we send the'),\n",
       " Document(page_content='execution. And we get the results for the 2×2 matrix corresponding to the\\ndata in “upper left” corner of AB.\\nFigure 7.3 c) sends different data for both the A and B matrices. This time we send the last\\ntwo rows of A and the first two columns of B. The results from this computation provide the\\n“lower left” corner of the AB matrix.\\nThe final execution of the streaming block matrix multiply, shown in Figure 7.3 d), uses the\\nsame last two rows of the A matrix from the previous iteration. And it sends the last two columns\\nof the B matrix. The result provides the elements in the “lower right” corner of the AB matrix.\\nBefore we show the code for the block matrix multiplication, we define some data types that\\nwe will use. Figure 7.4 shows the header file for the project. We create a custom data type DTYPE\\nthatspecifiesthetypeofdatathatwewillmultiplyintheAandBmatrices, andthecorresponding\\nAB matrix. This is currently set to an int data type.\\nIt is good coding practice to use a custom data type'),\n",
       " Document(page_content='DTYPE\\nthatspecifiesthetypeofdatathatwewillmultiplyintheAandBmatrices, andthecorresponding\\nAB matrix. This is currently set to an int data type.\\nIt is good coding practice to use a custom data type in your designs. This allows you to\\neasily change the data type, and to have one source of information so that you do not have\\nerrors when changing the data type in the future design iterations. And it is quite common to\\nchange the data type over the course of the design. For example, first you may start out with a\\nfloat or double type while you get a functionally correct design. This also provides a baseline\\nfor error since later you will likely change your design to use fixed point data type. Fixed point\\ndata can reduce the number of resources, and increase the performance potentially at the cost\\nof a reduction in the precision of the resulting data. You will likely try many different fixed\\n134MATRIX MULTIPLICATION\\na)\\nA A A A B B B B AB AB AB AB\\n11 12 13 14 11 12 13 14 11 1 2 13 1 4\\nA A A A'),\n",
       " Document(page_content='a reduction in the precision of the resulting data. You will likely try many different fixed\\n134MATRIX MULTIPLICATION\\na)\\nA A A A B B B B AB AB AB AB\\n11 12 13 14 11 12 13 14 11 1 2 13 1 4\\nA A A A B B B B AB AB AB AB\\n21 22 23 24 21 22 23 24 2 1 2 2 23 2 4\\nx =\\nA A A A B B B B AB AB AB AB\\n31 32 33 34 31 32 33 34 31 32 33 3 4\\nA A A A B B B B AB AB AB AB\\n41 42 43 44 41 42 43 44 41 42 43 4 4\\nb)\\nA A A A B B B B AB AB AB AB\\n11 12 13 14 11 12 13 14 1 1 1 2 13 1 4\\nA A A A B B B B AB AB AB AB\\n21 22 23 24 21 22 23 24 2 1 2 2 23 2 4\\nx =\\nA A A A B B B B AB AB AB AB\\n31 32 33 34 31 32 33 34 31 32 33 3 4\\nA 41 A 42 A 43 A 44 B 41 B 42 B 43 B 44 AB 41 AB 42 AB 43 AB 44\\nc)\\nA A A A B B B B AB AB AB AB\\n11 12 13 14 11 12 13 14 11 12 13 1 4\\nA A A A B B B B AB AB AB AB\\n21 22 23 24 21 22 23 24 21 22 23 24\\nx =\\nA A A A B B B B AB AB AB AB\\n31 32 33 34 31 32 33 34 31 32 33 34\\nA A A A B B B B AB AB AB AB\\n41 42 43 44 41 42 43 44 41 42 43 44\\nd)\\nA A A A B B B B AB AB AB AB\\n11 12 13 14 11 12 13 14 1 1 1 2 13 1 4\\nA A A A'),\n",
       " Document(page_content='A B B B B AB AB AB AB\\n31 32 33 34 31 32 33 34 31 32 33 34\\nA A A A B B B B AB AB AB AB\\n41 42 43 44 41 42 43 44 41 42 43 44\\nd)\\nA A A A B B B B AB AB AB AB\\n11 12 13 14 11 12 13 14 1 1 1 2 13 1 4\\nA A A A B B B B AB AB AB AB\\n21 22 23 24 21 22 23 24 21 22 23 2 4\\nx =\\nA A A A B B B B AB AB AB AB\\n31 32 33 34 31 32 33 34 31 32 33 34\\nA A A A B B B B AB AB AB AB\\n41 42 43 44 41 42 43 44 41 42 43 44\\nFigure 7.3: One possible blocked decomposition of the matrix multiplication of two 4×4 matrices.\\nThe entire AB product is decomposed into four matrix multiply operations operating on a 2×4\\nblock of A and a 4×2 block of B.\\n135MATRIX MULTIPLICATION\\n#ifndef BLOCK MM H\\n#define BLOCK MM H\\n#include ”hls stream.h”\\n#include <iostream>\\n#include <iomanip>\\n#include <vector>\\nusing namespace std;\\ntypedef int DTYPE;\\nconst int SIZE = 8;\\nconst int BLOCK SIZE = 4;\\ntypedef struct {\\nDTYPE a[BLOCK SIZE]; } blockvec;\\ntypedef struct {\\nDTYPE out[BLOCK SIZE][BLOCK SIZE]; } blockmat;\\nvoid blockmatmul(hls::stream<blockvec>'),\n",
       " Document(page_content='int SIZE = 8;\\nconst int BLOCK SIZE = 4;\\ntypedef struct {\\nDTYPE a[BLOCK SIZE]; } blockvec;\\ntypedef struct {\\nDTYPE out[BLOCK SIZE][BLOCK SIZE]; } blockmat;\\nvoid blockmatmul(hls::stream<blockvec> &Arows, hls::stream<blockvec> &Bcols,\\nblockmat & ABpartial, DTYPE iteration);\\n#endif\\nFigure 7.4: The header file for the block matrix multiplication architecture. The file defines the\\ndata types used within the function, the key constants, and the blockmatmul function interface.\\n136MATRIX MULTIPLICATION\\npointtypesuntilyoufindtherighttradeoff betweenaccuracy/error, performance, andresource\\nusage.\\nSIZE determines the number of rows and columns in the matrices to be multiplied. We limit\\nthis to square matrices although handling arbitrary matrix sizes could be done by changing the\\ncode in a number of different places. We leave that as an exercise for the reader.\\nChange the code to allow it to handle matrices of arbitrary size.\\nThe BLOCK SIZE variable defines the number of rows from A and the number'),\n",
       " Document(page_content='places. We leave that as an exercise for the reader.\\nChange the code to allow it to handle matrices of arbitrary size.\\nThe BLOCK SIZE variable defines the number of rows from A and the number of columns from\\nB that we operate upon in each execution. This also defines how much data that we stream at one\\ntime into the function. The output data that we receive from the function at each execution is an\\nBLOCK SIZE × BLOCK SIZE portion of the AB matrix.\\nThe blockvec data type is used to transfer the BLOCK SIZE rows of A and columns of B to the\\nfunction on each execution. The blockmat data type is where we store the partial results for the\\nAB matrix.\\nFinally, the blockmat data type is a structure consisting of an BLOCK SIZE × BLOCK SIZE\\narray. This holds the resulting values from one execution of the matrmatmul function.\\nThe function prototype itself takes the two inputs which are both of the type\\nhls::stream<blockvec> &. These are a sequence of blockvec data. Remember that a\\nblockvec is a'),\n",
       " Document(page_content='of the matrmatmul function.\\nThe function prototype itself takes the two inputs which are both of the type\\nhls::stream<blockvec> &. These are a sequence of blockvec data. Remember that a\\nblockvec is a data type that consists of an array with BLOCK SIZE elements.\\nThehls::stream<>templateclassisonewayinVivado(cid:13)R HLSofcreatingaFIFOdatastructure\\nthat works well in simulation and synthesis. The samples are sent in sequential order using the\\nwrite()function,andretrievedusingtheread()function. Thislibrarywasdevelopedsincestreaming\\nis a common methodology for passing data in hardware design, yet this same operation can be\\nmodeled in many different ways using the C programming language, for instance, by using arrays.\\nIn particular, it can be difficult for the Vivado(cid:13)R HLS tool to infer streaming behaviors when\\ndealing complex access patterns or multi-dimensional arrays. The built-in stream library enables\\nthe programmer to explicitly specify the order of stream accesses, avoiding'),\n",
       " Document(page_content='streaming behaviors when\\ndealing complex access patterns or multi-dimensional arrays. The built-in stream library enables\\nthe programmer to explicitly specify the order of stream accesses, avoiding any limitations of this\\ninference.\\nThe hls::stream class must always be passed by reference between functions, e.g., as we have\\ndone in the blockmatmul function in Figure 7.5.\\nThe code for executing one part of the streaming block matrix multiplication is shown in Figure\\n7.5. The code has three portions denoted by the labels loadA, partialsum, and writeoutput.\\nThe first part of the code, denoted by the loadA label, is only executed on certain conditions,\\nmore precisely when it % (SIZE/BLOCK SIZE) == 0. This is done to save some time in the cases\\nwhen we can reuse the data from the A matrix from the previous execution of the function.\\nRemember that in each execution of this blockmatmul function we send BLOCK SIZE rows from\\nthe A matrix and BLOCK SIZE columns from the B matrix. We send'),\n",
       " Document(page_content='from the previous execution of the function.\\nRemember that in each execution of this blockmatmul function we send BLOCK SIZE rows from\\nthe A matrix and BLOCK SIZE columns from the B matrix. We send multiple BLOCK SIZE of\\ncolumns for each BLOCK SIZE of rows from A. The variable it keeps track of the number of times\\nthat we have called the blockmatmul function. Thus, we do a check on each execution of the\\nfunction to determine if we need to load the rows from A. When we do not, this saves us some\\n137MATRIX MULTIPLICATION\\n#include ”block mm.h”\\nvoid blockmatmul(hls::stream<blockvec> &Arows, hls::stream<blockvec> &Bcols,\\nblockmat &ABpartial, int it) {\\n#pragma HLS DATAFLOW\\nint counter = it % (SIZE/BLOCK SIZE);\\nstatic DTYPE A[BLOCK SIZE][SIZE];\\nif(counter == 0){ //only load the A rows when necessary\\nloadA: for(int i = 0; i < SIZE; i++) {\\nblockvec tempA = Arows.read();\\nfor(int j = 0; j < BLOCK SIZE; j++) {\\n#pragma HLS PIPELINE II=1\\nA[j][i] = tempA.a[j];\\n}\\n}\\n}\\nDTYPE AB[BLOCK SIZE][BLOCK SIZE] ='),\n",
       " Document(page_content='for(int i = 0; i < SIZE; i++) {\\nblockvec tempA = Arows.read();\\nfor(int j = 0; j < BLOCK SIZE; j++) {\\n#pragma HLS PIPELINE II=1\\nA[j][i] = tempA.a[j];\\n}\\n}\\n}\\nDTYPE AB[BLOCK SIZE][BLOCK SIZE] = { 0 };\\npartialsum: for(int k=0; k < SIZE; k++) {\\nblockvec tempB = Bcols.read();\\nfor(int i = 0; i < BLOCK SIZE; i++) {\\nfor(int j = 0; j < BLOCK SIZE; j++) {\\nAB[i][j] = AB[i][j] + A[i][k] ∗ tempB.a[j];\\n}\\n}\\n}\\nwriteoutput: for(int i = 0; i < BLOCK SIZE; i++) {\\nfor(int j = 0; j < BLOCK SIZE; j++) {\\nABpartial.out[i][j] = AB[i][j];\\n}\\n}\\n}\\nFigure 7.5: The blockmatmul function takes a BLOCK SIZE set of rows from A matrix, a\\nBLOCK SIZE set of columns from the B matrix, and creates a BLOCK SIZE × BLOCK SIZE partial\\nresult for the AB matrix. The first part of the code (denoted by the label loadA) stores the rows\\nfrom A into a local memory, the second part in the nested partialsum for performs the computation\\nfor the partial results, and the final part (with the writeoutput label) takes these results and'),\n",
       " Document(page_content='rows\\nfrom A into a local memory, the second part in the nested partialsum for performs the computation\\nfor the partial results, and the final part (with the writeoutput label) takes these results and puts\\nthem the proper form to return from the function.\\n138MATRIX MULTIPLICATION\\ntime. When it is executed, it simply pulls data from the Arows stream and puts it into a static\\nlocal two-dimensional matrix A[BLOCK SIZE][SIZE].\\nFullyunderstandingthiscoderequiressomeexplanationaboutthestreamclass, andhowweare\\nusingit. ThestreamvariableArowshas elementsof thetypeblockvec. Ablockvecis amatrixof size\\nBLOCK SIZE. We use this in the following manner; each element in the Arows stream has an array\\nthat holds one element from each of the BLOCK SIZE rows of the A matrix. Thus, in each call the\\nthe blockmatmul function, the Arows stream will have SIZE elements in it, each of those holding\\none of each of the BLOCK SIZE rows. The statement tempA = Arows.read() takes one element from\\nthe Arows stream.'),\n",
       " Document(page_content='function, the Arows stream will have SIZE elements in it, each of those holding\\none of each of the BLOCK SIZE rows. The statement tempA = Arows.read() takes one element from\\nthe Arows stream. Then we load each of these elements into the appropriate index in the local A\\nmatrix.\\nThe stream class overloads the << operator to be equivalent to the read() function. Thus,\\nthe statements tempA = Arows.read() and tempA << Arows perform the same operation.\\nThe next part of the computation calculates the partial sums. This is the bulk of the compu-\\ntation in the blockmatmul function.\\nThe Bcols stream variable is utilized in a very similar manner to the Arows variable. However,\\ninstead of storing rows of A, it stores the data corresponding the columns of B that the current\\nexecution of the function is computing upon. Every call of the blockmatmul function will provide\\nnew data for the columns of the B matrix. Thus, we do not need to conditionally load this data\\nas we do with the A matrix. The'),\n",
       " Document(page_content='is computing upon. Every call of the blockmatmul function will provide\\nnew data for the columns of the B matrix. Thus, we do not need to conditionally load this data\\nas we do with the A matrix. The function itself works in a very similar manner to that from the\\nmatmul in Figure 7.1 except that we are only calculating BLOCK SIZE × BLOCK SIZE results from\\nthe AB matrix. Thus we only have to iterate across BLOCK SIZE rows of A and BLOCK SIZE\\ncolumns of B. But each row and column has SIZE elements, hence the bounds on the outer for\\nloop.\\nThe final portion of the function moves the data from the local AB array, which has dimensions\\nof BLOCK SIZE × BLOCK SIZE; this holds the partial results of the AB output matrix.\\nOf the three parts of the function, the middle part, which calculates the partial sum, requires\\nthe most computation. By inspecting the code, we can see that this part has three nested for\\nloops with a total of SIZE × BLOCK SIZE × BLOCK SIZE iterations. The first part has SIZE'),\n",
       " Document(page_content='sum, requires\\nthe most computation. By inspecting the code, we can see that this part has three nested for\\nloops with a total of SIZE × BLOCK SIZE × BLOCK SIZE iterations. The first part has SIZE ×\\nBLOCK SIZE iterations; and the last part has BLOCK SIZE × BLOCK SIZE iterations. Thus, we\\nshould focus our optimizations on the middle part, i.e., the partialsum nested for loops.\\nThe common starting point for optimizations of nested for loops is to pipeline the innermost\\nfor loop. Then, if that does not require too many resources, the designer can move the pipeline\\ndirective into higher level for loops. Whether the resulting design consume too many resource\\ndepends upon the specified BLOCK SIZE; if this is small, then it is likely worth moving the pipeline\\ndirective. It may even be worthwhile to move it inside the outermost for loop. This will unroll\\nthe two inner for loops and thus very likely increase the resource usage by a substantial amount.\\nHowever, it will increase the'),\n",
       " Document(page_content='be worthwhile to move it inside the outermost for loop. This will unroll\\nthe two inner for loops and thus very likely increase the resource usage by a substantial amount.\\nHowever, it will increase the performance.\\nHowdoeschangingtheBLOCK SIZEeffecttheperformanceandresourceusage? Howabout\\nchanging the SIZE constant? How does moving the pipeline directive across the three different\\nnested for loops in the partialsum portion of the function change the performance and resource\\nusage?\\nThe dataflow directive at the start of the function creates a pipeline across the portions of\\nthe function, i.e., the loadA for loop, the partialsum nested for loop, and the writeoutput for loop.\\n139MATRIX MULTIPLICATION\\nUsing this directive will decrease the interval of the blockmatmul function. However, this is limited\\nby the largest interval of all three of the portions of the code. That is, the maximum interval\\nfor the blockmatmul function – let us call it interval(blockmatmul) – is greater than or equal'),\n",
       " Document(page_content='limited\\nby the largest interval of all three of the portions of the code. That is, the maximum interval\\nfor the blockmatmul function – let us call it interval(blockmatmul) – is greater than or equal to\\nthe the interval of the three parts which are defined as interval(loadA), interval(partialsum), and\\ninterval(writeoutput). More formally,\\ninterval(blockmatmul) ≥ max(interval(loadA),interval(partialsum),\\ninterval(writeoutput)) (7.5)\\nWe need to keep Equation 7.5 in mind as we optimize the blockmatmul function. For example,\\nassume that interval(partialsum) is much larger than the other two portions of the function. Any\\nperformance optimizations that minimize interval(loadA) and interval(writeoutput) are useless since\\nthe function interval, i.e., interval(blockmatmul) would not decrease. Thus, the designer should\\nfocus any performance optimization effort to decrease interval(partialsum), i.e., target performance\\noptimizations on those three nested for loops.\\nIt is important to note that'),\n",
       " Document(page_content='the designer should\\nfocus any performance optimization effort to decrease interval(partialsum), i.e., target performance\\noptimizations on those three nested for loops.\\nIt is important to note that this only applies to performance optimizations. The designer can\\n(and should) optimize the resource usage of these other two parts. In fact, they are ripe for such\\noptimizations since reducing the resource usage often increases the interval and/or latency. In this\\ncase, it is ok to increase the interval as it will not effect the overall performance of the blockmatmul\\nfunction. In fact, the ideal case is to optimize all three parts of the function such that they all have\\nthe same interval, assuming that we can easily tradeoff between the interval and resource usage\\n(which is not always the case).\\nThe testbench for the blockmatmul function is shown in Figures 7.6 and 7.7. We split it across\\ntwo figures to make it more readable since it is a longer piece of code. Up until this point, we\\nhave not'),\n",
       " Document(page_content='testbench for the blockmatmul function is shown in Figures 7.6 and 7.7. We split it across\\ntwo figures to make it more readable since it is a longer piece of code. Up until this point, we\\nhave not shown the testbenches. We show this testbench for several reasons. First, it provides\\ninsight into how the blockmatmul function works. In particular, it partitions the input matrices\\ninto blocks and feeds them into the blockmatmul function in a block by block manner. Second, it\\ngives a complex usage scenario for using stream template for simulation. Finally, it gives the reader\\nan idea about how to properly design testbenches.\\nThe matmatmul sw function is a simple three for loop implementation of matrix multiplication.\\nIt takes two two-dimensional matrices as inputs, and outputs a single two-dimensional matrix. It is\\nvery similar to what we have seen in the matrixmul function in Figure 7.1. We use this to compare\\nour results from the blocked matrix multiplication hardware version.\\nLet us'),\n",
       " Document(page_content='matrix. It is\\nvery similar to what we have seen in the matrixmul function in Figure 7.1. We use this to compare\\nour results from the blocked matrix multiplication hardware version.\\nLet us focus on the first half of the testbench shown in Figure 7.6. The beginning block of\\ncode initializes variables of the rest of the function. The variable fail keeps track of whether the\\nmatrix multiplication was done correctly. We will check this later in the function. The variables\\nstrm matrix1 and strm matrix2 are hls:stream<> variables that hold the rows and columns of the\\nA and B matrices, respectively. Each element of these stream variables is a <blockvec>. Referring\\nback at the block mm.h file in Figure 7.4, we recall that a blockvec is defined as an array of data;\\nwe will use each blockvec to store one row or column of data.\\nThe stream variable resides in the hls namespace. Thus, we can use that namespace and\\nforgo the hls::stream and instead simply use stream. However, the preferred usage is'),\n",
       " Document(page_content='one row or column of data.\\nThe stream variable resides in the hls namespace. Thus, we can use that namespace and\\nforgo the hls::stream and instead simply use stream. However, the preferred usage is to keep the\\nhls:: in front of the stream to insure code readers that the stream is relevant to Vivado(cid:13)R HLS\\nand not C construct from another library. Also, it avoids having to deal with any potential\\nconflicts that may occur by introducing a new namespace.\\n140MATRIX MULTIPLICATION\\nThe next definitions in this beginning block of code are the variables strm matrix1 element\\nand strm matrix2 element. These two variables are used as placeholders to populate each blockvec\\nvariable that we write into the strm matrix1 and strm matrix2 stream variables. The block out\\nvariable is used to store the output results from the blockmatmul function. Note that this variable\\nuses the data type blockmat which is a two-dimensional array of BLOCK SIZE × BLOCK SIZE as\\ndefined in the block mm.h header file'),\n",
       " Document(page_content='output results from the blockmatmul function. Note that this variable\\nuses the data type blockmat which is a two-dimensional array of BLOCK SIZE × BLOCK SIZE as\\ndefined in the block mm.h header file (see Figure 7.4). The final definitions are A, B, matrix swout,\\nandmatrix hwout. These are all SIZE × SIZE two-dimensional arrays with the DTYPE data type.\\nYou can name the streams using an initializer. This is good practice as it gives bet-\\nter error messages. Without the name, the error message provides a generic reference to\\nthe stream with the data type. If you have multiple stream declarations with the same\\ndata type, then you will have to figure out which stream the error is referring to. Nam-\\ning the stream variable is done by giving the variable an argument which is the name, e.g.,\\nhls::stream<blockvec> strm matrix1(”strm matrix1”);.\\nThenextsetofnestedinitmatrices forloopssetsthevaluesofthefourtwo-dimensionalarraysA,\\nB, matrix swout, and matrix hwout. The variables A and B are input'),\n",
       " Document(page_content='strm matrix1(”strm matrix1”);.\\nThenextsetofnestedinitmatrices forloopssetsthevaluesofthefourtwo-dimensionalarraysA,\\nB, matrix swout, and matrix hwout. The variables A and B are input matrices. These are initialized\\nto a random value between [0, 512). We picked the number 512 for no particular reason other\\nthan it can fit any 9 bit value. Keep in mind that while the DTYPE is set as an int, and thus has\\nsignificantly more range than [0,512), we often move to fixed point values with much smaller ranges\\nlater in the design optimization process. The matrix swout and matrix hwout are both initialized to\\n0. These are filled in later by calls to the functions matmatmul sw and blockmatmul.\\nThe second part of the testbench is continued in Figure 7.7. This has the last portion of the\\ncode from the main function.\\nThe first part of this figure has a complex set of nested for loops. The overall goal of the\\ncomputation in these for loops is to set up the data from the input matrices A and B so that'),\n",
       " Document(page_content='main function.\\nThe first part of this figure has a complex set of nested for loops. The overall goal of the\\ncomputation in these for loops is to set up the data from the input matrices A and B so that it\\ncan be streamed to the blockmatmul function. Then the results of the blockmatmul function are\\nstored in the matrix hwout array.\\nThe outer two for loops are used to step across the input arrays in a blocked manner. You can\\nsee that these both iterate by a step of BLOCK SIZE. The next two for loops write rows from A\\ninto strm matrix1 element and the columns from B into strm matrix2 element. It does this in an\\nelement by element fashion by using the variable k to access the individual values from the rows\\n(columns) and write these into the one dimensional array for each of these “elements”. Remember\\nthat both strm matrix1 element and strm matrix2 element have the datatype blockvec, which is a\\none dimensional array of size BLOCK SIZE. It is meant to hold BLOCK SIZE elements from each\\nrow'),\n",
       " Document(page_content='Remember\\nthat both strm matrix1 element and strm matrix2 element have the datatype blockvec, which is a\\none dimensional array of size BLOCK SIZE. It is meant to hold BLOCK SIZE elements from each\\nrow or column. The inner for loop iterates BLOCK SIZE times. The strm matrix1 and strm matrix2\\nstream variables are written to SIZE times. That means that has a buffer of the entire row (or\\ncolumn) and each element in the buffer holds BLOCK SIZE values.\\nThe stream class overloads the >> operator to be equivalent to the write(data) function.\\nThis is similar to overloading the read() function to the << operator. Thus, the state-\\nmentsstrm matrix1.write(strm matrix1 element)andstrm matrix1 element >> strm matrix1per-\\nform the same operation.\\nThe final part of this portion of the code to highlight is the if statements. These are correspond\\nto the values A matrix. Essentially, these are there so that we do not constantly write the same\\n141MATRIX MULTIPLICATION\\n#include ”block mm.h”\\n#include'),\n",
       " Document(page_content='is the if statements. These are correspond\\nto the values A matrix. Essentially, these are there so that we do not constantly write the same\\n141MATRIX MULTIPLICATION\\n#include ”block mm.h”\\n#include <stdlib.h>\\nusing namespace std;\\nvoid matmatmul sw(DTYPE A[SIZE][SIZE], DTYPE B[SIZE][SIZE],\\nDTYPE out[SIZE][SIZE]){\\nDTYPE sum = 0;\\nfor(int i = 0; i < SIZE; i++){\\nfor(int j = 0; j<SIZE; j++){\\nsum = 0;\\nfor(int k = 0; k < SIZE; k++){\\nsum = sum + A[i][k] ∗ B[k][j];\\n}\\nout[i][j] = sum;\\n}\\n}\\n}\\nint main() {\\nint fail = 0;\\nhls::stream<blockvec> strm matrix1(”strm matrix1”);\\nhls::stream<blockvec> strm matrix2(”strm matrix2”);\\nblockvec strm matrix1 element, strm matrix2 element;\\nblockmat block out;\\nDTYPE A[SIZE][SIZE], B[SIZE][SIZE];\\nDTYPE matrix swout[SIZE][SIZE], matrix hwout[SIZE][SIZE];\\ninitmatrices: for(int i = 0; i < SIZE; i++){\\nfor(int j = 0; j < SIZE; j++){\\nA[i][j] = rand() % 512;\\nB[i][j] = rand() % 512;\\nmatrix swout[i][j] = 0;\\nmatrix hwout[i][j] = 0;\\n}\\n}\\n//the remainder of this testbench is'),\n",
       " Document(page_content='for(int i = 0; i < SIZE; i++){\\nfor(int j = 0; j < SIZE; j++){\\nA[i][j] = rand() % 512;\\nB[i][j] = rand() % 512;\\nmatrix swout[i][j] = 0;\\nmatrix hwout[i][j] = 0;\\n}\\n}\\n//the remainder of this testbench is displayed in the next figure\\nFigure 7.6: The first part of the testbench for block matrix multiplication. The function is split\\nacross two figures since it is too long to display on one page. The rest of the testbench is in\\nFigure 7.7. This has a “software” version of matrix multiplication, and variable declarations and\\ninitializations.\\n142MATRIX MULTIPLICATION\\n// The beginning of the testbench is shown in the previous figure\\nint main() {\\nint row, col, it = 0;\\nfor(int it1 = 0; it1 < SIZE; it1 = it1 + BLOCK SIZE) {\\nfor(int it2 = 0; it2 < SIZE; it2 = it2 + BLOCK SIZE) {\\nrow = it1; //row + BLOCK SIZE ∗ factor row;\\ncol = it2; //col + BLOCK SIZE ∗ factor col;\\nfor(int k = 0; k < SIZE; k++) {\\nfor(int i = 0; i < BLOCK SIZE; i++) {\\nif(it % (SIZE/BLOCK SIZE) == 0) strm matrix1 element.a[i] ='),\n",
       " Document(page_content='+ BLOCK SIZE ∗ factor row;\\ncol = it2; //col + BLOCK SIZE ∗ factor col;\\nfor(int k = 0; k < SIZE; k++) {\\nfor(int i = 0; i < BLOCK SIZE; i++) {\\nif(it % (SIZE/BLOCK SIZE) == 0) strm matrix1 element.a[i] = A[row+i][k];\\nstrm matrix2 element.a[i] = B[k][col+i];\\n}\\nif(it % (SIZE/BLOCK SIZE) == 0) strm matrix1.write(strm matrix1 element);\\nstrm matrix2.write(strm matrix2 element);\\n}\\nblockmatmul(strm matrix1, strm matrix2, block out, it);\\nfor(int i = 0; i < BLOCK SIZE; i++)\\nfor(int j = 0; j < BLOCK SIZE; j++)\\nmatrix hwout[row+i][col+j] = block out.out[i][j];\\nit = it + 1;\\n}\\n}\\nmatmatmul sw(A, B, matrix swout);\\nfor(int i = 0; i<SIZE; i++)\\nfor(int j = 0; j<SIZE; j++)\\nif(matrix swout[i][j] != matrix hwout[i][j]) { fail=1; }\\nif(fail==1) cout << ”failed” << endl;\\nelse cout << ”passed” << endl;\\nreturn 0;\\n}\\nFigure 7.7: The second portion of the block matrix multiply testbench. The first part is shown in\\nFigure 7.6. This shows the computation required to stream the data to the blockmatmul function,\\nand the'),\n",
       " Document(page_content='7.7: The second portion of the block matrix multiply testbench. The first part is shown in\\nFigure 7.6. This shows the computation required to stream the data to the blockmatmul function,\\nand the code that tests that this function matches a simpler three for loop implementation.\\n143MATRIX MULTIPLICATION\\nvalues to strm matrix1. Recall that the values from the A matrix are used across several calls to\\nthe blockmatmul function. See Figure 7.3 for a discussion on this. These if statements are placed\\nthere to highlight the fact that you should not continually write the same data over and over. This\\nis important because the internal code of the blockmatmul only does a read of this data when it is\\nnecessary. So if we continued to write this consistently, the code would not function correctly do\\nto the fact that this stream is written to more than it is read from.\\nNow that the input data, the testbench calls the blockmatmul function. After the function call,\\nit receives the partial computed'),\n",
       " Document(page_content='the fact that this stream is written to more than it is read from.\\nNow that the input data, the testbench calls the blockmatmul function. After the function call,\\nit receives the partial computed results in the block out variable. The next two for loops but these\\nresults into the appropriate locations in the matrix hwout array.\\nAfter this complex set of for loops, the block matrix multiplication is complete. And the\\ntestbench continues to insure that the code is written correctly. It does this by comparing the\\nresults from the multiple calls to the blockmatmul function to results that were computed in the\\nmatmatmul sw, whichisamuchsimplerversionofmatrixmatrixmultiplication. Afterthisfunction\\ncall, the testbench iterates through both two-dimensional matrices matrix hwout and matrix swout\\nand makes sure that all of the elements are equivalent. If there is one or more element that is not\\nequal, it sets the fail flag equal to 1. The testbench completes by printing out failed or passed.\\nIt'),\n",
       " Document(page_content='makes sure that all of the elements are equivalent. If there is one or more element that is not\\nequal, it sets the fail flag equal to 1. The testbench completes by printing out failed or passed.\\nIt is important that note that you cannot directly compare the performance of the function\\nblockmatmul with that of code for matrix multiplication, such as the code in Figure 7.1. This\\nis because it takes multiple calls to the blockmatmul function in order to perform the entire\\nmatrix multiplication. It is important to always compare apples to apples.\\nDerive a function to determine the number of times that blockmatmul must be called in\\norder to complete the entire matrix multiplication. This function should be generic, e.g.,\\nit should not be assume a specific value of BLOCK SIZE or size of the matrix (i.e., SIZE).\\nComparetheresourceusageofblockmatrixmultiplicationversusmatrixmultiplication.\\nHow do the resources change as the size of the matrices increases? Does the block size play\\na role in the'),\n",
       " Document(page_content='(i.e., SIZE).\\nComparetheresourceusageofblockmatrixmultiplicationversusmatrixmultiplication.\\nHow do the resources change as the size of the matrices increases? Does the block size play\\na role in the resource usage? What are the general trends, if any?\\nCompare the performance of block matrix multiplication versus matrix multiplication.\\nHow does the performance change as the size of the matrices increases? Does the block\\nsize play a role in the performance? Pick two architectures with similar resource usage.\\nHow does the performance for those architectures compare?\\n7.4 Conclusion\\nBlockmatrixmultiplicationprovidesadifferentwaytocomputematrixmultiplication. Itcomputes\\npartial results of the result matrix by streaming a subset of the input matrices to a function. This\\nfunction is then computed multiple times in order to complete the entire matrix multiplication\\ncomputation.\\n144Chapter 8\\nPrefix Sum and Histogram\\n8.1 Prefix Sum\\nPrefix sum is a common kernel used in many applications, e.g.,'),\n",
       " Document(page_content='multiple times in order to complete the entire matrix multiplication\\ncomputation.\\n144Chapter 8\\nPrefix Sum and Histogram\\n8.1 Prefix Sum\\nPrefix sum is a common kernel used in many applications, e.g., recurrence relations, compaction\\nproblems, string comparison, polynomial evaluation, histogram, radix sort, and quick sort [11].\\nPrefix sum requires restructuring in order to create an efficient FPGA design.\\nThe prefix sum is the cumulative sum of a sequence of numbers. Given a sequence of inputs\\nin , the prefix sum out is the summation of the first n inputs, namely out = in +in +in +\\nn n n 0 1 2\\n···+in +in . The following shows the computation for the first four elements of the output\\nn−1 n\\nsequence out.\\nout = in\\n0 0\\nout = in +in\\n1 0 1\\nout = in +in +in\\n2 0 1 2\\nout = in +in +in +in\\n3 0 1 2 3\\n···\\nOf course, in practice we don’t want to store and recompute the sum of all of the previous\\ninputs, so the prefix sum is often computed by the recurrence equation:\\nout = out +in (8.1)\\nn n−1 n\\nThe'),\n",
       " Document(page_content='3\\n···\\nOf course, in practice we don’t want to store and recompute the sum of all of the previous\\ninputs, so the prefix sum is often computed by the recurrence equation:\\nout = out +in (8.1)\\nn n−1 n\\nThe disadvantage of the recurrence equation is that we must compute out before computing\\nn−1\\nout , which fundamentally limits the parallelism and throughput that this computation can be\\nn\\nperformed. In contrast, the original equations have obvious parallelism where each output can be\\ncomputed independently at the expense of a significant amount of redundant computation. C code\\nimplementing the recurrence equation is shown in Figure 8.1. Ideally, we’d like to achieve II = 1\\nfor the loop in the code, but this can be challenging even for such simple code. Implementing this\\ncode with Vivado(cid:13)R HLS results in behavior like that shown in Figure 8.1.\\nThe way this code is written, each output is written into the output memory out[] and then in\\nthe next iteration is read back out of the memory'),\n",
       " Document(page_content='HLS results in behavior like that shown in Figure 8.1.\\nThe way this code is written, each output is written into the output memory out[] and then in\\nthe next iteration is read back out of the memory again. Since the memory read is has a latency of\\none, data read from memory cannot be processed until the following clock cycle. As a result, such\\na design can only achieve a loop II of 2. In this case there is a relatively easy way to rewrite the\\ncode: we can simply perform the accumulation on a separate local variable, rather than reading\\nthe previous value back from the array. Avoiding extra memory accesses in favor of register storage\\n145PREFIX SUM AND HISTOGRAM\\n#define SIZE 128\\nvoid prefixsum(int in[SIZE], int out[SIZE]) {\\nLoop Interval = 2\\nout[0]=in[0];\\nfor(int i = 1; i < SIZE; i++) {\\nIn\\n#pragma HLS PIPELINE\\nOut\\nout[i] = out[i−1] + in[i];\\nOut\\n}\\n}\\nFigure 8.1: Code for implementing prefix sum, and its accompanying behavior.\\n#define SIZE 128\\nvoid prefixsum(int in[SIZE], int out[SIZE])'),\n",
       " Document(page_content='HLS PIPELINE\\nOut\\nout[i] = out[i−1] + in[i];\\nOut\\n}\\n}\\nFigure 8.1: Code for implementing prefix sum, and its accompanying behavior.\\n#define SIZE 128\\nvoid prefixsum(int in[SIZE], int out[SIZE]) {\\nint A = in[0];\\nLoop Interval = 1\\nfor(int i=0; i < SIZE; i++) {\\n#pragma HLS PIPELINE\\nIn\\nA = A + in[i];\\nOut\\nout[i] = A;\\n}\\n}\\nFigure 8.2: Code for implementing an optimized prefix sum, and its accompanying behavior.\\nis often advantageous in processor code, but in HLS designs it is often more significant since other\\noperations are rarely a performance bottleneck. Code that does this is shown in Figure 8.2.\\nYou might ask why the compiler is not able to optimize the memory loads and stores auto-\\nmatically in order to improve the II of the design. It turns out that Vivado(cid:13)R HLS is capable of\\noptimizing loads and stores to array, but only for reads and writes within the scope of a single\\nbasic block. You can see this if we unroll the loop, as shown in Figure 8.3. Note that we also have\\nto add'),\n",
       " Document(page_content='loads and stores to array, but only for reads and writes within the scope of a single\\nbasic block. You can see this if we unroll the loop, as shown in Figure 8.3. Note that we also have\\nto add appropriate array partition s in order to be able to read and write multiple values at the\\ninterfaces. In this case, Vivado(cid:13)R HLS is able to eliminate most of the read operations of the out[]\\narray within the body of the loop, but we still only achieve a loop II of 2. In this case the first load\\nin the body of the loop is still not able to be removed. We can, however, rewrite the code manually\\nto use a local variable rather than read from the out[] array.\\nIdeally, when we unroll the inner loop, we the perform more operations per clock and reduce\\nthe interval to compute the function. If we unroll by a factor of two, then the performance doubles.\\nA factor of four would increase the performance by factor four, i.e., the performance scales in a\\nlinear manner as it is unrolled. While this is'),\n",
       " Document(page_content='by a factor of two, then the performance doubles.\\nA factor of four would increase the performance by factor four, i.e., the performance scales in a\\nlinear manner as it is unrolled. While this is mostly the case, as we unroll the inner loop there\\nare often some aspects of the design that don’t change. Under most circumstances, such as when\\nthe iteration space of loops execute for a long time, these aspects represent a small fixed overhead\\nwhich doesn’t contribute significantly to the performance of the overall function. However, as the\\nnumber of loop iterations decreases, these fixed portions of the design have more impact. The\\nlargest fixed component of a pipelined loop is the depth of the pipeline itself. The control logic\\ngenerate by Vivado(cid:13)R HLS for a pipelined loop requires the pipeline to completely flush before code\\nafter the loop can execute.\\n146PREFIX SUM AND HISTOGRAM\\n#define SIZE 128\\nLoop Interval = 2\\nvoid prefixsum(int in[SIZE], int out[SIZE]) {\\n#pragma HLS ARRAY'),\n",
       " Document(page_content='the pipeline to completely flush before code\\nafter the loop can execute.\\n146PREFIX SUM AND HISTOGRAM\\n#define SIZE 128\\nLoop Interval = 2\\nvoid prefixsum(int in[SIZE], int out[SIZE]) {\\n#pragma HLS ARRAY PARTITION variable=out cyclic factor=4 dim=1\\nIn[i*4+0]\\n#pragma HLS ARRAY PARTITION variable=in cyclic factor=4 dim=1 In[i*4+1]\\nIn[i*4+2]\\nint A = in[0];\\nIn[i*4+3]\\nfor(int i=0; i < SIZE; i++) { Out[i*4+3]\\n#pragma HLS UNROLL factor=4\\n#pragma HLS PIPELINE\\nout[i] = out[i−1] + in[i]; Out[i*4+0]\\nOut[i*4+1]\\n} Out[i*4+2]\\nOut[i*4+3]\\n}\\nFigure 8.3: Optimizing the prefixsum code using unroll, pipeline, and array partition directives.\\na) b)\\nin[] in[]\\n+ +\\nout[] A\\nout[]\\nFigure 8.4: Part a) displays an architecture corresponding to the code in Figure 8.1. The depen-\\ndence on the out[] array can prevent achieving a loop II of 1. Computing the recurrence with a\\nlocal variable, as shown in the code in Figure 8.2, is able to reduce the latency in the recurrence\\nand achieve a loop II of'),\n",
       " Document(page_content='array can prevent achieving a loop II of 1. Computing the recurrence with a\\nlocal variable, as shown in the code in Figure 8.2, is able to reduce the latency in the recurrence\\nand achieve a loop II of 1.\\nUnrolltheforloopcorrespondingtotheprefixsumcodeinFigure8.2bydifferentfactorsin\\ncombination with array partitioning to achieve a loop II of 1. How does the prefixsum function\\nlatency change? What are the trends in the resource usages? Why do you think you are seeing\\nthese trends? What happens when the loop becomes fully unrolled?\\nFigure 8.4 shows the hardware architecture resulting from synthesizing the code from Figure\\n8.1 and Figure 8.2. In part a), we can see that the ‘loop’ in the circuit includes the output memory\\nthat stores the out[] array, whereas in part b), the loop in the circuit includes only a register that\\nstores the accumulated value and the output memory is only written. Simplifying recurrences and\\neliminating unnecessary memory accesses is a common practice in'),\n",
       " Document(page_content='circuit includes only a register that\\nstores the accumulated value and the output memory is only written. Simplifying recurrences and\\neliminating unnecessary memory accesses is a common practice in optimizing HLS code.\\nThe goal of this section is to show that even a small changes in the code can sometimes have a\\nsignificant effect on the hardware design. Some changes may not necessarily be intuitive, but can\\nbe identified with feedback from the tool.\\n147PREFIX SUM AND HISTOGRAM\\nData Histogr am\\n4 2 4 4 3 3 3 4\\nHistogram Values\\nBins 2 3 4\\nCounts 1 4 3\\n0 1 2 3 4\\nFigure 8.5: An example of a histogram.\\nvoid histogram(int in[INPUT SIZE], int hist[VALUE SIZE]) {\\nint val;\\nfor(int i = 0; i < INPUT SIZE; i++) {\\n#pragma HLS PIPELINE\\nval = in[i];\\nhist[val] = hist[val] + 1;\\n}\\n}\\nFigure8.6: Originalcodeforcalculatingthehistogram. Theforloopiteratesacrosstheinputarray\\nand increments the corresponding element of the hist array.\\n8.2 Histogram\\nA Histogram models the probability distribution of a discrete'),\n",
       " Document(page_content='Theforloopiteratesacrosstheinputarray\\nand increments the corresponding element of the hist array.\\n8.2 Histogram\\nA Histogram models the probability distribution of a discrete signal. Given a sequence of discrete\\ninput values, the histogram counts the number of times each value appears in the sequence. When\\nnormalized by the total number of input values, the histogram becomesthe probability distribution\\nfunction of the sequence. Creating a histogram is a common function used in image processing,\\nsignal processing, database processing, and many other domains. In many cases, it is common\\nto quantize high-precision input data into a smaller number of intervals or bins as part of the\\nhistogram computation. For the purpose of this section, we will skip the actual process by which\\nthis is done and focus on what happens with the binned data.\\nFigure 8.5 provides a simple example of a histogram. The data set consists of a sequence of\\nbinnedvalues, inthiscaserepresentedbyanintegerin[0,4].'),\n",
       " Document(page_content='done and focus on what happens with the binned data.\\nFigure 8.5 provides a simple example of a histogram. The data set consists of a sequence of\\nbinnedvalues, inthiscaserepresentedbyanintegerin[0,4]. Thecorrespondinghistogram, consist-\\ning of a count for each bin, is shown below along with a graphical representation of the histogram,\\nwhere the height of each bar corresponding to the count of each separate value. Figure 8.6 shows\\nbaseline code for the histogram function.\\nThecodeendsuplookingverysimilartotheprefixsumintheprevioussection. Thedifferenceis\\nthat the prefix sum is essentially only performing one accumulation, while in the histogram function\\nwe compute one accumulation for each bin. The other difference is that in the prefix sum we added\\nthe input value each time, in this case we only add 1. When pipelining the inner loops using the\\npipeline directive, we return to the same problem as with the code in Figure 8.1, where we can\\nonly achieve a loop II of 2 due to the recurrence'),\n",
       " Document(page_content='only add 1. When pipelining the inner loops using the\\npipeline directive, we return to the same problem as with the code in Figure 8.1, where we can\\nonly achieve a loop II of 2 due to the recurrence through the memory. This is due to the fact that\\n148PREFIX SUM AND HISTOGRAM\\nin[] hist[]\\nval\\n+\\n1\\nFigure 8.7: An architecture resulting from the code in Figure 8.6. The val data from the in array is\\nused to index into the hist array. This data is incremented and stored back into the same location.\\nwe are reading from the hist array and writing to the same array in every iteration of the loop.\\nFigure 8.7 shows the hardware architecture for the code in Figure 8.6. You can see that the hist\\narray has a read and write operation. The val variable is used as the index into the hist array, and\\nthe variable at that index is read out, incremented, and written back into the same location.\\n8.3 Histogram Optimization and False Dependencies\\nLet’s look deeper at the recurrence here. In the first iteration'),\n",
       " Document(page_content='at that index is read out, incremented, and written back into the same location.\\n8.3 Histogram Optimization and False Dependencies\\nLet’s look deeper at the recurrence here. In the first iteration of the loop, we read the hist array\\nat some location x and write back to the same location x . The read operation has a latency of\\n0 0\\none clock cycle, so the write has to happen in the following clock. Then in the next iteration of the\\nloop, we read at another location x . Both x and x are dependent on the input and could take\\n1 0 1\\nany value, so we consider the worst case when generating the circuit. In this case, if x == x ,\\n0 1\\nthen the read at location x cannot begin until the previous write has completed. As a result, we\\n1\\nmust alternate between reads and writes.\\nIt turns out that we must alternate between reads and writes as long as x and x are indepen-\\n0 1\\ndent. What if they are not actually independent? For instance, we might know that the source of\\ndata never produces two consecutive'),\n",
       " Document(page_content='between reads and writes as long as x and x are indepen-\\n0 1\\ndent. What if they are not actually independent? For instance, we might know that the source of\\ndata never produces two consecutive pieces of data that actually have the same bin. What do we\\ndo now? If we could give this extra information to the HLS tool, then it would be able to read at\\nlocationx whilewritingatlocationx becauseitcouldguaranteethattheyaredifferentaddresses.\\n1 0\\nIn Vivado(cid:13)R HLS, this is done using the dependence directive.\\nThe modified code is shown in Figure 8.8. Here we’ve explicitly documented (informally) that\\nthe function has some preconditions. In the code, we’ve added an assert() call which checks the\\nsecond precondition. in Vivado(cid:13)R HLS, this assertion is enabled during simulation to ensure that\\nthe simulation testvectors meet the required precondition. The dependence directive captures the\\neffect of this precondition on the circuit, generated by the tool. Namely, it indicates to'),\n",
       " Document(page_content='ensure that\\nthe simulation testvectors meet the required precondition. The dependence directive captures the\\neffect of this precondition on the circuit, generated by the tool. Namely, it indicates to Vivado(cid:13)R\\nHLS that reads and writes to the hist array are dependent only in a particular way. In this case,\\ninter-iteration dependencies consisting of a read operation after a write operation (RAW) have a\\ndistance of 2. In this case a distance of n would indicate that read operations in iteration i+n\\nonly depend on write operations in iteration i. In this case, we assert that in[i+1] != in[i], but it\\ncould be the case that in[i+2] == in[i] so the correct distance is 2.\\n149PREFIX SUM AND HISTOGRAM\\n#include <assert.h>\\n#include ”histogram.h”\\n// Precondition: hist[] is initialized with zeros.\\n// Precondition: for all x, in[x] != in[x+1]\\nvoid histogram(int in[INPUT SIZE], int hist[VALUE SIZE]) {\\n#pragma HLS DEPENDENCE variable=hist inter RAW distance=2\\nint val;\\nint old = −1;\\nfor(int i ='),\n",
       " Document(page_content='Precondition: for all x, in[x] != in[x+1]\\nvoid histogram(int in[INPUT SIZE], int hist[VALUE SIZE]) {\\n#pragma HLS DEPENDENCE variable=hist inter RAW distance=2\\nint val;\\nint old = −1;\\nfor(int i = 0; i < INPUT SIZE; i++) {\\n#pragma HLS PIPELINE\\nval = in[i];\\nassert(old != val);\\nhist[val] = hist[val] + 1;\\nold = val;\\n}\\n}\\nFigure 8.8: An alternative function for computing a histogram. By restricting the inputs and\\nindicating this restriction to Vivado(cid:13)R HLS via the dependence directive, II=1 can be achieved\\nwithout significantly altering the code.\\nIn Figure 8.8, we added a precondition to the code, checked it using an assertion, and\\nindicated the effect of the precondition to the tool using the dependence directive. What\\nhappens if your testbench violates this precondition? What happens if you remove the assert()\\ncall? Does Vivado(cid:13)R HLS still check the precondition? What happens if the precondition is\\nnot consistent with the dependence directive?\\nUnfortunately, the dependence'),\n",
       " Document(page_content='if you remove the assert()\\ncall? Does Vivado(cid:13)R HLS still check the precondition? What happens if the precondition is\\nnot consistent with the dependence directive?\\nUnfortunately, the dependence directive doesn’t really help us if we are unwilling to accept the\\nadditional precondition. It’s also clear that we can’t directly apply same optimization as with the\\nprefixsum function, since we might need to use all of the values stored in the hist array. Another\\nalternative is implement the hist array with a different technology, for instance we could partition\\nthe hist array completely resulting in the array being implemented with flip-flop (FF) resources.\\nSince the data written into a FF on one clock cycle is available immediately on the next clock\\ncycle, this solves the recurrence problem and can be a good solution when a small number of bins\\nare involved. The architecture resulting from such a design is shown in Figure 8.9. However, it\\ntends to be a poor solution when a large number'),\n",
       " Document(page_content='and can be a good solution when a small number of bins\\nare involved. The architecture resulting from such a design is shown in Figure 8.9. However, it\\ntends to be a poor solution when a large number of bins are required. Commonly histograms are\\nconstructed with hundreds to thousands of bins and for large data sets can require many bits of\\nprecision to count all of the inputs. This results in a large number of FF resources and a large mux,\\nwhich also requires logic resources. Storing large histograms in block RAM (BRAM) is usually a\\nmuch better solution.\\nReturning to the code in Figure 8.6, we see that there are really two separate cases that the\\narchitecture must be able to handle. One case is when the input contains consecutive values in the\\nsamebin. Inthiscase,we’dliketouseasimpleregistertoperformtheaccumulationwithaminimal\\namount of delay. The second case is when the input does not contain consecutive values in the\\n150PREFIX SUM AND HISTOGRAM\\nin[] hist[]\\nval\\n+\\n1\\nFigure 8.9: An'),\n",
       " Document(page_content='of delay. The second case is when the input does not contain consecutive values in the\\n150PREFIX SUM AND HISTOGRAM\\nin[] hist[]\\nval\\n+\\n1\\nFigure 8.9: An architecture resulting from the code in Figure 8.6 when the hist array is completely\\npartitioned.\\nsame bin, in which case we need to read, modify, and write back the result to the memory. In this\\ncase, we can guarantee that the read operation of the hist array can not be affected by the previous\\nwrite operation. We’ve seen that both of these cases can be implemented separately, perhaps we\\ncan combine them into a single design. The code to accomplish this is shown in Figure 8.10. This\\ncode uses a local variable old to store the bin from the previous iteration and another local variable\\naccu to store the count for that bin. Each time through the loop we check to see if we are looking\\nat the same bin as the previous iteration. If so, then we can simply increment accu. If not, then\\nwe need to store the value in accu in the hist array and'),\n",
       " Document(page_content='the loop we check to see if we are looking\\nat the same bin as the previous iteration. If so, then we can simply increment accu. If not, then\\nwe need to store the value in accu in the hist array and increment the correct value in the hist\\narray instead. In either case, we update old and accu to contain the current correct values. The\\narchitecture corresponding to this code is shown in Figure 8.11.\\nIn this code, we still need a dependence directive, just as in Figure 8.8, however the form is\\nslightly different. In this case the read and write accesses are to two different addresses in the same\\nloop iteration. Both of these addresses are dependent on the input data and so could point to any\\nindividual element of the hist array. Because of this, Vivado(cid:13)R HLS assumes that both of these\\naccesses could access the same location and as a result schedules the read and write operations to\\nthe array in alternating cycles, resulting in a loop II of 2. However, looking at the code we'),\n",
       " Document(page_content='these\\naccesses could access the same location and as a result schedules the read and write operations to\\nthe array in alternating cycles, resulting in a loop II of 2. However, looking at the code we can\\nreadily see that hist[old] and hist[val] can never access the same location because they are in the\\nelse branch of the conditional if(old == val). Within one iteration (an intra-dependence) a read\\noperation after a write operation (RAW) can never occur and hence is a false dependence. In this\\ncase we are not using the dependence directive to inform the tool about a precondition of the\\nfunction, but instead about a property of the code itself.\\nSynthesize the code from Figure 8.6 and Figure 8.10. What is the initiation interval (II) in\\neach case? What happens when you remove the dependence directive from the code in Figure\\n8.10? How does the loop interval change in both cases? What about the resource usage?\\nFor the code in Figure 8.10, you might question why a tool like Vivado(cid:13)R'),\n",
       " Document(page_content='directive from the code in Figure\\n8.10? How does the loop interval change in both cases? What about the resource usage?\\nFor the code in Figure 8.10, you might question why a tool like Vivado(cid:13)R HLS cannot\\ndetermine this property. In fact, while in some simple cases like this one better code analysis\\n151PREFIX SUM AND HISTOGRAM\\n#include ”histogram.h”\\nvoid histogram(int in[INPUT SIZE], int hist[VALUE SIZE]) {\\nint acc = 0;\\nint i, val;\\nint old = in[0];\\n#pragma HLS DEPENDENCE variable=hist intra RAW false\\nfor(i = 0; i < INPUT SIZE; i++) {\\n#pragma HLS PIPELINE II=1\\nval = in[i];\\nif(old == val) {\\nacc = acc + 1;\\n} else {\\nhist[old] = acc;\\nacc = hist[val] + 1;\\n}\\nold = val;\\n}\\nhist[old] = acc;\\n}\\nFigure 8.10: Removing the read after write dependency from the for loop. This requires an if/else\\nstructure that may seem like it is adding unnecessary complexity to the design. However, it allows\\nfor more effective pipelining despite the fact that the datapath is more complicated.\\ncould propagate the'),\n",
       " Document(page_content='that may seem like it is adding unnecessary complexity to the design. However, it allows\\nfor more effective pipelining despite the fact that the datapath is more complicated.\\ncould propagate the if condition property into each branch, we must accept that there are\\nsome pieces of code where properties of memory accesses are actually undecidable. The highest\\nperformance in such cases will only be achieved in a static schedule with the addition of user\\ninformation. Several recent research works have looked to improve this by introducing some\\ndynamic control logic into the design[60, 44, 19].\\nApictorialdescriptionoftherestructuredcodefromFigure8.10isshowninFigure8.11. Notall\\nof the operations are shown here, but the major idea of the function is there. You can see the two\\nseparate if and else regions (denoted by dotted lines). The acc variable is replicated twice in order\\nto make the drawing more readable; the actual design will only have one register for that variable.\\nThe figure shows'),\n",
       " Document(page_content='regions (denoted by dotted lines). The acc variable is replicated twice in order\\nto make the drawing more readable; the actual design will only have one register for that variable.\\nThe figure shows the two separate datapaths for the if and the else clause with the computation\\ncorresponding to the if clause on the top and the else clause datapath on the bottom.\\n8.4 Increasing Histogram Performance\\nWith some effort, we’ve achieved a design with a loop II of 1. Previously we have seen how further\\nreducing the execution time of a design can be achieved by partial unrolling of the inner loop.\\nHowever, with the histogram function this is somewhat difficult for several reasons. One reason is\\nthe challenging recurrence, unless we can breakup the input data in somefashion, thecomputation\\nof one iteration of the loop must be completed with the computation of the next iteration of the\\nloop. A second reason is that with a loop II of 1, the circuit performs a read and a write of the\\n152PREFIX SUM'),\n",
       " Document(page_content='of the loop must be completed with the computation of the next iteration of the\\nloop. A second reason is that with a loop II of 1, the circuit performs a read and a write of the\\n152PREFIX SUM AND HISTOGRAM\\nold acc\\nin[] hist[]\\nval T\\n= + 1\\nF acc old\\n+ new\\n1\\nFigure 8.11: A depiction of the datapath corresponding to the code in Figure 8.10. There are two\\nseparate portions corresponding to the if and else clauses. The figure shows the important portions\\nof the computation, and leaves out some minor details.\\nhist array each clock cycle, occupying both ports of a BRAM resource in the FPGA. Previously we\\nhave considered array partitioning to increase the number of memory ports for accessing an array,\\nbut there’s not a particularly obvious way to partition the hist array since the access order depends\\non the input data.\\nAll is not lost, however, as there is a way we can expose more parallelism by decomposing\\nthe histogram computation into two stages. In the first stage, we divide the input data'),\n",
       " Document(page_content='the input data.\\nAll is not lost, however, as there is a way we can expose more parallelism by decomposing\\nthe histogram computation into two stages. In the first stage, we divide the input data into a\\nnumber of separate partitions. The histogram for each partition can be computed independently\\nusing a separate instance, often called a Processing Element (PE), of the histogram solution we’ve\\ndeveloped previously. In the second stage, the individual histograms are combined to generate the\\nhistogram of the complete data sets. This partitioning (or mapping) and merging (or reducing)\\nprocess is very similar to that adopted by the MapReduce framework [20] and is a common pattern\\nfor parallel computation. The map-reduce pattern is applicable whenever there is recurrence which\\nincludes a commutative and associative operation, such as addition in this case. This idea is shown\\nin Figure 8.12.\\nThe code for implementing this architecture is shown in Figure 8.13. The histogram map func-\\ntion'),\n",
       " Document(page_content='and associative operation, such as addition in this case. This idea is shown\\nin Figure 8.12.\\nThe code for implementing this architecture is shown in Figure 8.13. The histogram map func-\\ntion implements the ‘map’ portion of the map-reduce pattern and will be instantiated multiple\\ntimes. The code is very similar to the code in Figure 8.10. The main difference is that we have\\nadded the additional code to initialize the hist array. The histogram map function takes an input\\narray in which will contain a partition of the data being processed and computes the histogram of\\nthat partition in the hist array. The histogram reduce function implements the ‘reduce’ portion of\\nthe pattern. It takes as input a number of partial histograms and combines them into complete\\nhistogram by adding together the count for each histogram bin. In our code example in Figure\\n8.13, we have only two processing elements, thus the merge has two input arrays hist1 and hist2.\\nThis can easily be extended to handle more'),\n",
       " Document(page_content='count for each histogram bin. In our code example in Figure\\n8.13, we have only two processing elements, thus the merge has two input arrays hist1 and hist2.\\nThis can easily be extended to handle more processing elements.\\n153PREFIX SUM AND HISTOGRAM\\nin[]\\na) b)\\nProcessing Element (PE)\\nold acc PE PE . . . PE\\nin[] hist[]\\nval T\\n= + 1\\nF acc old\\n+ new\\nMerge\\n1\\nhist[]\\nFigure 8.12: The histogram computation implemented using a map-reduce pattern. The processing\\nelement (PE) in Part a) is the same architecture as shown in Figure 8.11. The in array is parti-\\ntioned and each partition is processed by a separate PE. The merge block combines the individual\\nhistograms to create the final histogram.\\nThe new histogram function takes as an input two partitions of the input data, stored in the\\ninputA and inputB arrays. It computes the histogram of each partition using the histogram map\\nfunction,whicharethenstoredinthehist1andhist2arrays. Thesearefeedintothehistogram reduce\\nfunction which combines them and'),\n",
       " Document(page_content='arrays. It computes the histogram of each partition using the histogram map\\nfunction,whicharethenstoredinthehist1andhist2arrays. Thesearefeedintothehistogram reduce\\nfunction which combines them and stores the result in the hist array, which is the final output of\\nthe top level function histogram.\\nModify the code in Figure 8.13 to support a parameterizable number NUM PE of PEs?\\nHint: You’ll need to combine some of the arrays into a single array that is correctly partitioned\\nand add some loops that depend on NUM PE. What happens to the throughput and task\\ninterval as you vary the number of PEs?\\nWe use the dataflow directive in the histogram function in order to enable a design with\\ntaskpipelining. Inthiscasetherearethreeprocesses: twoinstancesofthepartial histogramfunction\\nand one instance of the histogram reduce function. Within a single task, the two partial histogram\\nprocessescanexecuteconcurrentlysincetheyworkonindependentdata,whilethehistogram reduce\\nfunction must execute after'),\n",
       " Document(page_content='of the histogram reduce function. Within a single task, the two partial histogram\\nprocessescanexecuteconcurrentlysincetheyworkonindependentdata,whilethehistogram reduce\\nfunction must execute after since it uses the results from the partial histogram processes. Thus, the\\ndataflow directive essentially creates a two stage task pipeline with the partial histogram functions\\nin the first stage and the histogram reduce function in the second stage. As with any dataflow\\ndesign, the interval of the entire histogram function depends upon the maximum initiation in-\\nterval of the two stages. The two partial histogram functions in the first stage are the same\\nand will have the same interval (II ). The histogram reduce function will have an-\\nhistogram map\\nother interval (II ). The interval of the toplevel histogram function II is then\\nhistogram reduce histogram\\nmax(II ,II ).\\nhistogram map histogram reduce\\n154PREFIX SUM AND HISTOGRAM\\n#include ”histogram parallel.h”\\nvoid histogram map(int in[INPUT'),\n",
       " Document(page_content='histogram function II is then\\nhistogram reduce histogram\\nmax(II ,II ).\\nhistogram map histogram reduce\\n154PREFIX SUM AND HISTOGRAM\\n#include ”histogram parallel.h”\\nvoid histogram map(int in[INPUT SIZE/2], int hist[VALUE SIZE]) {\\n#pragma HLS DEPENDENCE variable=hist intra RAW false\\nfor(int i = 0; i < VALUE SIZE; i++) {\\n#pragma HLS PIPELINE II=1\\nhist[i] = 0;\\n}\\nint old = in[0];\\nint acc = 0;\\nfor(int i = 0; i < INPUT SIZE/2; i++) {\\n#pragma HLS PIPELINE II=1\\nint val = in[i];\\nif(old == val) {\\nacc = acc + 1;\\n} else {\\nhist[old] = acc;\\nacc = hist[val] + 1;\\n}\\nold = val;\\n}\\nhist[old] = acc;\\n}\\nvoid histogram reduce(int hist1[VALUE SIZE], int hist2[VALUE SIZE], int output[VALUE SIZE]) {\\nfor(int i = 0; i < VALUE SIZE; i++) {\\n#pragma HLS PIPELINE II=1\\noutput[i] = hist1[i] + hist2[i];\\n}\\n}\\n//Top level function\\nvoid histogram(int inputA[INPUT SIZE/2], int inputB[INPUT SIZE/2], int hist[VALUE SIZE]){\\n#pragma HLS DATAFLOW\\nint hist1[VALUE SIZE];\\nint hist2[VALUE SIZE];\\nhistogram map(inputA, hist1);\\nhistogram'),\n",
       " Document(page_content='histogram(int inputA[INPUT SIZE/2], int inputB[INPUT SIZE/2], int hist[VALUE SIZE]){\\n#pragma HLS DATAFLOW\\nint hist1[VALUE SIZE];\\nint hist2[VALUE SIZE];\\nhistogram map(inputA, hist1);\\nhistogram map(inputB, hist2);\\nhistogram reduce(hist1, hist2, hist);\\n}\\nFigure 8.13: Another implementation of histogram that uses task level parallelism and pipelining.\\nThe histogram operation is split into two sub tasks, which are executed in the two histogram map\\nfunctions. These results are combined in the final histogram result using the histogram reduce\\nfunction. The histogram function is the top level function that connects these three functions\\ntogether.\\n155PREFIX SUM AND HISTOGRAM\\nWhat happens when you add or change the locations of the pipeline directives? For exam-\\nple, is it beneficial to add a pipeline directive to the for loop in the histogram reduce function?\\nWhatistheresultofmovingthepipelinedirectiveintothehistogram mapfunction,i.e.,hoisting\\nit outside of the for loop where it currently'),\n",
       " Document(page_content='pipeline directive to the for loop in the histogram reduce function?\\nWhatistheresultofmovingthepipelinedirectiveintothehistogram mapfunction,i.e.,hoisting\\nit outside of the for loop where it currently resides?\\nThe goal of this section was to walk through the optimization the histogram computation,\\nanother small but important kernel of many applications. The key takeaway is that there are often\\nlimits to what tools can understand about our programs. In some cases we must take care in how\\nwewritethecodeandinothercaseswemustactuallygivethetoolmoreinformationaboutthecode\\nor the environment that the code is executing in. In particular, properties about memory access\\npatterns often critically affect the ability of HLS to generate correct and efficient hardware. In\\nVivado(cid:13)R HLS, these properties can be expressed using the dependence directive. Sometimes these\\noptimizations might even be counter-intuitive, such as the addition of the if/else control structure\\nin 8.10. In other cases'),\n",
       " Document(page_content='properties can be expressed using the dependence directive. Sometimes these\\noptimizations might even be counter-intuitive, such as the addition of the if/else control structure\\nin 8.10. In other cases optimizations might require some creativity, as in applying the map-reduce\\npattern in Figures 8.12 and 8.13).\\n8.5 Conclusion\\nIn this section, we’ve looked at the prefix sum and histogram kernels. Although these functions\\nseem different, they both contain recurrences through a memory access. These recurrences can\\nlimit throughput if the memory access is not pipelined. In both cases, by rewriting the code we can\\nremove the recurrence. In the case of the prefix sum, this is much easier since the access patterns\\nare deterministic. In the case of the histogram we must rewrite the code to address the recurrence\\nor ensure that recurrence never happens in practice. In either case we needed a way to describe to\\nVivado(cid:13)R HLS information about the environment or about the code itself that the'),\n",
       " Document(page_content='recurrence\\nor ensure that recurrence never happens in practice. In either case we needed a way to describe to\\nVivado(cid:13)R HLS information about the environment or about the code itself that the tool was unable\\ntodetermineforitself. Thisinformationiscapturedinthedependencedirective. Lastly, welooked\\nat ways of parallelizing both algorithms yet further, so that they could process a number of data\\nsamples each clock cycle.\\n156Chapter 9\\nVideo Systems\\n9.1 Background\\nVideoProcessingisacommonapplicationforFPGAs. Onereasonisthatcommonvideodatarates\\nmatchwelltheclockfrequenciesthatcanachievedwithmodernFPGAs. Forinstance, thecommon\\nHigh-Definition TV format known as FullHD or 1080P60 video requires 1920pixels ∗1080lines ∗\\nline frame\\n60frames = 124,416,000pixels.\\nsecond second\\nWhen encoded in a digital video stream, these pixels are transmitted along with some blank\\npixels at 148.5 MHz, and can be processed in a pipelined FPGA circuit at that frequency. Higher\\ndata rates can also be achieved'),\n",
       " Document(page_content='digital video stream, these pixels are transmitted along with some blank\\npixels at 148.5 MHz, and can be processed in a pipelined FPGA circuit at that frequency. Higher\\ndata rates can also be achieved by processing multiple samples per clock cycle. Details on how\\ndigital video is transmitted will come in Section 9.1.2. Another reason is that video is mostly\\nprocessed in scanline order line-by-line from the top left pixel to the lower right pixel, as shown in\\nFigure9.1. Thispredictableorderallowshighlyspecializedmemoryarchitecturestobeconstructed\\nin FPGA circuits to efficiently process video without excess storage. Details on these architectures\\nwill come in Section 9.2.1\\nVideo processing is also a good target application for HLS. Firstly, video processing is typically\\ntolerant to processing latency. Many applications can tolerate several frames of processing delay,\\nalthough some applications may limit the overall delay to less than one frame. As a result, highly\\npipelined'),\n",
       " Document(page_content='to processing latency. Many applications can tolerate several frames of processing delay,\\nalthough some applications may limit the overall delay to less than one frame. As a result, highly\\npipelined implementations can be generated from throughput and clock constraints in HLS with\\nlittle concern for processing latency. Secondly, video algorithms are often highly non-standardized\\nand developed based on the personal taste or intuition of an algorithm expert. This leads them\\n(0, 0) (0, width-1)\\n(height-1, 0) (height-1, width-1)\\nFigure 9.1: Scanline processing order for video frames.\\n157VIDEO SYSTEMS\\nto be developed in a high-level language where they can be quickly developed and simulated on\\nsequences of interest. It is not uncommon for FullHD video processing algorithms to run at 60\\nframespersecondinanFPGAsystem, oneframepersecondinsynthesizableC/C++coderunning\\non a development laptop, but only one frame per hour (or slower) in an RTL simulator. Lastly,\\nvideo processing algorithms are'),\n",
       " Document(page_content='oneframepersecondinsynthesizableC/C++coderunning\\non a development laptop, but only one frame per hour (or slower) in an RTL simulator. Lastly,\\nvideo processing algorithms are often easily expressed in a nested-loop programming style that is\\namenabletoHLS.ThismeansthatmanyvideoalgorithmscanbesynthesizedintoanFPGAcircuit\\ndirectly from the C/C++ code that an algorithm developer would write for prototyping purposes\\nanyway\\n9.1.1 Representing Video Pixels\\nMany video input and output systems are optimized around the way that the human vision system\\nperceives light. One aspect of this is that the cones in the eye, which sense color, are sensitive\\nprimarily to red, green, and blue light. Other colors are perceived as combinations of red, green,\\nand blue light. As a result, video cameras and displays mimic the capabilities of the human vision\\nsystem and are primarily sensitive or capable of displaying red, green, and blue light and pixels\\nare often represented in the RGB colorspace as a'),\n",
       " Document(page_content='displays mimic the capabilities of the human vision\\nsystem and are primarily sensitive or capable of displaying red, green, and blue light and pixels\\nare often represented in the RGB colorspace as a combination of red, green, and blue components.\\nMost commonly each component is represented with 8 bits for a total of 24 bits per pixel, although\\nother combinations are possible, such as 10 or even 12 bits per pixel in high-end systems.\\nA second aspect is that the human visual system interprets brightness with somewhat higher\\nresolution than color. Hence, within a video processing system it is common to convert from the\\nRGB colorspace to the YUV colorspace, which describes pixels as a combination of Luminance\\n(Y) and Chrominance (U and V). This allows the color information contained in the U and V\\ncomponents to be represented independently of the brightness information in the Y component.\\nOne common video format, known as YUV422, represents two horizontally adjacent pixels with\\ntwo Y'),\n",
       " Document(page_content='U and V\\ncomponents to be represented independently of the brightness information in the Y component.\\nOne common video format, known as YUV422, represents two horizontally adjacent pixels with\\ntwo Y values, one U value and one V value. This format essentially includes a simple form of video\\ncompression called chroma subsampling. Another common video format, YUV420, represents four\\npixels in a square with 4 Y values, one U value and one V value, further reducing the amount of\\ndata required. Video compression is commonly performed on data in the YUV colorspace.\\nA third aspect is that the rods and codes in eye are more sensitive to green light than red or\\nblue light and that the brain primarily interprets brightness primarily from green light. As a result,\\nsolid-statesensorsanddisplayscommonlyuseamosaicpattern, suchastheBayer pattern[7]which\\nconsists of 2 green pixels for every red or blue pixel. The end result is that higher resolution images\\ncan be produced for the same number of pixel'),\n",
       " Document(page_content='suchastheBayer pattern[7]which\\nconsists of 2 green pixels for every red or blue pixel. The end result is that higher resolution images\\ncan be produced for the same number of pixel elements, reducing the manufacturing cost of sensors\\nand displays.\\nVideo systems have been engineered around the human visual system for many years. The\\nearliestblackandwhitevideo cameraswere primarily sensitiveto blue-greenlightto matchthe\\neye’s sensitivity to brightness in that color range. However, they were unfortunately not very\\nsensitive to red light, as a result red colors (such as in makeup) didn’t look right on camera.\\nThe solution was decidedly low-tech: actors wore garish green and blue makeup.\\n9.1.2 Digital Video Formats\\nInadditiontorepresentingindividualpixels,digitalvideoformatsmustalsoencodetheorganization\\nof pixels into video frames. In many cases, this is done with synchronization or sync signals that\\nindicatethestartandstopofthevideoframeinanotherwisecontinuoussequenceofpixels.'),\n",
       " Document(page_content='pixels into video frames. In many cases, this is done with synchronization or sync signals that\\nindicatethestartandstopofthevideoframeinanotherwisecontinuoussequenceofpixels. Insome\\n158VIDEO SYSTEMS\\n5\\n37\\n1080\\n3\\n44 147 1920 89\\nFigure 9.2: Typical synchronization signals in a 1080P60 high definition video signal.\\nstandards (such as the Digital Video Interface or DVI) sync signals are represented as physically\\nseparate wires. In other standards (such as the Digital Television Standard BTIR 601/656) the\\nstart and stop of the sync signal is represented by special pixel values that don’t otherwise occur\\nin the video signal.\\nEach line of video (scanned from left to right) is separated by a Horizontal Sync Pulse. The\\nhorizontal sync is active for a number of cycles between each video line. In addition, there are\\na small number of pixels around the pulse where the horizontal sync is not active, but there are\\nnot active video pixels. These regions before and after the horizontal sync pulse are'),\n",
       " Document(page_content='there are\\na small number of pixels around the pulse where the horizontal sync is not active, but there are\\nnot active video pixels. These regions before and after the horizontal sync pulse are called the\\nHorizontal Front Porch and Horizontal Back Porch, respectively. Similarly, each frame of video\\n(scanned from top to bottom) is separated by a Vertical Sync Pulse. The vertical sync is active for\\na number of lines between each video frame. Note that the vertical sync signal only changes at the\\nstart of the horizontal sync signal. There are also usually corresponding Vertical Front Porch and\\nVertical Back Porch areas consisting of video lines where the vertical sync is not active, but there\\nare not active video pixels either. In addition, most digital video formats include a Data Enable\\nsignal that indicates the active video pixels. Together, all of the video pixels that aren’t active are\\ncalled the Horizontal Blanking Interval and Vertical Blanking Interval. These signals are'),\n",
       " Document(page_content='Enable\\nsignal that indicates the active video pixels. Together, all of the video pixels that aren’t active are\\ncalled the Horizontal Blanking Interval and Vertical Blanking Interval. These signals are shown\\ngraphically in Figure 9.2.\\nThe format of digital video signals is, in many ways, an artifact of the original analog\\ntelevisionstandards, suchasNTSCintheUnitedStatesandPALinmanyEuropeancountries.\\nSince the hardware for analog scanning of Cathode Ray Tubes contained circuits with limited\\nslew rates, the horizontal and vertical sync intervals allowed time for the scan recover to the\\nbeginning of the next line. These sync signals were represented by a large negative value in the\\nvideo signal. In addition, since televisions were not able to effectively display pixels close to\\nthe strong sync signal, the front porches and the back porches were introduced to increase the\\namount of the picture that could be shown. Even then, many televisions were designed with\\noverscan, where up to 20% of'),\n",
       " Document(page_content='signal, the front porches and the back porches were introduced to increase the\\namount of the picture that could be shown. Even then, many televisions were designed with\\noverscan, where up to 20% of the pixels at the edge the frame were not visible.\\nThe typical 1080P60 video frame shown in Figure 9.2 contains a total of 2200 ∗ 1125 data\\n159VIDEO SYSTEMS\\n#include ”video common.h”\\nunsigned char rescale(unsigned char val, unsigned char offset, unsigned char scale) {\\nreturn ((val − offset) ∗ scale) >> 4;\\n}\\nrgb pixel rescale pixel(rgb pixel p, unsigned char offset, unsigned char scale) {\\n#pragma HLS pipeline\\np.R = rescale(p.R, offset, scale);\\np.G = rescale(p.G, offset, scale);\\np.B = rescale(p.B, offset, scale);\\nreturn p;\\n}\\nvoid video filter rescale(rgb pixel pixel in[MAX HEIGHT][MAX WIDTH],\\nrgb pixel pixel out[MAX HEIGHT][MAX WIDTH],\\nunsigned char min, unsigned char max) {\\n#pragma HLS interface ap hs port = pixel out\\n#pragma HLS interface ap hs port = pixel in\\nrow loop:\\nfor (int row = 0; row'),\n",
       " Document(page_content='pixel pixel out[MAX HEIGHT][MAX WIDTH],\\nunsigned char min, unsigned char max) {\\n#pragma HLS interface ap hs port = pixel out\\n#pragma HLS interface ap hs port = pixel in\\nrow loop:\\nfor (int row = 0; row < MAX WIDTH; row++) {\\ncol loop:\\nfor (int col = 0; col < MAX HEIGHT; col++) {\\n#pragma HLS pipeline\\nrgb pixel p = pixel in[row][col];\\np = rescale pixel(p,min,max);\\npixel out[row][col] = p;\\n}\\n}\\n}\\nFigure 9.3: Code implementing a simple video filter.\\nsamples. At 60 frames per second, this corresponds to an overall sample rate of 148.5 Million\\nsamples per second. This is quite a bit higher than the average rate of active video pixels in a\\nframe, 1920∗1080∗60 = 124.4 Million pixels per second. Most modern FPGAs can comfortably\\nprocess at this clock rate, often leading to 1 sample-per-clock cycle architectures. Systems that\\nuse higher resolutions, such as 4K by 2K for digital cinema, or higher frame rates, such as 120\\nor even 240 frames per second often require more the one sample to be processed'),\n",
       " Document(page_content='Systems that\\nuse higher resolutions, such as 4K by 2K for digital cinema, or higher frame rates, such as 120\\nor even 240 frames per second often require more the one sample to be processed per clock cycle.\\nRemember that such architectures can often be generated by unrolling loops in HLS (see Section\\n1.4.2). Similarly, when processing lower resolutions or frame rates, processing each sample over\\nmultiple clocks may be preferable, enabling operator sharing. Such architectures can often be\\ngenerated by increasing the II of loops.\\nFor instance, the code shown in Figure 9.3 illustrates a simple video processing application that\\nprocesses one sample per clock cycle with the loop implemented at II=1. The code is written with\\na nested loop over the pixels in the image, following the scanline order shown in 9.1. An II=3\\n160VIDEO SYSTEMS\\nDataflow\\naddress address address address\\ndata BRAM data Accelerator 1 data BRAM data\\nWE WE\\nvoid video filter(rgb pixel pixel in[MAX HEIGHT][MAX WIDTH],\\nrgb'),\n",
       " Document(page_content='order shown in 9.1. An II=3\\n160VIDEO SYSTEMS\\nDataflow\\naddress address address address\\ndata BRAM data Accelerator 1 data BRAM data\\nWE WE\\nvoid video filter(rgb pixel pixel in[MAX HEIGHT][MAX WIDTH],\\nrgb pixel pixel out[MAX HEIGHT][MAX WIDTH]) {\\n#pragma HLS interface ap memory port = pixel out // The default\\n#pragma HLS interface ap memory port = pixel in // The default\\nFigure 9.4: Integration of a video design with BRAM interfaces.\\ndesign could share the rescale function computed for each component, enabling reduced area usage.\\nUnrolling the inner loop by a factor of 2 and partitioning the input and the output arrays by an\\nappropriate factor of 2 could enable processing 2 pixels every clock cycle in an II=1 design. This\\ncase is relatively straightforward, since the processing of each component and of individual pixels\\nis independent. More complicated functions might not benefit from resource sharing, or might not\\nbe able to process more than one pixel simultaneously.\\nA high-speed'),\n",
       " Document(page_content='component and of individual pixels\\nis independent. More complicated functions might not benefit from resource sharing, or might not\\nbe able to process more than one pixel simultaneously.\\nA high-speed computer vision application processes small video frames of 200 * 180 pixels\\nat 10000 frames per second. This application uses a high speed sensor interfaced directly to\\nthe FPGA and requires no sync signals. How many samples per clock cycle would you attempt\\nto process? Is this a good FPGA application? Write the nested loop structure to implement\\nthis structure using HLS.\\n9.1.3 Video Processing System Architectures\\nUp to this point, we have focused on building video processing applications without concern for\\nhow they are integrated into a system. In many cases, such as the example code in Figure 9.12,\\nthe bulk of the processing occurs within a loop over the pixels and can process one pixel per clock\\nwhen the loop is active. In this section we will discuss some possibilities for system'),\n",
       " Document(page_content='Figure 9.12,\\nthe bulk of the processing occurs within a loop over the pixels and can process one pixel per clock\\nwhen the loop is active. In this section we will discuss some possibilities for system integration.\\nBy default, Vivado(cid:13)R HLS generates a simple memory interface for interface arrays. This inter-\\nface consists of address and data signals and a write enable signal in the case of a write interface.\\nEach read or write of data is associated with a new address and the expected latency through the\\nmemory is fixed. It is simple to integrate such an interface with on-chip memories created from\\nBlock RAM resources as shown in Figure 9.4. However Block RAM resources are generally a poor\\nchoice for storing video data because of the large size of each frame, which would quickly exhaust\\nthe Block RAM resources even in large expensive devices.\\nFor 1920x1080 frames with 24 bits per pixel, how many Block RAM resources are required\\nto store each video frame? How many frames can be'),\n",
       " Document(page_content='Block RAM resources even in large expensive devices.\\nFor 1920x1080 frames with 24 bits per pixel, how many Block RAM resources are required\\nto store each video frame? How many frames can be stored in the Block RAM of the FPGA\\nyou have available?\\nA better choice for most video systems is to store video frames in external memory, typically\\nsome form of double-data-rate (DDR) memory. Typical system integration with external memory\\n161VIDEO SYSTEMS\\nDataflow\\nHDMI HDMI\\nVideo Input Accelerator 1 Accelerator 2 Video Output\\nAXI Interconnect\\nExternal\\nMemory\\nvoid video filter(pixel t pixel in[MAX HEIGHT][MAX WIDTH],\\npixel t pixel out[MAX HEIGHT][MAX WIDTH]) {\\n#pragma HLS interface m axi port = pixel out\\n#pragma HLS interface m axi port = pixel in\\nFigure 9.5: Integration of a video design with external memory interfaces.\\nisshowninFigure9.5. AnFPGAcomponentknownasexternal memory controller implementsthe\\nexternal DDR interface and provides a standardized interface to other FPGA components through\\na'),\n",
       " Document(page_content='memory interfaces.\\nisshowninFigure9.5. AnFPGAcomponentknownasexternal memory controller implementsthe\\nexternal DDR interface and provides a standardized interface to other FPGA components through\\na common interface, such as the ARM AXI4 slave interface [6]. FPGA components typically\\nimplement a complementary master interface which can be directly connected to the slave interface\\noftheexternalmemorycontrollerorconnectedthroughspecializedAXI4 interconnect components.\\nThe AXI4 interconnect allows multiple multiple master components to access a number of slave\\ncomponents. This architecture abstracts the details of the external memory, allowing different\\nexternal memory components and standards to be used interchangeably without modifying other\\nFPGA components.\\nAlthough most processor systems are built with caches and require them for high performance\\nprocessing,itistypicaltoimplementFPGA-basedvideoprocessingsystemsasshownin9.5without\\non-chip caches. In a processor system, the cache'),\n",
       " Document(page_content='are built with caches and require them for high performance\\nprocessing,itistypicaltoimplementFPGA-basedvideoprocessingsystemsasshownin9.5without\\non-chip caches. In a processor system, the cache provides low-latency access to previously accessed\\ndata and improves the bandwidth of access to external memory by always reading or writing\\ncomplete cache lines. Some processors also use more complex mechanisms, such as prefetching\\nand speculative reads in order to reduce external memory latency and increase external memory\\nbandwidth. For most FPGA-based video processing systems simpler techniques leveraging line\\nbuffers and window buffers are sufficient to avoid fetching any data from external memory more\\nthan once, due to the predictable access patterns of most video algorithms. Additionally, Vivado(cid:13)R\\nHLS is capable of scheduling address transactions sufficiently early to avoid stalling'),\n",
       " Document(page_content='more\\nthan once, due to the predictable access patterns of most video algorithms. Additionally, Vivado(cid:13)R\\nHLS is capable of scheduling address transactions sufficiently early to avoid stalling computation\\nduetoexternalmemorylatencyandiscapableofstaticallyinferringburstaccessesfromconsecutive\\nmemory accesses.\\nAn alternative external memory architecture is shown in Figure 9.6. In this architecture, an\\naccelerator is connected to an external Direct Memory Access (DMA) component that performs\\nthe details of generating addresses to the memory controller. The DMA provides a stream of data\\nto the accelerator for processing and consumes the data produced by the accelerator and writes it\\nback to memory. In Vivado(cid:13)R HLS, there are multiple coding styles that can generate a streaming\\n162VIDEO SYSTEMS\\nDataflow Accelerator 1 Accelerator 2\\nHDMI HDMI\\nVideo Input DMA DMA Video Output\\nAXI Interconnect\\nExternal\\nMemory\\nvoid video filter(pixel t pixel in[MAX HEIGHT][MAX WIDTH],\\npixel t pixel'),\n",
       " Document(page_content='SYSTEMS\\nDataflow Accelerator 1 Accelerator 2\\nHDMI HDMI\\nVideo Input DMA DMA Video Output\\nAXI Interconnect\\nExternal\\nMemory\\nvoid video filter(pixel t pixel in[MAX HEIGHT][MAX WIDTH],\\npixel t pixel out[MAX HEIGHT][MAX WIDTH]) {\\n#pragma HLS interface s axi port = pixel out\\n#pragma HLS interface s axi port = pixel in\\nFigure 9.6: Integration of a video design with external memory interfaces through a DMA compo-\\nnent.\\nvoid video filter(pixel t pixel in[MAX HEIGHT][MAX WIDTH],\\npixel t pixel out[MAX HEIGHT][MAX WIDTH]) {\\n#pragma HLS interface ap hs port = pixel out\\n#pragma HLS interface ap hs port = pixel in\\nvoid video filter(hls::stream<pixel t> &pixel in,\\nhls::stream<pixel t> &pixel out) {\\nFigure 9.7: Coding styles for modelling streaming interfaces in HLS.\\ninterface, as shown in Figure 9.7. One possibility is to model the streaming interfaces as arrays. In\\nthis case, the C code is very similar to the code seen previously, but different interface directives\\nare used. An alternative is to model'),\n",
       " Document(page_content='is to model the streaming interfaces as arrays. In\\nthis case, the C code is very similar to the code seen previously, but different interface directives\\nare used. An alternative is to model the streaming interface explicitly, using the hls::stream<>\\nclass. In either case, some care must be taken that the order of data generated by the DMA engine\\nis the same as the order in which the data is accessed in the C code.\\nOne advantage of streaming interfaces is that they allow multiple accelerators to be composed\\ninadesignwithouttheneedtostoreintermediatevaluesinexternalmemory. Insomecases, FPGA\\nsystems can be built without external memory at all, by processing pixels as they are received on\\nan input interface (such as HDMI) and sending them directly to an output interface, as shown in\\nFigure 9.8. Such designs typically have accelerator throughput requirements that must achieved in\\norder to meet the strict real-time constraints at the external interfaces. Having at least one frame\\nbuffer in'),\n",
       " Document(page_content='Such designs typically have accelerator throughput requirements that must achieved in\\norder to meet the strict real-time constraints at the external interfaces. Having at least one frame\\nbuffer in the system provides more flexibility to build complex algorithms that may be hard to\\nconstruct with guaranteed throughput. A frame buffer can also simplify building systems where\\ntheinputandoutputpixelratesaredifferentorpotentiallyunrelated(suchasasystemthatreceives\\n163VIDEO SYSTEMS\\nDataflow\\nHDMI HDMI\\nVideo Input Accelerator 1 Accelerator 2 Video Output\\nvoid video filter(pixel t pixel in[MAX HEIGHT][MAX WIDTH],\\npixel t pixel out[MAX HEIGHT][MAX WIDTH]) {\\n#pragma HLS interface ap hs port = pixel out\\n#pragma HLS interface ap hs port = pixel in\\nFigure 9.8: Integration of a video design with streaming interfaces.\\nan arbitrary input video format and outputs an different arbitrary format).\\n9.2 Implementation\\nWhen actually processing video in a system it is common to factor out the system'),\n",
       " Document(page_content='streaming interfaces.\\nan arbitrary input video format and outputs an different arbitrary format).\\n9.2 Implementation\\nWhen actually processing video in a system it is common to factor out the system integration\\naspects from the implementation of video processing algorithms. For the remainder of this chapter\\nwewillassumethatinputpixelsarearrivinginastreamofpixelsandmustbeprocessedinscanline\\norder. The actual means by which this happens is largely unimportant, as long as HLS meets the\\nrequired performance goals.\\n9.2.1 Line Buffers and Frame Buffers\\nVideo processing algorithms typically compute an output pixel or value from a nearby region of\\ninput pixels, often called a window. Conceptually, the window scans across the input image,\\nselecting a region of pixels that can be used to compute the corresponding output pixel. For\\ninstance, Figure 9.9 shows code that implements a 2-dimensional filter on a video frame. This code\\nreads a window of data from the input video frame (stored in an'),\n",
       " Document(page_content='the corresponding output pixel. For\\ninstance, Figure 9.9 shows code that implements a 2-dimensional filter on a video frame. This code\\nreads a window of data from the input video frame (stored in an array) before computing each\\noutput pixel.\\nIn Figure 9.9, there is the code int wi = row+i−1; int wj = col+j−1;. Explain why these\\nexpressions include a ’-1’. Hint: Would the number change if the filter were 7x7 instead of 3x3?\\nNote that in this code, multiple reads of pixel in must occur to populate the window memory\\nand compute one output pixel. If only one read can be performed per cycle, then this code is\\nlimited in the pixel rate that it can support. Essentially this is a 2-Dimensional version of the\\none-tap-per-cycle filter from Figure 1.8. In addition, the interfacing options are limited, because\\nthe input is not read in normal scan-line order. (This topic will be dealt with in more detail in\\nSection 9.1.3.\\nA key observation about adjacent windows is that they often overlap, implying'),\n",
       " Document(page_content='because\\nthe input is not read in normal scan-line order. (This topic will be dealt with in more detail in\\nSection 9.1.3.\\nA key observation about adjacent windows is that they often overlap, implying a high locality\\nof reference. This means that pixels from the input image can be buffered locally or cached and\\naccessedmultipletimes. Byrefactoringthecodetoreadeachinputpixelexactlyonceandstorethe\\nresult in a local memory, a better result can be achieved. In video systems, the local buffer is also\\ncalled a line buffer, since it typically stores several lines of video around the window. Line buffers\\n164VIDEO SYSTEMS\\nrgb pixel filter(rgb pixel window[3][3]) {\\nconst char h[3][3] = {{1, 2, 1}, {2, 4, 2}, {1, 2, 1}};\\nint r = 0, b = 0, g = 0;\\ni loop: for (int i = 0; i < 3; i++) {\\nj loop: for (int j = 0; j < 3; j++) {\\nr += window[i][j].R ∗ h[i][j];\\ng += window[i][j].G ∗ h[i][j];\\nb += window[i][j].B ∗ h[i][j];\\n}\\n}\\nrgb pixel output;\\noutput.R = r / 16;\\noutput.G = g / 16;\\noutput.B = b / 16;\\nreturn'),\n",
       " Document(page_content='j = 0; j < 3; j++) {\\nr += window[i][j].R ∗ h[i][j];\\ng += window[i][j].G ∗ h[i][j];\\nb += window[i][j].B ∗ h[i][j];\\n}\\n}\\nrgb pixel output;\\noutput.R = r / 16;\\noutput.G = g / 16;\\noutput.B = b / 16;\\nreturn output;\\n}\\nvoid video 2dfilter(rgb pixel pixel in[MAX HEIGHT][MAX WIDTH],\\nrgb pixel pixel out[MAX HEIGHT][MAX WIDTH]) {\\nrgb pixel window[3][3];\\nrow loop: for (int row = 0; row < MAX HEIGHT; row++) {\\ncol loop: for (int col = 0; col < MAX WIDTH; col++) {\\n#pragma HLS pipeline\\nfor (int i = 0; i < 3; i++) {\\nfor (int j = 0; j < 3; j++) {\\nint wi = row + i − 1;\\nint wj = col + j − 1;\\nif (wi < 0 || wi >= MAX HEIGHT || wj < 0 || wj >= MAX WIDTH) {\\nwindow[i][j].R = 0;\\nwindow[i][j].G = 0;\\nwindow[i][j].B = 0;\\n} else\\nwindow[i][j] = pixel in[wi][wj];\\n}\\n}\\nif (row == 0 || col == 0 || row == (MAX HEIGHT − 1) || col == (MAX WIDTH − 1)) {\\npixel out[row][col].R = 0;\\npixel out[row][col].G = 0;\\npixel out[row][col].B = 0;\\n} else\\npixel out[row][col] = filter(window);\\n}\\n}\\n}\\nFigure 9.9: Code implementing a 2D filter'),\n",
       " Document(page_content='col == (MAX WIDTH − 1)) {\\npixel out[row][col].R = 0;\\npixel out[row][col].G = 0;\\npixel out[row][col].B = 0;\\n} else\\npixel out[row][col] = filter(window);\\n}\\n}\\n}\\nFigure 9.9: Code implementing a 2D filter without an explicit line buffer.\\n165VIDEO SYSTEMS\\naretypicallyimplementedinblockRAM(BRAM)resources, whilewindowbuffersareimplemented\\nusing flip-flop (FF) resources. Refactored code using a line buffer is shown in Figure 9.10. Note\\nthat for an NxN image filter, only N-1 lines need to be stored in line buffers.\\nThe line buffer and window buffer memories implemented from the code in Figure 9.10 are\\nshown in Figure 9.11. Each time through the loop, the window is shifted and filled with one pixel\\ncoming from the input and two pixels coming from the line buffer. Additionally, the input pixel\\nis shifted into the line buffer in preparation to repeat the process on the next line. Note that in\\norder to process one pixel each clock cycle, most elements of the window buffer must be read from\\nand'),\n",
       " Document(page_content='shifted into the line buffer in preparation to repeat the process on the next line. Note that in\\norder to process one pixel each clock cycle, most elements of the window buffer must be read from\\nand written to every clock cycle. In addition, after the ’i’ loop is unrolled, each array index to\\nthe window array is a constant. In this case, Vivado(cid:13)R HLS will convert each element of the array\\ninto a scalar variable (a process called scalarization). Most of the elements of the window array\\nwill be subsequently implemented as Flip Flops. Similarly, each row of the line buffer is accessed\\ntwice (being read once and written once). The code explicitly directs each row of the line buffer\\narray to be partitioned into a separate memory. For most interesting values of MAX WIDTH the\\nresulting memories will be implemented as one or more Block RAMs. Note that each Block RAM\\ncan support two independent accesses per clock cycle.\\nLine buffers are a special case of a more general concept known as a'),\n",
       " Document(page_content='will be implemented as one or more Block RAMs. Note that each Block RAM\\ncan support two independent accesses per clock cycle.\\nLine buffers are a special case of a more general concept known as a reuse buffer, which is\\noften used in stencil-style computations. High-level synthesis of reuse buffers and line buffers\\nfrom code like Figure 9.9 is an area of active research. See, for instance [8][31].\\nVivado(cid:13)R HLS includes hls::line buffer<> and hls::window buffer<> classes that simplify the\\nmanagement of window buffers and line buffers.\\nFor a 3x3 image filter, operating on 1920x1080 images with 4 bytes per pixel, How many\\nFPGA Block RAMs are necessary to store each video line?\\n9.2.2 Causal Filters\\nThe filter implemented in Figure 9.10 reads a single input pixel and produces a single output pixel\\neach clock cycle, however the behavior is not quite the same as the code in Figure 9.9. The output\\nis computed from the window of previously read pixels, which is ’up and to the left’ of the'),\n",
       " Document(page_content='pixel\\neach clock cycle, however the behavior is not quite the same as the code in Figure 9.9. The output\\nis computed from the window of previously read pixels, which is ’up and to the left’ of the pixel\\nbeing produced. As a result, the output image is shifted ’down and to the right’ relative to the\\ninput image. The situation is analogous to the concept of causal and non-causal filters in signal\\nprocessing. Most signal processing theory focuses on causal filters because only causal filters are\\npractical for time sampled signals (e.g. where x[n] = x(n*T) and y[n] = y(n*T)).\\nA causal filter h[n] is a filter where ∀k < 0,h[k] = 0. A finite filter h[n] which is not causal\\ncan be converted to a causal filter hˆ[n] by delaying the taps of the filter so that hˆ[n] = h[n−D].\\nTheoutputofthenewfilteryˆ= x⊗hˆ isthesameasadelayedoutputoftheoldfiltery = x⊗h.\\nSpecifically, yˆ[n] = y[n−D].\\n166VIDEO SYSTEMS\\nvoid video 2dfilter linebuffer(rgb pixel pixel in[MAX HEIGHT][MAX WIDTH],\\nrgb pixel pixel'),\n",
       " Document(page_content='x⊗hˆ isthesameasadelayedoutputoftheoldfiltery = x⊗h.\\nSpecifically, yˆ[n] = y[n−D].\\n166VIDEO SYSTEMS\\nvoid video 2dfilter linebuffer(rgb pixel pixel in[MAX HEIGHT][MAX WIDTH],\\nrgb pixel pixel out[MAX HEIGHT][MAX WIDTH]) {\\n#pragma HLS interface ap hs port=pixel out\\n#pragma HLS interface ap hs port=pixel in\\nrgb pixel window[3][3];\\nrgb pixel line buffer[2][MAX WIDTH];\\n#pragma HLS array partition variable=line buffer complete dim=1\\nrow loop: for (int row = 0; row < MAX HEIGHT; row++) {\\ncol loop: for (int col = 0; col < MAX WIDTH; col++) {\\n#pragma HLS pipeline\\nfor(int i = 0; i < 3; i++) {\\nwindow[i][0] = window[i][1];\\nwindow[i][1] = window[i][2];\\n}\\nwindow[0][2] = (line buffer[0][col]);\\nwindow[1][2] = (line buffer[0][col] = line buffer[1][col]);\\nwindow[2][2] = (line buffer[1][col] = pixel in[row][col]);\\nif (row == 0 || col == 0 ||\\nrow == (MAX HEIGHT − 1) ||\\ncol == (MAX WIDTH − 1)) {\\npixel out[row][col].R = 0;\\npixel out[row][col].G = 0;\\npixel out[row][col].B = 0;\\n} else {\\npixel out[row][col] ='),\n",
       " Document(page_content='(row == 0 || col == 0 ||\\nrow == (MAX HEIGHT − 1) ||\\ncol == (MAX WIDTH − 1)) {\\npixel out[row][col].R = 0;\\npixel out[row][col].G = 0;\\npixel out[row][col].B = 0;\\n} else {\\npixel out[row][col] = filter(window);\\n}\\n}\\n}\\n}\\nFigure 9.10: Code implementing a 2D filter with an explicit line buffer.\\n167VIDEO SYSTEMS\\n0 1 col-1colcol+1 width-1\\n0\\n1\\nline buffer\\n0 1 2\\n0\\npixel in[row][col]\\n1\\n2\\nwindow\\nloc\\nrow\\nFigure 9.11: Memories implemented by the code in Figure 9.10. These memories store a portion of\\nthe input image shown in the diagram on the right at the end of a particular iteration of the loop.\\nThe pixels outlined in black are stored in the line buffer and the pixels outlined in red are stored\\nin the window buffer.\\nProve the fact in the previous aside using the definition of the convolution for\\n∞\\n(cid:80)\\ny = x⊗h: y[n] = x[k]∗h[n−k]\\nk=−∞\\nFor the purposes of this book, most variables aren’t time-sampled signals and the times that\\nindividual inputs and outputs are created may be determined during the'),\n",
       " Document(page_content='= x⊗h: y[n] = x[k]∗h[n−k]\\nk=−∞\\nFor the purposes of this book, most variables aren’t time-sampled signals and the times that\\nindividual inputs and outputs are created may be determined during the synthesis process. For\\nsystems involving time-sampled signals, we treat timing constraints as a constraint during the HLS\\nimplementationprocess. Aslongastherequiredtasklatencyisachieved, thenthedesigniscorrect.\\nInmostvideoprocessingalgorithms,thespatialshiftintroducedinthecodeaboveisundesirable\\nand needs to be eliminated. Although there are many ways to write code that solves this problem,\\na common way is known as extending the iteration domain. In this technique, the loop bounds are\\nincreased by a small amount so that the first input pixel is read on the first loop iteration, but\\nthe first output pixel is not written until later in the iteration space. A modified version of the\\nfilter code is shown in Figure 9.12. The behavior of this code is shown in Figure 9.14, relative to\\nthe original'),\n",
       " Document(page_content='pixel is not written until later in the iteration space. A modified version of the\\nfilter code is shown in Figure 9.12. The behavior of this code is shown in Figure 9.14, relative to\\nthe original linebuffer code in Figure 9.10. After implementation with HLS, we see that the data\\ndependencies are satisfied in exactly the same way and that the implemented circuit is, in fact,\\nimplementable.\\n9.2.3 Boundary Conditions\\nIn most cases, the processing window contains a region of the input image. However, near the\\nboundary of the input image, the filter may extend beyond the boundary of the input image.\\nDependingontherequirementsofdifferentapplications,therearemanydifferentwaysofaccounting\\nfor the behavior of the filter near the boundary. Perhaps the simplest way to account for the\\nboundary condition is to compute a smaller output image that avoids requiring the values of input\\npixels outside of the input image. However, in applications where the output image size is fixed,\\nsuch as Digital'),\n",
       " Document(page_content='is to compute a smaller output image that avoids requiring the values of input\\npixels outside of the input image. However, in applications where the output image size is fixed,\\nsuch as Digital Television, this approach is generally unacceptable. In addition, if a sequence of\\n168VIDEO SYSTEMS\\nvoid video 2dfilter linebuffer extended(\\nrgb pixel pixel in[MAX HEIGHT][MAX WIDTH],\\nrgb pixel pixel out[MAX HEIGHT][MAX WIDTH]) {\\n#pragma HLS interface ap hs port=pixel out\\n#pragma HLS interface ap hs port=pixel in\\nrgb pixel window[3][3];\\nrgb pixel line buffer[2][MAX WIDTH];\\n#pragma HLS array partition variable=line buffer complete dim=1\\nrow loop: for(int row = 0; row < MAX HEIGHT+1; row++) {\\ncol loop: for(int col = 0; col < MAX WIDTH+1; col++) {\\n#pragma HLS pipeline II=1\\nrgb pixel pixel;\\nif(row < MAX HEIGHT && col < MAX WIDTH) {\\npixel = pixel in[row][col];\\n}\\nfor(int i = 0; i < 3; i++) {\\nwindow[i][0] = window[i][1];\\nwindow[i][1] = window[i][2];\\n}\\nif(col < MAX WIDTH) {\\nwindow[0][2] = (line'),\n",
       " Document(page_content='< MAX HEIGHT && col < MAX WIDTH) {\\npixel = pixel in[row][col];\\n}\\nfor(int i = 0; i < 3; i++) {\\nwindow[i][0] = window[i][1];\\nwindow[i][1] = window[i][2];\\n}\\nif(col < MAX WIDTH) {\\nwindow[0][2] = (line buffer[0][col]);\\nwindow[1][2] = (line buffer[0][col] = line buffer[1][col]);\\nwindow[2][2] = (line buffer[1][col] = pixel);\\n}\\nif(row >= 1 && col >= 1) {\\nint outrow = row−1;\\nint outcol = col−1;\\nif(outrow == 0 || outcol == 0 ||\\noutrow == (MAX HEIGHT−1) || outcol == (MAX WIDTH−1)) {\\npixel out[outrow][outcol].R = 0;\\npixel out[outrow][outcol].G = 0;\\npixel out[outrow][outcol].B = 0;\\n} else {\\npixel out[outrow][outcol] = filter(window);\\n}\\n}\\n}\\n}\\n}\\nFigure 9.12: Code implementing a 2D filter with an explicit line buffer. The iteration space is\\nextended by 1 to allow the filter to be implemented without a spatial shift.\\n169VIDEO SYSTEMS\\na) Original input b) Reference output c) Shifted output d) Corrected output\\nFigure9.13: Resultsofdifferentfilterimplementations. Thereferenceoutputinimagebisproduced\\nby'),\n",
       " Document(page_content='spatial shift.\\n169VIDEO SYSTEMS\\na) Original input b) Reference output c) Shifted output d) Corrected output\\nFigure9.13: Resultsofdifferentfilterimplementations. Thereferenceoutputinimagebisproduced\\nby the code in Figure 9.9. The shifted output in image c is produced by the code in Figure 9.10.\\nThe output in image d is produced by the code in Figure 9.12 and is identical to image b.\\nIn\\nOut\\nIn\\nOut\\nFigure 9.14: Timeline of execution of code implemented with line buffers. The top timeline shows\\nthe behavior of the code in Figure 9.10. The bottom timeline shows the behavior of the code in\\nFigure 9.12. The pixel marked in red is output on the same cycle in both implementations. In the\\nfirst case it is interpreted to be the second pixel of the second line and in the second case, it is\\ninterpreted as the first pixel of the first line.\\n170VIDEO SYSTEMS\\na) Original input b) Zero extension c) Constant extension d) Reflect extension\\nFigure 9.15: Examples of the effect of different kinds of'),\n",
       " Document(page_content='as the first pixel of the first line.\\n170VIDEO SYSTEMS\\na) Original input b) Zero extension c) Constant extension d) Reflect extension\\nFigure 9.15: Examples of the effect of different kinds of boundary conditions.\\nfilters is required, dealing with a large number images with slightly different sizes can be somewhat\\ncumbersome. The code in Figure 9.10 creates an output with the same size as the input by\\npadding the smaller output image with a known value (in this case, the color black). Alternatively,\\nthe missing values can be synthesized, typically in one of several ways.\\n• Missing input values can be filled with a constant\\n• Missing input values can be filled from the boundary pixel of the input image.\\n• Missing input values can be reconstructed by reflecting pixels from the interior of the input\\nimage.\\nOf course, more complicated and typically more computationally intensive schemes are also used.\\nOne way of writing code to handle boundary conditions is shown in Figure 9.16. This'),\n",
       " Document(page_content='of the input\\nimage.\\nOf course, more complicated and typically more computationally intensive schemes are also used.\\nOne way of writing code to handle boundary conditions is shown in Figure 9.16. This code\\ncomputes an offset address into the window buffer for each pixel in the window buffer. However,\\nthere is a significant disadvantage in this code since each read from the window buffer is at a\\nvariable address. This variable read results in multiplexers before the filter is computed. For an\\nN-by-N tap filter, there will be approximately N*N multiplexers with N inputs each. For simple\\nfilters, the cost of these multiplexers (and the logic required to compute the correct indexes) can\\ndominate the cost of computing the filter.\\nAn alternative technique is to handle the boundary condition when data is written into the\\nwindow buffer and to shift the window buffer in a regular pattern. In this case, there are only N\\nmultiplexers, instead of N*N, resulting in significantly lower resource'),\n",
       " Document(page_content='when data is written into the\\nwindow buffer and to shift the window buffer in a regular pattern. In this case, there are only N\\nmultiplexers, instead of N*N, resulting in significantly lower resource usage.\\nModify the code in Figure 9.16 to read from the window buffer using constant addresses.\\nHow many hardware resources did you save?\\n9.3 Conclusion\\nVideo processing is a common FPGA application and are highly amenable to HLS implementation.\\nA key aspect of most video processing algorithms is a high degree of data-locality enabling either\\nstreaming implementations or applications with local buffering and a minimum amount of external\\nmemory access.\\n171VIDEO SYSTEMS\\nvoid video 2dfilter linebuffer extended constant(\\nrgb pixel pixel in[MAX HEIGHT][MAX WIDTH], rgb pixel pixel out[MAX HEIGHT][MAX WIDTH]) {\\n#pragma HLS interface ap hs port=pixel out\\n#pragma HLS interface ap hs port=pixel in\\nrgb pixel window[3][3];\\nrgb pixel line buffer[2][MAX WIDTH];\\n#pragma HLS array partition variable=line'),\n",
       " Document(page_content='WIDTH]) {\\n#pragma HLS interface ap hs port=pixel out\\n#pragma HLS interface ap hs port=pixel in\\nrgb pixel window[3][3];\\nrgb pixel line buffer[2][MAX WIDTH];\\n#pragma HLS array partition variable=line buffer complete dim=1\\nrow loop: for(int row = 0; row < MAX HEIGHT+1; row++) {\\ncol loop: for(int col = 0; col < MAX WIDTH+1; col++) {\\n#pragma HLS pipeline II=1\\nrgb pixel pixel;\\nif(row < MAX HEIGHT && col < MAX WIDTH) {\\npixel = pixel in[row][col];\\n}\\nfor(int i = 0; i < 3; i++) {\\nwindow[i][0] = window[i][1];\\nwindow[i][1] = window[i][2];\\n}\\nif(col < MAX WIDTH) {\\nwindow[0][2] = (line buffer[0][col]);\\nwindow[1][2] = (line buffer[0][col] = line buffer[1][col]);\\nwindow[2][2] = (line buffer[1][col] = pixel);\\n}\\nif(row >= 1 && col >= 1) {\\nint outrow = row−1;\\nint outcol = col−1;\\nrgb pixel window2[3][3];\\nfor (int i = 0; i < 3; i++) {\\nfor (int j = 0; j < 3; j++) {\\nint wi, wj;\\nif (i < 1 − outrow) wi = 1 − outrow;\\nelse if (i >= MAX HEIGHT − outrow + 1) wi = MAX HEIGHT − outrow;\\nelse wi = i;\\nif (j < 1 −'),\n",
       " Document(page_content='(int i = 0; i < 3; i++) {\\nfor (int j = 0; j < 3; j++) {\\nint wi, wj;\\nif (i < 1 − outrow) wi = 1 − outrow;\\nelse if (i >= MAX HEIGHT − outrow + 1) wi = MAX HEIGHT − outrow;\\nelse wi = i;\\nif (j < 1 − outcol) wj = 1 − outcol;\\nelse if (j >= MAX WIDTH − outcol + 1) wj = MAX WIDTH − outcol;\\nelse wj = j;\\nwindow2[i][j] = window[wi][wj];\\n}\\n}\\npixel out[outrow][outcol] = filter(window2);\\n}\\n}\\n}\\n}\\nFigure 9.16: Code implementing a 2D filter with an explicit line buffer and constant extension to\\nhandle the boundary condition. Although correct, this code is relatively expensive to implement.\\n172Chapter 10\\nSorting Algorithms\\n10.1 Introduction\\nSorting is a common algorithm in many systems. It is a core algorithm for many data structures\\nbecause sorted data can be efficiently searched in O(logn) time using binary search. For example,\\ngiven an sequence of elements:\\nA = {1,4,17,23,31,45,74,76}'),\n",
       " Document(page_content='a core algorithm for many data structures\\nbecause sorted data can be efficiently searched in O(logn) time using binary search. For example,\\ngiven an sequence of elements:\\nA = {1,4,17,23,31,45,74,76} (10.1)\\nWecanfindwhetheranumberexistsinthissetofdatawithoutcomparingitagainstall8elements.\\nBecausethedataissortedwecanstartbypickinganelementinthemiddleofthearrayandchecking\\nto see whether our number is greater than or less than the element. For example, if we were looking\\ntofindtheelement45,wecouldstartbycomparingwithA(4) = 31. Since45 > 31wecaneliminate\\nA(0..4) from further comparison and only consider A(5..7).\\nA = {1,4,17,23,31,45,74,76} (10.2)\\nNext if we compare with A(6) = 74, we see that 45 < 74, so we can eliminate all but A(5) from\\nconsideration.\\nA = {1,4,17,23,31,45,74,76} (10.3)\\nOne additional comparison with A(5) allows us to determine that 45 is indeed contained in the\\nsequence.\\nInourexample,thesequenceAcouldrepresentavarietyofdifferentconcepts.'),\n",
       " Document(page_content='(10.3)\\nOne additional comparison with A(5) allows us to determine that 45 is indeed contained in the\\nsequence.\\nInourexample,thesequenceAcouldrepresentavarietyofdifferentconcepts. Acouldrepresent\\namathematicalsetandsearchingforanelementinthesequencecouldtelluswhetheravalueexists\\nin the set. A could also represent only a portion of the data, often called a key, which is useful\\nfor indexing the rest of the information. For instance, the key could be a person’s name. After\\nsearching based on the key then we know the position where the rest of the data about the person,\\nsuch as their birthdate, is stored. In yet other cases, the key could be something more abstract,\\nsuch as a cryptographic hash of some data or a key. In this case, the order that the data is stored\\nis likely to be randomized, but we can still find it if we know the right cryptographic hash to search\\nfor. In each case, the fundamental operations of required for sorting and searching are pretty much\\nthe same, primarily we need'),\n",
       " Document(page_content='we can still find it if we know the right cryptographic hash to search\\nfor. In each case, the fundamental operations of required for sorting and searching are pretty much\\nthe same, primarily we need to have the ability to compare two different values. For the remainder\\nof this chapter we will mostly ignore these differences.\\nThere are a wide variety of different sorting techniques that have been studied in processor\\nsystems[36]. These different algorithms vary in terms of their fundamental O() complexity, but\\nmany require O(nlogn) comparisons to sort n elements. Conceptually, this is intuitive since we\\n173SORTING ALGORITHMS\\ncould search for the right location to insert a new value in a sorted data set with O(logn) com-\\nparisons using binary search. To insert n elements this process would need to be repeated n times,\\nonce for each element.\\nIn practice, the cost of inserting an element can be significant, depending on the data structure\\nbeing sorted. In processor systems a variety of'),\n",
       " Document(page_content='need to be repeated n times,\\nonce for each element.\\nIn practice, the cost of inserting an element can be significant, depending on the data structure\\nbeing sorted. In processor systems a variety of factors influence overall performance, such as\\nmemory locality when processing large data sets or their ability to easily parallelize across multiple\\ncores. In HLS, we have similar considerations where it is common to trade off increased resource\\nusage for reduced processing time. In many cases, this might require a variety of algorithms and\\nimplementationtechniquesinordertoobtainthebestdesign. Thebesttechniquesformakingthese\\ntradeoffs are an area of active research[45, 54, 49].\\nCharacteristicsotherthanperformancealsoaffectthechoiceofsortingalgorithms. Forinstance,\\nwe might consider:\\n• Stability: A sort is a stable if when two items in the input data have the same key, then\\nthey will appear in the same order on the output. For example, we might sort a set of records\\ncontaining people’s names'),\n",
       " Document(page_content='A sort is a stable if when two items in the input data have the same key, then\\nthey will appear in the same order on the output. For example, we might sort a set of records\\ncontaining people’s names and ages using the ages as the sort key. In the input data, John\\nappears before Jane, and both are 25 years old. A stable sort will ensure that John and Jane\\nremain in the same order after sorting.\\n• Online: The algorithm allows for data to be sorted as it is received. This can be particularly\\nvaluable when data is not accessible when the sort starts or must be read in sequence from\\nexternal storage.\\n• In-place: A list with n elements can be sorted using n memory elements. Some algorithms\\nrequire additional storage during the sorting process.\\n• Adaptive: It is efficient for data that is already relatively sorted. For example, if data is\\nalready sorted, then some algorithms might run faster, e.g. in linear O(n) time.\\n10.2 Insertion Sort\\nInsertion sort is one of the basic sorting algorithms.'),\n",
       " Document(page_content='relatively sorted. For example, if data is\\nalready sorted, then some algorithms might run faster, e.g. in linear O(n) time.\\n10.2 Insertion Sort\\nInsertion sort is one of the basic sorting algorithms. It works by iteratively placing the items of an\\narray into sorted order and builds the sorted array one element at a time. Each iteration selects\\nan unsorted element and places it in the appropriate order within the previously sorted elements.\\nIt then moves onto the next element. This continues until all of the elements are considered, and\\nthe entire array is sorted.\\nTo make this more formal, assume that we are given an input array A that should be put into\\nsorted order. The base case is the first element of that array A[0], which by default is a sorted\\nsubarray (since it is only one element). The next step is to consider element A[1], and place it into\\nthe sorted subarray such that the new subarray (with two elements) is also sorted. We continue\\nthis process for each element A[i] until we'),\n",
       " Document(page_content='The next step is to consider element A[1], and place it into\\nthe sorted subarray such that the new subarray (with two elements) is also sorted. We continue\\nthis process for each element A[i] until we have iterated across all of the elements of A. At each\\nstep, we take the new element A[i] and insert it into the proper location such that the subarray\\nA[0..i−1] remains sorted. Figure 10.1 gives a step by step view of insertion sort operating on an\\narray. The first line is trivial. We consider only the first value 3 which makes a subarray with one\\nelement. Any subarray with one element is in sorted order. The second line places the second value\\n2 into the sorted subarray. The end result is that the value 2 is placed into the first element of\\nthe sorted subarray, which shifts the previous sorted element 3 to the right. The third line moves\\nthe third entry of the initial array into its appropriate place in the sorted subarray. In this case,\\nsince A[2] = 5, it is already in its correct'),\n",
       " Document(page_content='sorted element 3 to the right. The third line moves\\nthe third entry of the initial array into its appropriate place in the sorted subarray. In this case,\\nsince A[2] = 5, it is already in its correct location. Thus, nothing needs to happen. The fourth line\\n174SORTING ALGORITHMS\\n{3, 2, 5, 4, 1}\\n{3, 2, 5, 4, 1}\\n{2, 3, 5, 4, 1}\\n{2, 3, 5, 4, 1}\\n{2, 3, 4, 5, 1}\\n{1, 2, 3, 4, 5}\\nFigure 10.1: The Insertion Sort algorithm operating on an array. The initial array is shown at the\\ntop. In each step of the algorithm, the underlined algorithm is considered and placed into sorted\\norder of the elements to it’s left. At each stage, the shaded elements are in sorted order.\\n#include ”insertion sort.h”\\nvoid insertion sort(DTYPE A[SIZE]) {\\nL1:\\nfor(int i = 1; i < SIZE; i++) {\\nDTYPE item = A[i];\\nint j = i;\\nDTYPE t = A[j−1];\\nL2:\\nwhile(j > 0 && A[j−1] > item && j > 0) {\\n#pragma HLS pipeline II=1\\nA[j] = A[j−1];\\nj−−;\\n}\\nA[j] = item;\\n}\\n}\\nFigure 10.2: The complete code for insertion sort. The outer for loop iterates'),\n",
       " Document(page_content='t = A[j−1];\\nL2:\\nwhile(j > 0 && A[j−1] > item && j > 0) {\\n#pragma HLS pipeline II=1\\nA[j] = A[j−1];\\nj−−;\\n}\\nA[j] = item;\\n}\\n}\\nFigure 10.2: The complete code for insertion sort. The outer for loop iterates across the elements\\none at a time. The inner while loop moves the current element into sorted place.\\nconsiders the value 4. This is moved into its appropriate place, shifting 5 to the right. Finally, the\\nfifth line considers the placement of the value 1. This is placed into the first location of the array,\\nand all of the previous sorted values are shifted by one location.\\nInsertion sort is a stable, online, in-place, adaptive sorting algorithm. Because of these prop-\\nerties, insertion sort is often preferred when sorting small arrays or as a base case in a recursive\\nsorting algorithm. For example, more complex algorithms might decompose a large data set into a\\nnumber of smaller arrays and then these small arrays will be sorted using insertion sort. The result\\nis that formed by combining'),\n",
       " Document(page_content='more complex algorithms might decompose a large data set into a\\nnumber of smaller arrays and then these small arrays will be sorted using insertion sort. The result\\nis that formed by combining the sorted arrays.\\n10.2.1 Basic Insertion Sort Implementation\\nFigure10.2showsbasicCcodeforinsertionsort. Theouterloop, labeledL1, iteratesfromelements\\nA[1] to A[SIZE − 1] where SIZE denotes the number of elements in the array A. We do not need\\nto start at element A[0] since any one element is already in sorted order. Each iteration of the L1\\nloop starts by copying the current element that we wish to insert into the sorted subarray (i.e.,\\nA[i]) into the item variable and then executes the inner L2 loop. The inner loop walks down the\\n175SORTING ALGORITHMS\\nsorted portion of A[] looking for the appropriate location to place the value index. The inner loop\\nexecutes as long as it has not arrived at the end of the array (the condition j > 0) and the array\\nelements are greater than the item being'),\n",
       " Document(page_content='location to place the value index. The inner loop\\nexecutes as long as it has not arrived at the end of the array (the condition j > 0) and the array\\nelements are greater than the item being inserted (the condition A[j−1] > index). As long as the\\nloopconditionissatisfied,elementsofthesortedsubarrayareshiftedbyoneelement(thestatement\\nA[j] = A[j−1]). This will make room for the insertion of index when we eventually find its correct\\nlocation. When the loop exits, we have found the correct location for index and store it there. After\\nthe completion of iteration i, the elements from A[0] to A[i] are in sorted order.\\nThe code in Figure 10.2 is an straightforward implementation without any optimizations. We\\ncanoptimizeitusingdifferentVivado(cid:13)R HLSdirectives,suchaspipeline,unroll,andarray partition.\\nThe simplestoptimization wouldbe topipeline theinner loop, byapplying thepipelinedirective to\\nthe body of the inner loop. In this case, even though the inner loop is accessing different'),\n",
       " Document(page_content='partition.\\nThe simplestoptimization wouldbe topipeline theinner loop, byapplying thepipelinedirective to\\nthe body of the inner loop. In this case, even though the inner loop is accessing different elements\\nof A[] there are no data dependencies in these array accesses, so we could expect to achieve a loop\\nII of 1. The resulting accelerator would perform roughly N2/4 data comparisons[58] on average\\nand have a latency of roughly N2/4 cycles, since it performs one comparison per clock cycle. In\\nactuality, an accelerator generated from Vivado(cid:13)R HLS will have slightly higher latency to account\\nfor the sequential execution of the outer loop. In order to achieve higher performance, we could\\nalso attempt to move the pipeline directive to the outer L1 loop or to the function body itself. We\\ncould also combine these options with partial loop unrolling. Some of these options are shown in\\nTable ??.\\nTable 10.1: Possible options to optimize the basic insertion sort function in Figure 10.2'),\n",
       " Document(page_content='We\\ncould also combine these options with partial loop unrolling. Some of these options are shown in\\nTable ??.\\nTable 10.1: Possible options to optimize the basic insertion sort function in Figure 10.2 through\\ndirectives.\\nDirectives II Period Slices\\n1 L2: pipeline II=1 ? ? ?\\n2 L2: pipeline II=1 ? ? ?\\nL2: unroll factor=2\\narray partition variable=A cyclic factor=2\\n3 L1: pipeline II=1 ? ? ?\\n4 L1: pipeline II=1 ? ? ?\\nL1: unroll factor=2\\narray partition variable=A complete\\n5 function pipeline II=1 ? ? ?\\narray partition variable=A complete\\nExplore the options in Table 10.1. Synthesize each of these designs and determine the\\ninitiationinterval(II),clockperiod,andrequirednumberofslices. Whichoptionsaresuccessful\\nin improving latency and/or throughput? What would happen if you combined the directives\\nfrom multiple rows into one design?\\nUnfortunately, although this code seems similar to other nested loop programs we’ve looked\\nat previously, it does have some aspects that can make it difficult to'),\n",
       " Document(page_content='multiple rows into one design?\\nUnfortunately, although this code seems similar to other nested loop programs we’ve looked\\nat previously, it does have some aspects that can make it difficult to optimize. Even Option 1,\\nwhich simply attempts to pipeline the inner loop, can fail to achieve II=1. Although there are no\\nsignificant data recurrences, there is a recurrence in the control path that affects whether or not\\n176SORTING ALGORITHMS\\n#include ”insertion sort.h”\\nvoid insertion sort(DTYPE A[SIZE]) {\\nL1:\\nfor(i = 1; i < SIZE; i++) {\\nDTYPE item = A[i];\\nj = i;\\nDTYPE t = A[j−1];\\nL2:\\nwhile(j > 0 && t > item) {\\n#pragma HLS pipeline II=1\\nA[j] = t;\\nt = A[j−2];\\nj−−;\\n}\\nA[j] = item;\\n}\\n}\\nFigure 10.3: Refactored insertion sort for Option 1 in Table 10.1.\\nthe pipeline can execute. In this case, the code must read A[i−1] in order to determine whether the\\nloop should actually execute. Vivado(cid:13)R HLS incorporates this read into the loop pipeline, but if'),\n",
       " Document(page_content='pipeline can execute. In this case, the code must read A[i−1] in order to determine whether the\\nloop should actually execute. Vivado(cid:13)R HLS incorporates this read into the loop pipeline, but if the\\nreadofAispipelinedthentheloopexitcheckcannotbemadeinthefirststageofthepipeline. This\\nis an example of a recurrence which includes the HLS-generated control logic for the loop. If such\\nrecurrences exist, then Vivado(cid:13)R HLS will issue a message indicating that the loop exit condition\\ncannot be scheduled in the first II clock cycles. This situation can also occur a break or continue\\nstatement exists under a complex control condition. One solution is to explicitly speculate the read\\nof A[i−1] out of the loop. This enables the loop exit check to be scheduled with II=1 at the expense\\nof an additional array access on the last iteration. This code is shown in Figure 10.3.\\nOption 2 in the Table unrolls the inner loop by a factor of 2, attempting to perform two shift\\noperations every clock'),\n",
       " Document(page_content='additional array access on the last iteration. This code is shown in Figure 10.3.\\nOption 2 in the Table unrolls the inner loop by a factor of 2, attempting to perform two shift\\noperations every clock cycle. This could potentially reduce the latency to compute insertion sort\\nby a factor of two. Unfortunately, Vivado(cid:13)R HLS cannot achieve a loop II of 1 for this code using\\narray partitioning, because each array access cannot be assigned to a different memory partition.\\nIn Vivado(cid:13)R HLS, the array partition directive results in implementing a number of com-\\npletely separate memories. For instance, array partition variable=A cyclic factor=4 would re-\\nsult in generating four separate memories from array A[], each of which contains a portion of\\nthe array contents. We can often think that this optimization provides four times the number\\nof memory accesses each clock, but this is only the case if each memory access can be assigned\\nto exactly one of the memory partitions. For'),\n",
       " Document(page_content='think that this optimization provides four times the number\\nof memory accesses each clock, but this is only the case if each memory access can be assigned\\nto exactly one of the memory partitions. For example an array access A[i] for unknown i could\\nreference data stored in any partition, while an array access A[4∗i+2] would only access data in\\nthe third partition. More complex logic, often called memory banking, can resolve a number of\\nindependent accesses A[i], A[i+1], A[i+6], A[i+7] and perform these accesses in the same clock\\ncycle. Memory banking requires additional crossbar logic to route these simultaneous accesses\\ninacircuit,sinceicanbearbitrary. Atcompiletime,wecanguaranteethattheconstantoffsets\\n177SORTING ALGORITHMS\\nof these accesses will hit in different banks, but the actual banks cannot be determined until i\\nis known. Yet more complex logic could implement stalling logic, enabling a set of unrelated\\naccesses to complete in a single clock cycle if they hit in different'),\n",
       " Document(page_content='banks cannot be determined until i\\nis known. Yet more complex logic could implement stalling logic, enabling a set of unrelated\\naccesses to complete in a single clock cycle if they hit in different banks. If the accesses happen\\nto all hit in the same bank, then the stalling logic can delay the progress of the circuit for a\\nnumber of clocks until all of the accesses have completed. Lastly, multiport architectures have\\ndesigned that can allow a number ofaccesses guaranteed completion every clock cycle[62, 1,37]\\nby replicating data across normal memories with one or two physical ports.\\nOption 3 in the table also fails to achieve significant improvement. Since the inner L2 loop\\ndoes not have a statically computable loop bound Vivado(cid:13)R HLS is unable to construct a pipeline\\nfrom the body of the L1 loop. Unfortunately, this is a case where exploring the design space of\\ninteresting alternatives requires code restructuring in addition to the use of directives. Finding the\\nbest code'),\n",
       " Document(page_content='body of the L1 loop. Unfortunately, this is a case where exploring the design space of\\ninteresting alternatives requires code restructuring in addition to the use of directives. Finding the\\nbest code restructuring requires not only understanding the algorithm, but also having a sense of\\nthe architecture that will be generated by the HLS process[28, 48]. For example, we discuss one\\nsuch code restructuring for insertion sort in the following section. You will see that that code is\\nsignificantly different from the code in Figure 10.2.\\nIn the following, we attempt to demonstrate several concepts. First, writing efficient high-\\nlevel synthesis code requires that the designer must understand hardware concepts like unrolling\\nand partitioning. Second, the designer must be able to diagnose any throughput problems, which\\nrequiressubstantialknowledgeaboutboththeapplicationandthehardwareimplementationofthat\\ndesign. Third, andmostimportantly, inordertoachievethebestresults, i.e.'),\n",
       " Document(page_content='to diagnose any throughput problems, which\\nrequiressubstantialknowledgeaboutboththeapplicationandthehardwareimplementationofthat\\ndesign. Third, andmostimportantly, inordertoachievethebestresults, i.e. highperformanceand\\nlow-area, it is often required to rewrite the code in a manner that will create an efficient hardware\\narchitecture. This can be very different from code that results in the most efficient software.\\n10.2.2 Parallelising Insertion Sort\\nIn order significantly increase the performance of insertion sort, we’d like to get to the point where\\nwe can insert a new element every clock cycle. When inserting the last element into the sorted list,\\nthis might require shifting all of the elements in the array. For the code in Figure 10.2, this means\\nthat the inner while loop could actually execute over all of the elements in the array. Intuitively, we\\nrealize that inserting a new element into the sorted list every clock cycle requires enough hardware\\noperators to perform a comparison'),\n",
       " Document(page_content='execute over all of the elements in the array. Intuitively, we\\nrealize that inserting a new element into the sorted list every clock cycle requires enough hardware\\noperators to perform a comparison on every element of the array in a single clock cycle. To enable\\npipelining of the outer loop, we can convert the inner L2 loop with variable loop bounds into a\\nfixed-bound loop, enabling it to be unrolled and integrated into the L1 loop pipeline. Code that\\ndoes this is shown in Figure 10.4.\\nThis code contains the exit condition of the original loop (L2 in Figure 10.2) as an if condition\\nin the body of the new L2 loop. The other branches of the conditional are added to handle the\\nexpanded iteration space of the loop, essentially performing no operations when the original loop\\nwould not be executing. In addition, the loop now contains the final assignment of item in the\\narray, rather than performing the assignment outside of the loop. When the inner loop is unrolled,\\nremember that j will'),\n",
       " Document(page_content='In addition, the loop now contains the final assignment of item in the\\narray, rather than performing the assignment outside of the loop. When the inner loop is unrolled,\\nremember that j will become a constant in all of the unrolled instances of the loop. As a result,\\neach read and write from B[] will be performed at a constant index and comparisons between j and\\na constant will be completely optimized away. The item variable, on the other hand, is possibly\\nassigned in every copy of the inner loop. During compilation, Vivado(cid:13)R HLS creates a separate\\ncopy of this variable for each separate possible assignment and each possible assignment results in\\nmultiplexer in the implemented circuit.\\n178SORTING ALGORITHMS\\n#include ”insertion sort parallel.h”\\n#include ”assert.h”\\nvoid insertion sort parallel(DTYPE A[SIZE], DTYPE B[SIZE]) {\\n#pragma HLS array partition variable=B complete\\nL1:\\nfor(int i = 0; i < SIZE; i++) {\\n#pragma HLS pipeline II=1\\nDTYPE item = A[i];\\nL2:\\nfor(int j = SIZE−1; j >='),\n",
       " Document(page_content='parallel(DTYPE A[SIZE], DTYPE B[SIZE]) {\\n#pragma HLS array partition variable=B complete\\nL1:\\nfor(int i = 0; i < SIZE; i++) {\\n#pragma HLS pipeline II=1\\nDTYPE item = A[i];\\nL2:\\nfor(int j = SIZE−1; j >= 0; j−−) {\\nDTYPE t;\\nif(j > i) {\\nt = B[j];\\n} else if(j > 0 && B[j−1] > item) {\\nt = B[j−1];\\n} else {\\nt = item;\\nif (j > 0)\\nitem = B[j−1];\\n}\\nB[j] = t;\\n}\\n}\\n}\\nFigure 10.4: Refactored insertion sort for Option 3 in Table 10.1.\\n179SORTING ALGORITHMS\\nThe conversion of a single variable into multiple versions is a common internal transforma-\\ntion used by compilers. The resulting internal representation is called static single assignment\\n(SSA). To merge values coming from different points in the code, the SSA internal represen-\\ntation include artificial ‘phi-functions’ represented by the greek letter φ. These phi-functions\\noften result in multiplexers in the circuit generated by Vivado(cid:13)R HLS and you’ll probably find\\nthe related resources in the tool reports if you look carefully.\\nThe'),\n",
       " Document(page_content='letter φ. These phi-functions\\noften result in multiplexers in the circuit generated by Vivado(cid:13)R HLS and you’ll probably find\\nthe related resources in the tool reports if you look carefully.\\nThe parallelized insertion sort in Figure 10.4 essentially results in a number of copies of the\\nbody of the inner loop. The contents of this inner loop consists of a few multiplexers, a comparator\\nto determine the smallest of two elements, and a register to store an element of B[]. Incidentally,\\neach stage might also include pipeline registers, if needed, to ensure that the resulting circuit runs\\nat an effective clock frequency. We’ll call the contents of the inner loop a sorting cell. The whole\\ninsertion sort function consists of a one-dimensional array of sorting cells, combined with a small\\namountofadditionallogictofeeddataintheinputandcapturetheoutputattherighttime, inthis\\ncase after SIZE elements have been processed by the outer loop. This array of sorting cells has an\\ninteresting'),\n",
       " Document(page_content='inthis\\ncase after SIZE elements have been processed by the outer loop. This array of sorting cells has an\\ninteresting property where each sorting cell only communicates with it’s neighboring sorting cells,\\nrather than with all cells. Designs like this are called systolic arrays and are a common technique\\nfor parallelizing algorithms. In many cases, including sorting, systolic array implementations can\\nnaturally arise when we unroll inner loops, as long as the communication between different loop\\niterations is limited. We call this type of design an implicit systolic array.\\n10.2.3 Explicit Systolic Array For Insertion Sort\\nSystolic arrays have been well researched and many parallel algorithms are published as systolic\\narrays. In particular, the idea of using a linear array of sorting cells to implement insertion sort\\nis well understood[55, 9, 45, 5]. However, rather than being described as an unrolled loop, systolic\\narraysareoftendescribedascomponentscommunicatingbystreamsofdata.'),\n",
       " Document(page_content='cells to implement insertion sort\\nis well understood[55, 9, 45, 5]. However, rather than being described as an unrolled loop, systolic\\narraysareoftendescribedascomponentscommunicatingbystreamsofdata. Thissectiondescribes\\nan alternative coding style based on explicit streams and the dataflow directive that can be a more\\nintuitive way to describe a systolic array.\\nFigure 10.5 shows a systolic array implementing insertion sort. Each cell is identical and com-\\nparesitsinput(in)withthevalueincurrentregisterlocal. Thesmallervalueispassedtotheoutput\\nout, while the larger value is stored back in local. In other words, out = min(in,local). The output\\nof cell i is passed as input to the next cell i+1 in the linear array. As a new input arrives it will\\nbe compared against elements stored in the array until it finds it’s correct place. If a new input is\\nlarger than all of the values in the array, then the sorted values will shift one cell to the right. If a\\nnew input is smaller than all of the'),\n",
       " Document(page_content='until it finds it’s correct place. If a new input is\\nlarger than all of the values in the array, then the sorted values will shift one cell to the right. If a\\nnew input is smaller than all of the values in the array, then it will propagate through the array,\\nand eventually become stored in the furthest right cell. After all the data has moved through the\\narray, the smallest data element will be sorted in cell N −1 and can be read from the output.\\nThe code for one insertion cell is shown in Figure 10.6. The code uses a streaming interface\\nby declaring the input and output variables as an hls::stream type. DTYPE is a type parameter\\nenabling different types to be operated on. The local variable stores one element of the array being\\nsorted. It is static because we want to preserve its value across multiple function calls. This does\\ncreate a problem since we must replicate this cell function N times. Using the same function (e.g.,\\ncalling the same cell function N times) would cause a'),\n",
       " Document(page_content='across multiple function calls. This does\\ncreate a problem since we must replicate this cell function N times. Using the same function (e.g.,\\ncalling the same cell function N times) would cause a problem since each cell must have a separate\\nstatic variable. One static variable cannot be shared across N functions.\\nLinking the insertion cells together is straightforward. The function insertion cell sort in Figure\\n10.7 shows the code for sorting eight elements. Expanding this to a larger number of elements\\n180SORTING ALGORITHMS\\nUnsorted Sorted\\ncell 0 ... cell i ... cell N-1\\nOutput Output\\nComparison\\nLogic\\nlocal\\nout\\nin\\nFigure 10.5: The architecture of one insertion cell. Each cell holds exactly one value in the register\\nlocal. On each execution it receives an input value in, compares that to its local value, and writes\\nthe smaller of the two to output. Sorting N values requires N cells.\\nvoid cell0(hls::stream<DTYPE> & in, hls::stream<DTYPE> & out)\\n{\\nstatic DTYPE local = 0;\\nDTYPE in copy ='),\n",
       " Document(page_content='its local value, and writes\\nthe smaller of the two to output. Sorting N values requires N cells.\\nvoid cell0(hls::stream<DTYPE> & in, hls::stream<DTYPE> & out)\\n{\\nstatic DTYPE local = 0;\\nDTYPE in copy = in.read();\\nif(in copy > local) {\\nout.write(local);\\nlocal = in copy;\\n}\\nelse\\n{\\nout.write(in copy);\\n}\\n}\\nFigure10.6: TheVivado(cid:13)R HLSCcodecorrespondingtooneinsertioncellcell0. Theothercellshave\\nthe exact same code except with a different function name (cell1, cell2, etc.). The code performs\\nthe same functionality as shown in the architectural diagram in Figure 10.5. It uses the hls:stream\\ninterface for the input and output variables. hls::stream provides a convenient method to create\\nFIFOs that work both for synthesis and simulation.\\n181SORTING ALGORITHMS\\nvoid insertion cell sort(hls::stream<DTYPE> & in, hls::stream<DTYPE> & out)\\n{\\n#pragma HLS DATAFLOW\\nhls::stream<DTYPE> out0(”out0 stream”);\\nhls::stream<DTYPE> out1(”out1 stream”);\\nhls::stream<DTYPE> out2(”out2'),\n",
       " Document(page_content='cell sort(hls::stream<DTYPE> & in, hls::stream<DTYPE> & out)\\n{\\n#pragma HLS DATAFLOW\\nhls::stream<DTYPE> out0(”out0 stream”);\\nhls::stream<DTYPE> out1(”out1 stream”);\\nhls::stream<DTYPE> out2(”out2 stream”);\\nhls::stream<DTYPE> out3(”out3 stream”);\\nhls::stream<DTYPE> out4(”out4 stream”);\\nhls::stream<DTYPE> out5(”out5 stream”);\\nhls::stream<DTYPE> out6(”out6 stream”);\\n// Function calls;\\ncell0(in, out0);\\ncell1(out0, out1);\\ncell2(out1, out2);\\ncell3(out2, out3);\\ncell4(out3, out4);\\ncell5(out4, out5);\\ncell6(out5, out6);\\ncell7(out6, out);\\n}\\nFigure 10.7: Insertion cell sorting for eight elements. The function takes an input hls::stream and\\noutputs the elements in sorted order one at a time through the variable out. The order starts with\\nthe smallest element first, and then continues on to increasingly larger elements.\\nsimply requires replicating more cell functions, additional hls::stream variables, instantiating these\\nfunctions, and connecting their input and output function arguments in the'),\n",
       " Document(page_content='larger elements.\\nsimply requires replicating more cell functions, additional hls::stream variables, instantiating these\\nfunctions, and connecting their input and output function arguments in the appropriate manner.\\nTheexistingimplementationofinsertion cell sortoutputsthedatastartingwiththesmallest\\nelement and continues outputting increasingly larger elements. What changes are required in\\norder to reverse this order, i.e., to output the largest element first, and then the decreasingly\\nsmaller outputs?\\nThe insertion cell sort function must be called multiple times in order to sort the entire data.\\nEachcalltoinsertion cell sortprovidesonedataelementtothearrayforsorting. Thefirstcallplaces\\ntheinputdatainitsappropriateplace. Wherewouldthisdatabeplaced? Toanswerthatquestion,\\nwe should point out that we make an assumption on the input data since we are initializing the\\nlocal variable to 0. This is done in all of the cell functions.\\nInitializing the static variable local to 0 makes an'),\n",
       " Document(page_content='out that we make an assumption on the input data since we are initializing the\\nlocal variable to 0. This is done in all of the cell functions.\\nInitializing the static variable local to 0 makes an assumption on the type of data that\\nwill be provided for sorting. What is this assumption? In other words, what is the range of\\nvalues for the input data that will be properly handled? What would happen if input data\\nfrom outside of this range was given to the insertion cell sort function? Is there a better way to\\ninitialize the local variable?\\n182SORTING ALGORITHMS\\nAfter making eight calls to the insertion cell sort function, all of the data will be put into the\\neight local variables in each of the cell functions.\\nHow many times must we call the insertion cell sort function in order to get the first sorted\\nelement? How many calls are necessary in order to all of the sorted data? What if we in-\\ncreased the array to N cells? Can you generalize these two functions (number of times to\\ncall'),\n",
       " Document(page_content='the first sorted\\nelement? How many calls are necessary in order to all of the sorted data? What if we in-\\ncreased the array to N cells? Can you generalize these two functions (number of times to\\ncall insertion cell sort to load N elements, and the number of calls required to output all N\\nelements)?\\nTo achieve a task-pipelined architecture consisting of the eight cell functions, the code specifies\\nthe dataflow directive. Each execution of the toplevel function processes a new input sample and\\ninserts it into the sorted sequence. The actual insertion is pipelined, with one pipeline stage for\\neach call to the cell function. With eight calls to the cell function, we can only sort sequences with\\neight values, but this can be extended almost arbitrarily since the code contains no recurrences\\nbetween the cells.\\nHow many cycles are required to sort an entire array? We define the sorting to be com-\\npleted when all of the sorted data is output from the array of cell function inside of'),\n",
       " Document(page_content='the cells.\\nHow many cycles are required to sort an entire array? We define the sorting to be com-\\npleted when all of the sorted data is output from the array of cell function inside of the\\ninsertion cell sort, i.e., all eight elements have been output from argument out in the function\\ninsertion cell sort? How does the cycle count change if we remove the dataflow directive? How\\ndoes change the resource utilization?\\nFigure10.8showsthecodeforthetestbench. Thetestbenchgeneratesrandomdatatobesorted\\nin input[]. This array is sorted by calling the insertion cell sort() function multiple times, with the\\nresult appearing in cell output[]. Next, the same data is sorted in place using the insertion sort\\nfunction from Figure 10.2. Finally, the testbench compares the results of these two sort implemen-\\ntations. The testbench passes if the sorted order from both implementations is the same.\\nIn the testbench, the SIZE constant is set to the number of elements being sorted. This would\\nbe 8 in the'),\n",
       " Document(page_content='The testbench passes if the sorted order from both implementations is the same.\\nIn the testbench, the SIZE constant is set to the number of elements being sorted. This would\\nbe 8 in the running example throughout this chapter. The DEBUG constant is used to provided\\noutput detailing the execution of the testbench. This should be set to a non-zero value if you wish\\nto see the debugging data, and 0 if you want the output to be quiet. The input data is randomly\\ngenerated using the rand() function and stored in the input[] array. You can change the data by\\nmodifying the argument in the call to srand(). The argument sets the seed for the random number\\ngenerator to a specific value to ensure the generation of the same sequence of random numbers on\\neach execution. Changing this value to a different integer will result in a different, but predictable,\\nrandom sequence.\\nNote again that the testbench calls the insertion cell sort() function a total of SIZE∗2 times.\\nEach of the first SIZE function'),\n",
       " Document(page_content='integer will result in a different, but predictable,\\nrandom sequence.\\nNote again that the testbench calls the insertion cell sort() function a total of SIZE∗2 times.\\nEach of the first SIZE function calls feeds a single input element into function, but produce no\\nuseful output. The next SIZE calls provide dummy input data and produce one sorted element\\neach function call. The data is produced one at a time starting with the smallest element.\\nAfter executing a sort, the code in Figure 10.6 leaves the local variable in a different state\\nthan when it started. Unfortunately, this means that we can only sort one array! In most\\ncases, we not only want to sort more than one array, but we’d like to process each array back\\n183SORTING ALGORITHMS\\n#include ”insertion cell sort.h”\\n#include <iostream>\\n#include <stdlib.h>\\nconst static int DEBUG=1;\\nconst static int MAX NUMBER=1000;\\nint main () {\\nint fail = 0;\\nDTYPE input[SIZE];\\nDTYPE cell output[SIZE] = {0};\\nhls::stream<DTYPE> in, out;\\n//generate'),\n",
       " Document(page_content='<stdlib.h>\\nconst static int DEBUG=1;\\nconst static int MAX NUMBER=1000;\\nint main () {\\nint fail = 0;\\nDTYPE input[SIZE];\\nDTYPE cell output[SIZE] = {0};\\nhls::stream<DTYPE> in, out;\\n//generate random data to sort\\nif(DEBUG) std::cout << ”Random Input Data\\\\n”;\\nsrand(20); //change me if you want different numbers\\nfor(int i = 0; i < SIZE; i++) {\\ninput[i] = rand() % MAX NUMBER + 1;\\nif(DEBUG) std::cout << input[i] << ”\\\\t”;\\n}\\n//process the data through the insertion cell sort function\\nfor(int i = 0; i < SIZE∗2; i++) {\\nif(i < SIZE) {\\n//feed in the SIZE elements to be sorted\\nin.write(input[i]);\\ninsertion cell sort(in, out);\\ncell output[i] = out.read();\\n} else {\\n//then send in dummy data to flush pipeline\\nin.write(MAX NUMBER);\\ninsertion cell sort(in, out);\\ncell output[i−SIZE] = out.read();\\n}\\n}\\n//sort the data using the insertion sort function\\ninsertion sort(input);\\n//compare the results of insertion sort to insertion cell sort; fail if they differ\\nif(DEBUG) std::cout << ”\\\\nSorted Output\\\\n”;\\nfor(int i'),\n",
       " Document(page_content='data using the insertion sort function\\ninsertion sort(input);\\n//compare the results of insertion sort to insertion cell sort; fail if they differ\\nif(DEBUG) std::cout << ”\\\\nSorted Output\\\\n”;\\nfor(int i = 0; i < SIZE; i++) {\\nif(DEBUG) std::cout << cell output[i] << ”\\\\t”;\\n}\\nfor(int i = 0; i < SIZE; i++) {\\nif(input[i] != cell output[i]) {\\nfail = 1;\\nstd::cout << ”golden=” << input[i] << ”hw=” << cell output[i] << ”\\\\n”;\\n}\\n}\\nif(fail == 0) std::cout << ”PASS\\\\n”;\\nelse std::cout << ”FAIL\\\\n”;\\nreturn fail;\\n}\\nFigure 10.8: The testbench for the insertion cell sort function.\\n184SORTING ALGORITHMS\\nwidth = 1, A[] = {3,7,6,4,5,8,2,1}\\nwidth = 2, A[] = {3,7,4,6,5,8,1,2}\\nwidth = 4, A[] = {3,4,7,6,1,2,5,8}\\nwidth = 8, A[] = {1,2,3,4,5,6,7,8}\\nFigure 10.9: The Merge Sort algorithm operating on an array. The initial state where each element\\nis considered to be a sorted subarray of length one is shown at the top. At each step of the\\nalgorithm, subarrays are merged placing the shaded elements are in sorted'),\n",
       " Document(page_content='initial state where each element\\nis considered to be a sorted subarray of length one is shown at the top. At each step of the\\nalgorithm, subarrays are merged placing the shaded elements are in sorted order.\\nin1[] = {3, 4, 6, 7} in2[] = {1, 2, 5, 8} out[] = { }\\nin1[] = {3, 4, 6, 7} in2[] = {1, 2, 5, 8} out[] = {1}\\nin1[] = {3, 4, 6, 7} in2[] = {1, 2, 5, 8} out[] = {1, 2}\\nin1[] = {3, 4, 6, 7} in2[] = {1, 2, 5, 8} out[] = {1, 2, 3}\\nin1[] = {3, 4, 6, 7} in2[] = {1, 2, 5, 8} out[] = {1, 2, 3, 4}\\nin1[] = {3, 4, 6, 7} in2[] = {1, 2, 5, 8} out[] = {1, 2, 3, 4, 5}\\nin1[] = {3, 4, 6, 7} in2[] = {1, 2, 5, 8} out[] = {1, 2, 3, 4, 5, 6}\\nin1[] = {3, 4, 6, 7} in2[] = {1, 2, 5, 8} out[] = {1, 2, 3, 4, 5, 6, 7}\\nin1[] = {3, 4, 6, 7} in2[] = {1, 2, 5, 8} out[] = {1, 2, 3, 4, 5, 6, 7, 8}\\nFigure 10.10: The process of merging two sorted arrays. The initial state is shown at the top. In\\neach step of the algorithm, the underlined elements are considered and one is placed into sorted\\norder in the output'),\n",
       " Document(page_content='The process of merging two sorted arrays. The initial state is shown at the top. In\\neach step of the algorithm, the underlined elements are considered and one is placed into sorted\\norder in the output array.\\nto back with no bubbles in the pipeline. Modify the code and the testbench to enable this and\\ndemonstrate that your modified code can sort multiple arrays.\\n10.3 Merge Sort\\nMerge sort is a stable, divide and conquer algorithm invented by John von Neumann in 1945 [36].\\nThe basic idea of the merge sort algorithm is that combining two sorted arrays into a larger sorted\\narray is a relatively simple operation, which can be completed in O(n) time. Conceptually, we\\ndivide the array into two subarrays, sort each subarray, and then combine the sorted subarrays into\\nthe final result.\\nSince we focus on sorting arrays of data, it turns out that the ‘divide’ operation is actually\\ntrivial. It requires no arithmetic or data movement and we can simply consider each individual\\nelement of the input'),\n",
       " Document(page_content='on sorting arrays of data, it turns out that the ‘divide’ operation is actually\\ntrivial. It requires no arithmetic or data movement and we can simply consider each individual\\nelement of the input array as a trivially sorted subarray. All of the computation is involved with\\nmerging subarrays into larger sorted subarrays. With other data representations, such as a linked-\\nlist, dividing an input array into subarrays can require traversing the data structure. The merge\\nsort process is shown in Figure 10.9.\\nThe process of combining two sorted arrays into one larger sorted array is sometimes called the\\n“two finger algorithm”. Figure 10.10 describes the process using two sorted input arrays, named\\nin1[] and in2[]. These are merged into a sorted output array, named out[].\\nThe process starts with a “finger” pointing to the first element of each array. This conceptual\\nfinger is just an index into the array storing the data. As the algorithm proceeds, the fingers will\\npoint to different elements'),\n",
       " Document(page_content='a “finger” pointing to the first element of each array. This conceptual\\nfinger is just an index into the array storing the data. As the algorithm proceeds, the fingers will\\npoint to different elements of the arrays. We underline these elements in order to show where the\\n185SORTING ALGORITHMS\\nfingers are placed. To begin with, the fingers point to the first element of each array, elements 3\\nand 1 in arrays in1[] and in2[], respectively.\\nThe first line of Figure 10.10 shows the initial state. There are four elements in each of the two\\ninput arrays and zero elements in the output array. We compare the first two elements of the input\\narrays, and move the smaller of these two to the output array. In this case, we compare 3 and 1,\\nand move 1 into out[]. This reduces the number of elements in in2[], and our “finger” moves to the\\nnext element in in2[] which is the next smallest element since the array is sorted. Once again, we\\ncompare the two elements from each of the input arrays, and move'),\n",
       " Document(page_content='in2[], and our “finger” moves to the\\nnext element in in2[] which is the next smallest element since the array is sorted. Once again, we\\ncompare the two elements from each of the input arrays, and move the smaller of the two elements\\nto out[]. In this case, we compare 3 and 2, and move the element from in2[] to out[]. This process\\ncontinuesuntilalloftheelementsinoneofthearraysisempty. Inthatcase, wecopytheremaining\\nelements from the non-empty array into the output array.\\nAlthough the merge sort algorithm is a common textbook example of recursive function calls,\\nmost high-level synthesis tools do not support recursion or only support it in a limited manner.\\nThus, we will focus on a non-recursive implementation of the algorithm. The code might look\\nsubstantially different from what you are used to, but the core of the algorithm is exactly the same.\\n10.3.1 Basic Merge Sort\\nFigure 10.11 shows basic code implementing a non-recursive merge sort. This code sorts an array\\nby performing roughly'),\n",
       " Document(page_content='to, but the core of the algorithm is exactly the same.\\n10.3.1 Basic Merge Sort\\nFigure 10.11 shows basic code implementing a non-recursive merge sort. This code sorts an array\\nby performing roughly N logN comparisons, but requires a temporary array to store the partially\\nsorted data. The code starts by considering each element of the array as a length one sorted\\nsubarray. Each iteration of the outer loop merges pairs of sorted subarrays into larger sorted\\nsubarrays. After the first iteration we have sorted subarrays with maximum size 2, after the second\\niteration the sorted subarrays have maximum size 4, then 8 and so on. Note that if the size of the\\ninput array is not a power of two, then it is possible that we might end up with some subarrays\\nwhich are smaller than the maximum size.\\nThe sorting process starts in the merge sort() function. The function primarily operates on\\nthe input A[] array, but leverages internal storage in temp[]. The size of both of these arrays is\\ndetermined by'),\n",
       " Document(page_content='sorting process starts in the merge sort() function. The function primarily operates on\\nthe input A[] array, but leverages internal storage in temp[]. The size of both of these arrays is\\ndetermined by the SIZE parameter. The parameter DTYPE determines the type of data being\\nsorted.\\nThe computation of the function consists of two nested for loops. The outer stage loop keeps\\ntrack of the number of elements in each sorted subarray in the width variable. The function starts\\nbyconsideringeachelementasaseparatesubarray; hencewidthisinitializedas1. Everyiterationof\\nthe stage loop results in the generation of longer sorted subarrays. These new subarrays potentially\\nhave twice as many elements, which is why width doubles on each iteration of the stage loop. The\\nstage loop terminates when width is greater than or equal to SIZE, indicating that all elements of\\nA[] are in a single sorted subarray.\\nEach iteration of the inner for loop, labeled merge arrays merges two consecutive subarrays.\\nThese'),\n",
       " Document(page_content='greater than or equal to SIZE, indicating that all elements of\\nA[] are in a single sorted subarray.\\nEach iteration of the inner for loop, labeled merge arrays merges two consecutive subarrays.\\nThese subarrays each consist of up to width elements, starting at index i1 and i2. These two\\nsubarrays are merged and copied into a single subarray stored in temp[] using the merge() function.\\nThe main complexity here is dealing with the boundary condition at the end of the loop if SIZE is\\nnotexactlyapoweroftwo. Inthiscasethesubarraysmightcontainlessthanwidthelements. After\\nmerging subarrays into temp[], the final loop copies the data back into A[] for the next iteration of\\nthe loop.\\nThecodeinFigureattemptstohandleawidevarietyofvaluesfortheparameterSIZE. What\\nvalues are allowed? When the merge() function is called, whare are the possible relationships\\n186SORTING ALGORITHMS\\n#include ”merge sort.h”\\n#include ”assert.h”\\n// subarray1 is in[i1..i2−1], subarray2 is in[i2..i3−1], result is in'),\n",
       " Document(page_content='merge() function is called, whare are the possible relationships\\n186SORTING ALGORITHMS\\n#include ”merge sort.h”\\n#include ”assert.h”\\n// subarray1 is in[i1..i2−1], subarray2 is in[i2..i3−1], result is in out[i1..i3−1]\\nvoid merge(DTYPE in[SIZE], int i1, int i2, int i3, DTYPE out[SIZE]) {\\nint f1 = i1, f2 = i2;\\n// Foreach element that needs to be sorted...\\nfor(int index = i1; index < i3; index++) {\\n// Select the smallest available element.\\nif((f1 < i2 && in[f1] <= in[f2]) || f2 == i3) {\\nout[index] = in[f1];\\nf1++;\\n} else {\\nassert(f2 < i3);\\nout[index] = in[f2];\\nf2++;\\n}\\n}\\n}\\nvoid merge sort(DTYPE A[SIZE]) {\\nDTYPE temp[SIZE];\\n// Each time through the loop, we try to merge sorted subarrays of width elements\\n// into a sorted subarray of 2∗width elements.\\nstage:\\nfor (int width = 1; width < SIZE; width = 2 ∗ width) {\\nmerge arrays:\\nfor (int i1 = 0; i1 < SIZE; i1 = i1 + 2 ∗ width) {\\n// Try to merge two sorted subarrays:\\n// A[i1..i1+width−1] and A[i1+width..i1+2∗width−1] to temp[i1..2∗width−1]\\nint i2 ='),\n",
       " Document(page_content='= 2 ∗ width) {\\nmerge arrays:\\nfor (int i1 = 0; i1 < SIZE; i1 = i1 + 2 ∗ width) {\\n// Try to merge two sorted subarrays:\\n// A[i1..i1+width−1] and A[i1+width..i1+2∗width−1] to temp[i1..2∗width−1]\\nint i2 = i1 + width;\\nint i3 = i1 + 2∗width;\\nif(i2 >= SIZE) i2 = SIZE;\\nif(i3 >= SIZE) i3 = SIZE;\\nmerge(A, i1, i2, i3, temp);\\n}\\n// Copy temp[] back to A[] for next iteration\\ncopy:\\nfor(int i = 0; i < SIZE; i++) {\\nA[i] = temp[i];\\n}\\n}\\n}\\nFigure 10.11: A non-recursive implementation of merge sort. The merge sort() function iteratively\\nmerges subarrays until the entire array has been sorted.\\n187SORTING ALGORITHMS\\nbetween i1, i2, and i3? If we restrict the allowed values of the parameter SIZE can the code be\\nsimplified? What is the affect on the resulting HLS generated circuit?\\nThemerge()functionperformsthe“twofinger”algorithmontwosubarrayswithinthein[]array.\\nThe function takes input in the in[] array and produces output in the out[] array. The function also\\ntakes as input variables i1, i2, and i3 which'),\n",
       " Document(page_content='function takes input in the in[] array and produces output in the out[] array. The function also\\ntakes as input variables i1, i2, and i3 which describe the extent of the two subarrays to be merged.\\nOne subarray starts at index i1 and includes all the elements before i2 and the second subarray\\nstarts at index i2 and includes the elements up to i3. The merged output subarray will be stored\\nfrom index i1 up to i3 in out[].\\nThe merge() function consists of a single loop which iterates over the elements being stored into\\nout[]. Each iteration places an element into its correctly sorted location in out[]. The variables\\nf1 and f2 within the function correspond to the position of the fingers for each subarray. The if\\ncondition selects the smaller of in[f1] or in[f2] to copy to the next sorted position in out[]. However,\\nthe if condition is more complex since it has to deal with several special cases. One case is where\\nf1 == i2 and we have run out of elements to consider for in[f1], in which'),\n",
       " Document(page_content='in out[]. However,\\nthe if condition is more complex since it has to deal with several special cases. One case is where\\nf1 == i2 and we have run out of elements to consider for in[f1], in which case we must select in[f2]\\nas the smallest element. Alternatively, if f2 == i3, then we have run out of elements to consider\\nfor in[f2], in which case we must select in[f1] as the smallest element.\\nWhat happens to the in[] array over the course of the computation? Describe the state of\\nthe elements in the in[] array after each iteration of the outer for loop. What is the final order\\nof the elements in the in[] array when merge sort() returns?\\nThe performance report after synthesis may not be able to determine the number of cycles\\nfor the latency and interval. Why is that the case? What are appropriate min, max, and avg\\nvalues to provide in a loop tripcount directive(s)?\\nThe code is not particularly optimized for any particular high-level synthesis (HLS) implemen-\\ntation. The best starting point'),\n",
       " Document(page_content='min, max, and avg\\nvalues to provide in a loop tripcount directive(s)?\\nThe code is not particularly optimized for any particular high-level synthesis (HLS) implemen-\\ntation. The best starting point for optimization is by adding directives. Given that we have several\\nnested for loops, we generally look first at optimizing the inner loops. Optimizations of inner loops\\nare usually much more significant than optimizations of outer loops, which are executed relatively\\nrarely. By this point, you should be familiar with the common pipeline and unroll directives for\\nloop optimization.\\nPerform different optimizations using the pipeline and unroll directives on the for loops.\\nWhatprovidesthebestperformance? Whichgivesthebesttradeoffbetweenresourceutilization\\nand performance? What aspects of the code prevent higher performance? Are these aspects\\nfundamental to the algorithm, or only because of the way the algorithm is captured in'),\n",
       " Document(page_content='performance? What aspects of the code prevent higher performance? Are these aspects\\nfundamental to the algorithm, or only because of the way the algorithm is captured in the\\ncode?\\nPipeliningandunrollingcanbehinderedbyresourceconstraints;inparticular,wemustcarefully\\nconsider the number of memory ports for the arrays. The arrays in this code seem relatively\\nstraightforward as they are both one-dimensional. Yet, the designer must carefully consider the\\naccess patterns to insure that performance optimizations match with the resource constraints.\\n188SORTING ALGORITHMS\\nOptimize the code using loop optimizations and array partitioning, i.e., create a set of\\ndesigns using the array partition, pipeline, and unroll directives. Were you able to achieve\\nbetter results than by using the pipeline and unroll directives alone? What was the best\\nstrategy for performing design space exploration using these directives? What was your best\\ndesign in terms of performance? What was the best design that'),\n",
       " Document(page_content='and unroll directives alone? What was the best\\nstrategy for performing design space exploration using these directives? What was your best\\ndesign in terms of performance? What was the best design that provides a good tradeoff\\nbetween resource utilization and performance?\\nMany times the best designs are only possible by performing code restructuring. Although\\nVivado(cid:13)R HLS provides many directives to enable common code optimizations, it is impractical\\nto provide directives for every optimization. Sometimes we must resort to rewriting the code in\\naddition to providing directives in order to achieve a design that meets our requirements. In the\\nnextsectionwedescribeonewaytosignificantlyrestructurethemergesortcodeinordertoincrease\\nthroughput.\\n10.3.2 Restructured Merge Sort\\nLooking first at the inner loop of the merge() function, you might have found that it was difficult\\nto achieve a loop II of 1. One challenge is that there are actually four reads of in[], but only at\\ntwo different'),\n",
       " Document(page_content='at the inner loop of the merge() function, you might have found that it was difficult\\nto achieve a loop II of 1. One challenge is that there are actually four reads of in[], but only at\\ntwo different addresses. The HLS tool must recognize that these reads are redundant, since block\\nRAM (BRAM) memories can only support two accesses each clock. However because these reads\\nare in different basic blocks, it is more difficult for a compiler to eliminate the redundant loads. By\\neliminating the redundant reads, the compiler needs to do less optimization to achieve a loop II of\\n1. The restructured code is shown in Figure 10.12. In addition, there is a recurrence through the\\nf1 and f2 variables. These variables are incremented in one of the branches of the if conditional,\\nwhich must be used in the next iteration to determine which locations in in[] to compare and\\nsubsequently which branch of the conditional to take. Because the floating point comparison is\\nrelatively complex, this recurrence'),\n",
       " Document(page_content='next iteration to determine which locations in in[] to compare and\\nsubsequently which branch of the conditional to take. Because the floating point comparison is\\nrelatively complex, this recurrence can also limit the achievable II and clock period.\\nThe behavior of this code is shown in Figure 10.13. Although the inner loop achieves a loop II\\nof 1, this inner loop often has a very small number of loop iterations. When the inner loop finishes,\\nthe pipeline must empty before code executing after the pipeline can execute. Although the loop\\npipelineisrelativelyshortinthiscase, thebubblecausedbytheloopcompletingissignificant, since\\nthe number of iterations is also small. Unfortunately, because of the limits of static loop analysis,\\nthe performance of this particular code is somewhat hard to visualize. In this case, the number of\\niterations of the inner loop is data dependent.\\nA common approach is to flatten loop nests like these into a single loop, reducing the number\\nof times that the'),\n",
       " Document(page_content='visualize. In this case, the number of\\niterations of the inner loop is data dependent.\\nA common approach is to flatten loop nests like these into a single loop, reducing the number\\nof times that the pipeline must flush when exiting a loop. In fact, Vivado(cid:13)R HLS will automatically\\nflatten perfect loop nests. In this case, however, since the code does not contain a perfect loop\\nnest, we can resort to flattening the loops manually. Code resulting from manually flattening the\\nmerge arrays loop with the loop inside the merge() function is shown in Figure 10.14. Note that one\\nadvantage of this code is that the merge arrays loop also has a constant number of loop iterations,\\nmaking understanding performance much easier.\\nEstimate the performance of the code in Figure 10.14. Even though the inner loops have\\nachievedaloopIIof1, isthedesignusinghardwareefficiently? Isthereawaytofurtherreduce\\nthe latency of the merge sort() function to the point where it is using approximately N logN\\nclock'),\n",
       " Document(page_content='inner loops have\\nachievedaloopIIof1, isthedesignusinghardwareefficiently? Isthereawaytofurtherreduce\\nthe latency of the merge sort() function to the point where it is using approximately N logN\\nclock cycles?\\n189SORTING ALGORITHMS\\n#include ”merge sort.h”\\n#include ”assert.h”\\n// subarray1 is in[ii..i2−1]; subarray2 is in[i2..i3−1]\\n// sorted merge is stored in out[i1..i3−1]\\nvoid merge(DTYPE in[SIZE], int i1, int i2, int i3, DTYPE out[SIZE]) {\\nint f1 = i1, f2 = i2;\\n// Foreach element that needs to be sorted...\\nfor(int index = i1; index < i3; index++) {\\n#pragma HLS pipeline II=1\\nDTYPE t1 = in[f1];\\nDTYPE t2 = in[f2];\\n// Select the smallest available element.\\nif((f1 < i2 && t1 <= t2) || f2 == i3) {\\nout[index] = t1;\\nf1++;\\n} else {\\nassert(f2 < i3);\\nout[index] = t2;\\nf2++;\\n}\\n}\\n}\\nFigure 10.12: Restructured code for the merge() function, which can achieve a loop II of 1 in\\nVivado(cid:13)R HLS.\\nwidth=1 width = 2\\nmerge arrays copy merge arrays copy\\nmerge merge merge merge merge merge\\nFigure 10.13:'),\n",
       " Document(page_content='code for the merge() function, which can achieve a loop II of 1 in\\nVivado(cid:13)R HLS.\\nwidth=1 width = 2\\nmerge arrays copy merge arrays copy\\nmerge merge merge merge merge merge\\nFigure 10.13: Behavior of the restructured code in Figure 10.12.\\n190SORTING ALGORITHMS\\n#include ”merge sort.h”\\n#include ”assert.h”\\nvoid merge sort(DTYPE A[SIZE]) {\\nDTYPE temp[SIZE];\\nstage:\\nfor (int width = 1; width < SIZE; width = 2 ∗ width) {\\nint f1 = 0;\\nint f2 = width;\\nint i2 = width;\\nint i3 = 2∗width;\\nif(i2 >= SIZE) i2 = SIZE;\\nif(i3 >= SIZE) i3 = SIZE;\\nmerge arrays:\\nfor (int i = 0; i < SIZE; i++) {\\n#pragma HLS pipeline II=1\\nDTYPE t1 = A[f1];\\nDTYPE t2 = A[f2];\\nif((f1 < i2 && t1 <= t2) || f2 == i3) {\\ntemp[i] = t1;\\nf1++;\\n} else {\\nassert(f2 < i3);\\ntemp[i] = t2;\\nf2++;\\n}\\nif(f1 == i2 && f2 == i3) {\\nf1 = i3;\\ni2 += 2∗width;\\ni3 += 2∗width;\\nif(i2 >= SIZE) i2 = SIZE;\\nif(i3 >= SIZE) i3 = SIZE;\\nf2 = i2;\\n}\\n}\\ncopy:\\nfor(int i = 0; i < SIZE; i++) {\\n#pragma HLS pipeline II=1\\nA[i] = temp[i];\\n}\\n}\\n}\\nFigure 10.14: Restructured'),\n",
       " Document(page_content='+= 2∗width;\\nif(i2 >= SIZE) i2 = SIZE;\\nif(i3 >= SIZE) i3 = SIZE;\\nf2 = i2;\\n}\\n}\\ncopy:\\nfor(int i = 0; i < SIZE; i++) {\\n#pragma HLS pipeline II=1\\nA[i] = temp[i];\\n}\\n}\\n}\\nFigure 10.14: Restructured code for Merge Sort which can achieve a loop II of 1 with fewer pipeline\\nbubbles in Vivado(cid:13)R HLS.\\n191SORTING ALGORITHMS\\nA[] temp[0] temp[1] temp[2] B[]\\nmerge arrays merge arrays merge arrays merge arrays\\nFigure 10.15: Dataflow pipelined architecture for implementing 4 stages of Merge Sort. This\\narchitecture can sort up to 16 elements.\\nSo far we have focused on optimizing the merge sort() function to reduce the latency of the\\ncomputation without significantly increasing the number of resources. As a result, the accelerator\\nis becoming more efficient. However, after achieving a reasonably efficient design, the only way to\\nfurther reduce latency and/or increase throughput is increase parallelism. Previously we have seen\\nways of unrolling inner loops and partitioning arrays as a way to perform'),\n",
       " Document(page_content='design, the only way to\\nfurther reduce latency and/or increase throughput is increase parallelism. Previously we have seen\\nways of unrolling inner loops and partitioning arrays as a way to perform more work each clock\\ncycle. Analternativewaytoincreaseparallelismisleveragepipelining. Inadditiontooperator-level\\npipelinling we can also look for coarser-granularity task-level pipelining.\\nWith Merge Sort it turns out that we can make a dataflow pipeline out of each iteration of the\\nstage loop, assuming that we have a fixed size array to sort. In Vivado(cid:13)R HLS this implementation\\ncan be conceptually achieved by unrolling the stage loop and using the dataflow directive. Each\\ninstance of the merge arrays loop then becomes an independent process which can operate on a\\ndifferent set of data. The resulting architecture is shown in Figure 10.15.\\nThe code to implement this is shown in Figure 10.16. This code is similar in many ways to the\\noriginalcode,buthasseveralimportantdifferences.'),\n",
       " Document(page_content='data. The resulting architecture is shown in Figure 10.15.\\nThe code to implement this is shown in Figure 10.16. This code is similar in many ways to the\\noriginalcode,buthasseveralimportantdifferences. Onekeydifferenceisthatthemerge arraysloop\\nhas been extracted into a function, making it easier simpler to rewrite the toplevel code. A second\\nkey difference is that the output of merge sort parallel() is produced in a separate array than the\\ninput, enabling Vivado(cid:13)R HLS to build an architecture that pipelined architecture processing using\\nthedataflowdirective. Additionally,thetemp[][]arrayisnowusedtomodelthedataflowping-pong\\nchannels between the processes implemented by the merge arrays() function, making extra array\\ncopiesunnecessary. This two-dimensionalarrayispartitionedinthefirstdimension, makingiteasy\\nto represent a parameterized number of channels.\\nThe merge sort parallel() function consists of STAGES calls to the merge arrays() function.'),\n",
       " Document(page_content='makingiteasy\\nto represent a parameterized number of channels.\\nThe merge sort parallel() function consists of STAGES calls to the merge arrays() function. Each\\nThefirstcallreadsfromtheinputandwritestotemp[0]. Theloopperformstheintermediatestages\\nand writes to the remaining partitions of temp[]. The final call writes to the output array B[]. The\\ncode is parameterized in terms of SIZE and STAGES and supports array lengths 4 or greater.\\nEstimate the performance of the code in Figure 10.16. What is the interval and latency of\\nthe implementation? How much memory is required to support that processing rate? Is all of\\nthe memory necessary?\\n10.4 Conclusion\\nThis chapter introduced a number of basic sorting algorithms with fundamentally different algo-\\nrithmic tradeoffs. Insertion sort operating on an array performs on average N2/4 comparisons, but\\nrequires much fewer (approximately N comparisons) when operating on sorted data. We showed\\nseveral ways of parallelizing Insertion sort which can'),\n",
       " Document(page_content='on an array performs on average N2/4 comparisons, but\\nrequires much fewer (approximately N comparisons) when operating on sorted data. We showed\\nseveral ways of parallelizing Insertion sort which can increase performance at the expense of us-\\ning more hardware resources. Because of bubbles in the statically scheduled pipeline, we end up\\nneeding roughly N comparators with an interval of N cycles to perform insertion sort. Merge\\nsort operating on an array performs generally fewer comparisons, roughly N logN, but requires\\n192SORTING ALGORITHMS\\n#include ”merge sort parallel.h”\\n#include ”assert.h”\\nvoid merge arrays(DTYPE in[SIZE], int width, DTYPE out[SIZE]) {\\nint f1 = 0;\\nint f2 = width;\\nint i2 = width;\\nint i3 = 2∗width;\\nif(i2 >= SIZE) i2 = SIZE;\\nif(i3 >= SIZE) i3 = SIZE;\\nmerge arrays:\\nfor (int i = 0; i < SIZE; i++) {\\n#pragma HLS pipeline II=1\\nDTYPE t1 = in[f1];\\nDTYPE t2 = in[f2];\\nif((f1 < i2 && t1 <= t2) || f2 == i3) {\\nout[i] = t1;\\nf1++;\\n} else {\\nassert(f2 < i3);\\nout[i] = t2;\\nf2++;\\n}\\nif(f1'),\n",
       " Document(page_content='i = 0; i < SIZE; i++) {\\n#pragma HLS pipeline II=1\\nDTYPE t1 = in[f1];\\nDTYPE t2 = in[f2];\\nif((f1 < i2 && t1 <= t2) || f2 == i3) {\\nout[i] = t1;\\nf1++;\\n} else {\\nassert(f2 < i3);\\nout[i] = t2;\\nf2++;\\n}\\nif(f1 == i2 && f2 == i3) {\\nf1 = i3;\\ni2 += 2∗width;\\ni3 += 2∗width;\\nif(i2 >= SIZE) i2 = SIZE;\\nif(i3 >= SIZE) i3 = SIZE;\\nf2 = i2;\\n}\\n}\\n}\\nvoid merge sort parallel(DTYPE A[SIZE], DTYPE B[SIZE]) {\\n#pragma HLS dataflow\\nDTYPE temp[STAGES−1][SIZE];\\n#pragma HLS array partition variable=temp complete dim=1\\nint width = 1;\\nmerge arrays(A, width, temp[0]);\\nwidth ∗= 2;\\nfor (int stage = 1; stage < STAGES−1; stage++) {\\n#pragma HLS unroll\\nmerge arrays(temp[stage−1], width, temp[stage]);\\nwidth ∗= 2;\\n}\\nmerge arrays(temp[STAGES−2], width, B);\\n}\\nFigure 10.16: Restructured code for Merge Sort which can be implemented as a dataflow pipeline\\nby Vivado(cid:13)R HLS.\\n193SORTING ALGORITHMS\\nadditional memory to store partially sorted results. The more complex loop structure of Merge\\nSort means that we required additional'),\n",
       " Document(page_content='a dataflow pipeline\\nby Vivado(cid:13)R HLS.\\n193SORTING ALGORITHMS\\nadditional memory to store partially sorted results. The more complex loop structure of Merge\\nSort means that we required additional refactoring to reach an efficient solution that took roughly\\nN logN cycles with one comparator. We also showed a task-pipelined implementation which per-\\nforms logN comparisons every clock cycle with an interval of N cycles to perform merge sort. In\\ncontrast with insertion sort, this requires much fewer comparisons to achieve the same interval, but\\nrequires more memory (in the form of dataflow channels) and has much higher latency.\\nIn practice, many FPGA-based implementations of sorting will have to address these funda-\\nmental tradeoffs in order to achieve the best integration with other aspects of a system. Another\\nsort with a fundamentally different tradeoff is the Radix Sort, which focuses on data that exists\\nwithin a bounded range, unlike the more general sorting algorithms in this'),\n",
       " Document(page_content='of a system. Another\\nsort with a fundamentally different tradeoff is the Radix Sort, which focuses on data that exists\\nwithin a bounded range, unlike the more general sorting algorithms in this chapter which only\\nrequire comparison between items. Radix sort will be implemented as part of the Huffman Coding\\nin Chapter 11.\\nParallel implementations of sorting algorithms are often described as sorting networks. Sorting\\nnetworks are sometimes described as systolic arrays and other times as pipelines. We can often\\ngain inspiration for HLS designs by looking investigating these existing parallel implementations\\nand then capturing them as C code. In Vivado(cid:13)R HLS, these networks can be described using either\\nloop pipelines, dataflow pipelines, or a combination of both.\\n194Chapter 11\\nHuffman Encoding\\n11.1 Background\\nLossless data compression is a key ingredient for efficient data storage, and Huffman coding is\\namongst the most popular algorithms for variable length coding [33]. Given a'),\n",
       " Document(page_content='Encoding\\n11.1 Background\\nLossless data compression is a key ingredient for efficient data storage, and Huffman coding is\\namongst the most popular algorithms for variable length coding [33]. Given a set of data symbols\\nand their frequencies of occurrence, Huffman coding generates codewords in a way that assigns\\nshorter codes to more frequent symbols to minimize the average code length. Since it guarantees\\noptimality, Huffmancodinghasbeenwidelyadoptedforvariousapplications[25]. Inmodernmulti-\\nstage compression designs, it often functions as a back-end of the system to boost compression\\nperformance after a domain-specific front-end as in GZIP [23], JPEG [57], and MP3 [59]. Although\\narithmetic encoding [61] (a generalized version of Huffman encoding which translates an entire\\nmessage into a single number) can achieve better compression for most scenarios, Huffman coding\\nhasoftenbeenthealgorithmofchoiceformanysystemsbecauseofpatentconcernswitharithmetic\\nencoding [38].\\nCanonical Huffman'),\n",
       " Document(page_content='a single number) can achieve better compression for most scenarios, Huffman coding\\nhasoftenbeenthealgorithmofchoiceformanysystemsbecauseofpatentconcernswitharithmetic\\nencoding [38].\\nCanonical Huffman coding has two main benefits over traditional Huffman coding. In basic\\nHuffmancoding,theencoderpassesthecompleteHuffmantreestructuretothedecoder. Therefore,\\nthe decoder must traverse the tree to decode every encoded symbol. On the other hand, canonical\\nHuffman coding only transfers the number of bits for each symbol to the decoder, and the decoder\\nreconstructs the codeword for each symbol. This makes the decoder more efficient both in memory\\nusage and computation requirements. Thus, we focus on canonical Huffman coding.\\nIn basic Huffman coding, the decoder decompresses the data by traversing the Huffman tree\\nfromtherootuntilithitstheleafnode. Thishastwomajordrawbacks: itrequiresstoringtheentire\\nHuffman tree which increases memory usage. Furthermore, traversing the tree for each symbol'),\n",
       " Document(page_content='the Huffman tree\\nfromtherootuntilithitstheleafnode. Thishastwomajordrawbacks: itrequiresstoringtheentire\\nHuffman tree which increases memory usage. Furthermore, traversing the tree for each symbol is\\ncomputationally expensive. Canonical Huffman encoding addresses these two issues by creating\\ncodes using a standardized canonical format. The benefit of using a canonical encoding is that\\nwe only need to transmit the length of each Huffman codeword. A Canonical Huffman code has\\ntwo additional properties. Firstly, longer length codes have a higher numeric value than the same\\nlength prefix of shorter codes. Secondly, codes with the same length increase by one as the symbol\\nvalue increases. This means if we know the starting symbol for each code length, we can easily\\nreconstruct the canonical Huffman code. The Huffman tree is essentially equivalent to a ‘sorted’\\nversion of the original Huffman tree so that longer codewords are on the right-most branch of the\\ntree and all of the nodes at the'),\n",
       " Document(page_content='code. The Huffman tree is essentially equivalent to a ‘sorted’\\nversion of the original Huffman tree so that longer codewords are on the right-most branch of the\\ntree and all of the nodes at the same level of the tree are sorted in order of the symbols.\\nFigure 11.1 shows the process of creating a canonical Huffman code. The filter module only\\npasses symbols with non-zero frequencies. The sort module rearranges the symbols in ascending\\norder based upon their frequencies. Next, the create tree module builds the Huffman tree using\\n195HUFFMAN ENCODING\\nBasic Huffman Encoding\\ncreate compute (continued\\nfilter sort\\ntree bit length below)\\nS F S F S F 17 L N\\n0 1\\nA 3 A 3 F 1 2 3\\n7 10\\nB 1 B 1 B 1 0 1 0 1 3 1\\nA 4 D E\\nC 2 C 2 C 2 0 1 4 2\\nD 5 D 5 A 3 C 2\\n0 1\\nE 5 E 5 D 5\\nB F\\nF 1 F 1 E 5\\nG 0\\ntruncate canonize cr eate\\ntree tree codeword\\n(from\\nabove) L N S L S L S CW 17\\n0 1\\n2 3 A 2 A 2\\nA 00\\n7 10\\n3 1 B 4 D 2 B 1110 0 1 0 1\\n4 2 C 3 E 2 C 110 A D E\\n0\\n4\\n1\\nD 2 C 3 D 01\\nC 2\\nE 2 B 4 E 10 0 1\\nB F\\nF 4 F 4 F'),\n",
       " Document(page_content='canonize cr eate\\ntree tree codeword\\n(from\\nabove) L N S L S L S CW 17\\n0 1\\n2 3 A 2 A 2\\nA 00\\n7 10\\n3 1 B 4 D 2 B 1110 0 1 0 1\\n4 2 C 3 E 2 C 110 A D E\\n0\\n4\\n1\\nD 2 C 3 D 01\\nC 2\\nE 2 B 4 E 10 0 1\\nB F\\nF 4 F 4 F 1111\\nFigure 11.1: The Canonical Huffman Encoding process. The symbols are filtered and sorted, and\\nused to build a Huffman tree. Instead of passing the entire tree to the decoder (as is done in\\n“basic” Huffman coding), the encoding is done such that only the length of the symbols in the\\ntree is required by the decoder. Note that the final canonical tree is different from the initial tree\\ncreated near the beginning of the process.\\n196HUFFMAN ENCODING\\nthree steps: 1) it uses the two minimum frequency nodes as an initial sub-tree and generates a\\nnew parent node by summing their frequencies; 2) it adds the new intermediate node to the list\\nand sorts them again; and 3) it selects the two minimum elements from the list and repeats these\\nsteps until one element remains. The result is a Huffman'),\n",
       " Document(page_content='it adds the new intermediate node to the list\\nand sorts them again; and 3) it selects the two minimum elements from the list and repeats these\\nsteps until one element remains. The result is a Huffman tree where each leaf node in the tree\\nrepresents a symbol that can be coded and each internal node is labeled with the frequency of the\\nnodes in that sub-tree. By associating the left and right edges in the tree with bits 0 and 1, we\\ncan determine the unique codeword for each symbol based on the path to reach it from the root\\nnode. For example, the codeword for A is 00 and codeword for B is 1110. This completes the basic\\nHuffman encoding process, but does not necessarily create the canonical Huffman tree.\\nTo create the canonical Huffman tree, we perform several additional transformations. First, the\\ncompute bit len module calculates the bit length of each codeword and then counts the frequency\\nof each length. The result is a histogram of the codeword lengths (see Section 8.2). In the'),\n",
       " Document(page_content='First, the\\ncompute bit len module calculates the bit length of each codeword and then counts the frequency\\nof each length. The result is a histogram of the codeword lengths (see Section 8.2). In the example\\ncase, we have three symbols (A,D,E) with the code length of 2. Therefore, the computed histogram\\nmaps contains value 3 in location 2. Next, the truncate tree module rebalances the Huffman tree\\nin order to avoid excessively long codewords. This can improve decoder speed at the cost of a\\nslight increase in encoding time. This is not necessary in the example in Figure 11.1. We set the\\nmaximum height of the tree to 27. Lastly, the canonize tree module creates two sorted tables. The\\nfirst table contains symbols and lengths sorted by symbol. The second table contains symbols and\\nlengths sorted by lengths. These tables simplify the creation of the canonical Huffman codewords\\nfor each symbol.\\nThe create codeword module creates a table of canonical Huffman codewords by traversing the\\nsorted'),\n",
       " Document(page_content='by lengths. These tables simplify the creation of the canonical Huffman codewords\\nfor each symbol.\\nThe create codeword module creates a table of canonical Huffman codewords by traversing the\\nsorted tables. Beginning with the first codeword in the sorted table, it is assigned the all-zero\\ncodeword with the appropriate length. Each following symbol with the same bit length is assigned\\nthe following codeword, which is formed by simply adding 1 to the previous code word. In our\\nexample, symbols A, D, and E all have bit length l = 2 and are assigned the codewords A = 00,\\nD = 01, and E = 10. Note that the symbols are considered in alphabetical order, which is necessary\\nto make the tree canonical. This process continues until we get to a codeword that requires a\\nlarger length, in which case we not only increment the previous codeword, but also shift left to\\ngenerate a codeword of the correct length. In the example, the next symbol is C with a length of\\n3, which receives the codeword C ='),\n",
       " Document(page_content='we not only increment the previous codeword, but also shift left to\\ngenerate a codeword of the correct length. In the example, the next symbol is C with a length of\\n3, which receives the codeword C = (10+1) << 1 = 11 << 1 = 110. Continuing on, the next\\nsymbol is B with a length of 4. Once again we increment and shift by one. Thus the codeword for\\nB = (110+1) << 1 = 1110. The final codeword for symbol F = 1110+1 = 1111. We explain this\\nin more detail in Chapter 11.2.7.\\nThecreationofacanonicalHuffmancodeincludesmanycomplexandinherentlysequentialcom-\\nputations. For example, the create tree module needs to track the correct order of the created sub\\ntrees, requiring careful memory management. Additionally, there is very limited parallelism that\\ncan be exploited. In the following, we discuss the hardware architecture and the implementation\\nof the canonical Huffman encoding design using Vivado(cid:13)R HLS.\\nFigure11.2showstheentire“top”huffman encodingfunction.'),\n",
       " Document(page_content='In the following, we discuss the hardware architecture and the implementation\\nof the canonical Huffman encoding design using Vivado(cid:13)R HLS.\\nFigure11.2showstheentire“top”huffman encodingfunction. Thissetsupthearraysandother\\nvariables that are passed between the various subfunctions. And it instantiates these functions.\\nThere is some additional copying of data that may seem unnecessary. This is due to our use\\nof the dataflow directive. This imparts some restrictions on the flow of the variables between the\\nsubfunctions. In particular, there are some strict rules on producer and consumer relationships\\nof data between the parts of the function. This requires that we replicate some of the data. For\\nexample, we create two copies of the arrays parent, left and right. We also do the same with the\\narray truncated bit length. The former is done in a for loop in the top huffman encoding function;\\nthe latter is done inside of the canonize tree function.\\n197HUFFMAN ENCODING\\n#include'),\n",
       " Document(page_content='same with the\\narray truncated bit length. The former is done in a for loop in the top huffman encoding function;\\nthe latter is done inside of the canonize tree function.\\n197HUFFMAN ENCODING\\n#include ”huffman.h”\\n#include ”assert.h”\\nvoid huffman encoding(\\n/∗ input ∗/ Symbol symbol histogram[INPUT SYMBOL SIZE],\\n/∗ output ∗/ PackedCodewordAndLength encoding[INPUT SYMBOL SIZE],\\n/∗ output ∗/ int ∗num nonzero symbols) {\\n#pragma HLS DATAFLOW\\nSymbol filtered[INPUT SYMBOL SIZE];\\nSymbol sorted[INPUT SYMBOL SIZE];\\nSymbol sorted copy1[INPUT SYMBOL SIZE];\\nSymbol sorted copy2[INPUT SYMBOL SIZE];\\nap uint<SYMBOL BITS> parent[INPUT SYMBOL SIZE−1];\\nap uint<SYMBOL BITS> left[INPUT SYMBOL SIZE−1];\\nap uint<SYMBOL BITS> right[INPUT SYMBOL SIZE−1];\\nint n;\\nfilter(symbol histogram, filtered, &n);\\nsort(filtered, n, sorted);\\nap uint<SYMBOL BITS> length histogram[TREE DEPTH];\\nap uint<SYMBOL BITS> truncated length histogram1[TREE DEPTH];\\nap uint<SYMBOL BITS> truncated length histogram2[TREE DEPTH];\\nCodewordLength'),\n",
       " Document(page_content='sorted);\\nap uint<SYMBOL BITS> length histogram[TREE DEPTH];\\nap uint<SYMBOL BITS> truncated length histogram1[TREE DEPTH];\\nap uint<SYMBOL BITS> truncated length histogram2[TREE DEPTH];\\nCodewordLength symbol bits[INPUT SYMBOL SIZE];\\nint previous frequency = −1;\\ncopy sorted:\\nfor(int i = 0; i < n; i++) {\\nsorted copy1[i].value = sorted[i].value;\\nsorted copy1[i].frequency = sorted[i].frequency;\\nsorted copy2[i].value = sorted[i].value;\\nsorted copy2[i].frequency = sorted[i].frequency;\\n// std::cout << sorted[i].value << ” ” << sorted[i].frequency << ”\\\\n”;\\nassert(previous frequency <= (int)sorted[i].frequency);\\nprevious frequency = sorted[i].frequency;\\n}\\ncreate tree(sorted copy1, n, parent, left, right);\\ncompute bit length(parent, left, right, n, length histogram);\\n#ifndef SYNTHESIS\\n// Check the result of computing the tree histogram\\nint codewords in tree = 0;\\n198HUFFMAN ENCODING\\nmerge bit length:\\nfor(int i = 0; i < TREE DEPTH; i++) {\\n#pragma HLS PIPELINE II=1\\nif(length histogram[i] >'),\n",
       " Document(page_content='the result of computing the tree histogram\\nint codewords in tree = 0;\\n198HUFFMAN ENCODING\\nmerge bit length:\\nfor(int i = 0; i < TREE DEPTH; i++) {\\n#pragma HLS PIPELINE II=1\\nif(length histogram[i] > 0)\\nstd::cout << length histogram[i] << ” codewords with length ” << i << ”\\\\n”;\\ncodewords in tree += length histogram[i];\\n}\\nassert(codewords in tree == n);\\n#endif\\ntruncate tree(length histogram, truncated length histogram1, truncated length histogram2);\\ncanonize tree(sorted copy2, n, truncated length histogram1, symbol bits);\\ncreate codeword(symbol bits, truncated length histogram2, encoding);\\n∗num nonzero symbols = n;\\n}\\nFigure 11.2: The “top” huffman encoding function. It defines the arrays and variables between the\\nvarious subfunctions. These are described graphically in Figures 11.1 and 11.4.\\nThedataflowdirectiveimposesrestrictionsontheflowofinformationinthefunction. Many\\noftherestrictionsenforceastrictproducerandconsumerrelationshipbetweenthesubfunctions.\\nOne such restriction is that an'),\n",
       " Document(page_content='Many\\noftherestrictionsenforceastrictproducerandconsumerrelationshipbetweenthesubfunctions.\\nOne such restriction is that an array should be written to by only one function and it should\\nbe read by only one function. i.e., it should only serve as an output from one function and an\\ninput to another function. If multiple functions read from the same array, Vivado(cid:13)R HLS will\\nsynthesize the code but will issue a warning and not use a dataflow pipelined architecture. As\\na result, using dataflow mode often requires replicating data into multiple arrays. A similar\\nproblem occurs if a function attempts to read from and write to an array which is also accessed\\nby another function. In this case it is necessary to maintain an additional internal copy of\\nthe data inside the function. We will discuss both of these requirements and how to adhere to\\nthem as we go through the code in the remainder of this chapter.\\n11.2 Implementation\\nThe canonical Huffman encoding process is naturally divided'),\n",
       " Document(page_content='discuss both of these requirements and how to adhere to\\nthem as we go through the code in the remainder of this chapter.\\n11.2 Implementation\\nThe canonical Huffman encoding process is naturally divided into subfunctions. Thus, we can work\\non each of these subfunctions on a one-by-one basis. Before we do that, we should consider the\\ninterface for each of these functions.\\nFigure 11.4 shows the functions and their input and output data. For the sake of simplicity,\\nit only shows the interfaces with arrays, which, since they are large, we can assume are stored in\\nblock rams (BRAMs). Before we describe the functions and their inputs and outputs, we need to\\ndiscuss the constants, custom data types, and the function interface that are defined in huffman.h.\\nFigure 11.3 shows the contents of this file.\\nThe INPUT SYMBOL SIZE parameter specifies the maximum number of symbols that will be\\ngiven as input for encoding. In this case, we’ve set it to 256, enabling the encoding of 8-bit ASCII\\n199HUFFMAN'),\n",
       " Document(page_content='INPUT SYMBOL SIZE parameter specifies the maximum number of symbols that will be\\ngiven as input for encoding. In this case, we’ve set it to 256, enabling the encoding of 8-bit ASCII\\n199HUFFMAN ENCODING\\n#include ”ap int.h”\\n// input number of symbols\\nconst static int INPUT SYMBOL SIZE = 256;\\n// upper bound on codeword length during tree construction\\nconst static int TREE DEPTH = 64;\\n// maximum codeword tree length after rebalancing\\nconst static int MAX CODEWORD LENGTH = 27;\\n// Should be log2(INPUT SYMBOL SIZE)\\nconst static int SYMBOL BITS = 10;\\n// Should be log2(TREE DEPTH)\\nconst static int TREE DEPTH BITS = 6;\\n// number of bits needed to record MAX CODEWORD LENGTH value\\n// Should be log2(MAX CODEWORD LENGTH)\\nconst static int CODEWORD LENGTH BITS = 5;\\n// A marker for internal nodes\\nconst static ap uint<SYMBOL BITS> INTERNAL NODE = −1;\\ntypedef ap uint<MAX CODEWORD LENGTH> Codeword;\\ntypedef ap uint<MAX CODEWORD LENGTH + CODEWORD LENGTH BITS> PackedCodewordAndLength;\\ntypedef ap'),\n",
       " Document(page_content='static ap uint<SYMBOL BITS> INTERNAL NODE = −1;\\ntypedef ap uint<MAX CODEWORD LENGTH> Codeword;\\ntypedef ap uint<MAX CODEWORD LENGTH + CODEWORD LENGTH BITS> PackedCodewordAndLength;\\ntypedef ap uint<CODEWORD LENGTH BITS> CodewordLength;\\ntypedef ap uint<32> Frequency;\\nstruct Symbol {\\nap uint<SYMBOL BITS> value;\\nap uint<32> frequency;\\n};\\nvoid\\nhuffman encoding (\\nSymbol in[INPUT SYMBOL SIZE],\\nPackedCodewordAndLength encoding[INPUT SYMBOL SIZE],\\nint ∗num nonzero symbols\\n);\\nFigure 11.3: The parameters, custom data type, and function interface for the top level function\\nhuffman encoding.\\n200HUFFMAN ENCODING\\nSymbol\\nparent\\nsorted\\nSym inbol filter S fiy ltm erb eo dl sort c tr re ea et e left c bo im\\nt\\nlp eu nt e bitin leiti na gl th*\\nS sy om rtb eo dl\\nright\\ninitial * truncate truncate d canonize symbol create\\nencoding\\nbit length tree bit length tree bits codeword\\ntruncated\\nbit length\\nFigure 11.4: The block diagram for our hardware implementation of canonical Huffman encoding.\\nThe gray blocks'),\n",
       " Document(page_content='symbol create\\nencoding\\nbit length tree bit length tree bits codeword\\ntruncated\\nbit length\\nFigure 11.4: The block diagram for our hardware implementation of canonical Huffman encoding.\\nThe gray blocks represent the significant input and output data that is generated and consumed\\nby the different subfunctions. The white blocks correspond to the functions (computational cores).\\nNote that the array initial bit length appears twice to allow the figure to be more clear.\\ndata. The TREE DEPTH parameter specifies the upper bound for the length of an individual code-\\nword during the initial Huffman tree generation. The CODEWORD LENGTH parameter specifies\\nthetargettreeheightwhentheHuffmantreeisrebalancedinthefunctiontruncate tree. Finally,the\\nCODEWORD LENGTH BITSconstantdeterminesthenumberofbitsrequiredtoencodeacodeword\\nlength. This is equal to log (cid:100)CODEWORD LENGTH(cid:101), which in this case is 5.\\n2\\nWe create a custom data type Symbol to hold the data corresponding the input values and'),\n",
       " Document(page_content='This is equal to log (cid:100)CODEWORD LENGTH(cid:101), which in this case is 5.\\n2\\nWe create a custom data type Symbol to hold the data corresponding the input values and their\\nfrequencies. This datatype is used in the filter, sort, and other functions in the encoding process\\nthat require access to such information. The data type has two fields value and frequency. In this\\ncase we’ve assumed that the block of data being encoded contains no more than 232 symbols.\\nFinally, the huffman.h file has the huffman encoding function interface. This is the specified top\\nlevel function for the Vivado(cid:13)R HLS tool. It has three arguments. The first argument is an array of\\nSymbols of size INPUT SYMBOL SIZE. This array represents a histogram of the frequencies of the\\ndata in the block being encoded. The next two arguments are outputs. The encoding argument\\noutputs the codeword for each possible symbol. The num nonzero symbols argument is the number\\nof non-zero symbols from the input data. This'),\n",
       " Document(page_content='The next two arguments are outputs. The encoding argument\\noutputs the codeword for each possible symbol. The num nonzero symbols argument is the number\\nof non-zero symbols from the input data. This is the same as the number of symbols that remain\\nafter the filter operation.\\nThe input to the system is an array of Symbol. This holds the symbol value and frequencies in\\nthe array in. Each symbol holds a 10-bit value and a 32-bit frequency. The size of this array is set\\nas the constant INPUT SYMBOL SIZE which is 256 in our example. The filter module reads from\\nthe in array and writes its output to the filtered array. This is an array of Symbols which holds\\nthe number of non-zero elements which is the input to the sort module. The sort module writes\\nthe symbols sorted by frequency into two different arrays – one is used for the create tree module\\nand the other for the canonize tree module. The create tree module creates a Huffman tree from\\nthe sorted array and stores it into three arrays'),\n",
       " Document(page_content='arrays – one is used for the create tree module\\nand the other for the canonize tree module. The create tree module creates a Huffman tree from\\nthe sorted array and stores it into three arrays (parent, left, and right); these arrays hold all the\\n201HUFFMAN ENCODING\\n#include ”huffman.h”\\n// Postcondition: out[x].frequency > 0\\nvoid filter(\\n/∗ input ∗/ Symbol in[INPUT SYMBOL SIZE],\\n/∗ output ∗/ Symbol out[INPUT SYMBOL SIZE],\\n/∗ output ∗/ int ∗n) {\\n#pragma HLS INLINE off\\nap uint<SYMBOL BITS> j = 0;\\nfor(int i = 0; i < INPUT SYMBOL SIZE; i++) {\\n#pragma HLS pipeline II=1\\nif(in[i].frequency != 0) {\\nout[j].frequency = in[i].frequency;\\nout[j].value = in[i].value;\\nj++;\\n}\\n}\\n∗n = j;\\n}\\nFigure 11.5: The filter function iterates across the input array in and add any Symbol entry with\\na non-zero frequency field to the output array out. Additionally, it records the number of non-zero\\nfrequency elements and passes that in the output argument n.\\ninfo for each node of the Huffman tree. Using the Huffman tree'),\n",
       " Document(page_content='to the output array out. Additionally, it records the number of non-zero\\nfrequency elements and passes that in the output argument n.\\ninfo for each node of the Huffman tree. Using the Huffman tree information, the compute bit len\\nmodule calculates the bit length of each symbol and stores this information to a initial bit len array.\\nWe set the maximum number of entries to 64, covering up to maximum 64-bit frequency number,\\nwhich is sufficient for most applications given that our Huffman tree creation rebalances its height.\\nThe truncate tree module rebalances the tree height and copies the bit length information of each\\ncodewordintotwoseparatetruncated bit lengtharrays. Theyeachhavetheexactsameinformation,\\nbut they must be copied to ensure that the Vivado(cid:13)R HLS tool can perform functional pipelining;\\nwe will talk about that in more detail later. The canonize tree module walks through each symbol\\nfrom the sort module and assigns the appropriate bit length using the truncated bit'),\n",
       " Document(page_content='pipelining;\\nwe will talk about that in more detail later. The canonize tree module walks through each symbol\\nfrom the sort module and assigns the appropriate bit length using the truncated bit length array.\\nTheoutputofthecanonizemoduleisanarraythatcontainsthebitlengthsforthecodewordofeach\\nsymbol. Finally, the create codeword module generates the canonical codewords for each symbol.\\n11.2.1 Filter\\nThe first function for the Huffman encoding process is filter, which is shown in Figure 11.5. This\\nfunction takes as input a Symbol array. The output is another Symbol array that is a subset of the\\ninput array in. The filter function removes any entry with a frequency equal to 0. The function\\nitself simply iterates across the in array, storing each element to the out array if its frequency field\\nis non-zero. In addition, the function counts the number of non-zero entries to the output. This is\\npassed as the output argument n, enabling further functions to only process the ‘useful’'),\n",
       " Document(page_content='field\\nis non-zero. In addition, the function counts the number of non-zero entries to the output. This is\\npassed as the output argument n, enabling further functions to only process the ‘useful’ data.\\nVivado(cid:13)R HLS can decide to automatically inline functions in order to generate a more\\nefficient architecture. Most often, this happens for small functions. The directive inline allows\\n202HUFFMAN ENCODING\\nthe user to explicitly specify whether or not Vivado(cid:13)R HLS should inline particular functions.\\nIn this case, INLINE off ensures that this function will not be inlined and will appear as a\\nmodule in the generated register-transfer level (RTL) design. In this case, disabling inlining\\nallows us to get a performance and resource usage for this function and to ensure that it will\\nbe implemented as a process in the toplevel dataflow design.\\n11.2.2 Sort\\nThe sort function, shown in Figure 11.6, orders the input symbols based on their frequency values.\\nThe function itself consists of'),\n",
       " Document(page_content='implemented as a process in the toplevel dataflow design.\\n11.2.2 Sort\\nThe sort function, shown in Figure 11.6, orders the input symbols based on their frequency values.\\nThe function itself consists of two for loops, labeled copy in to sorting and radix sort.\\nThe copy in to sorting loop moves input data from the in array into the sorting array. This\\nensures that the in array is read-only to meet the requirements of the dataflow directive used at\\nthe toplevel. The sorting function reads and writes to the sorting array throughout its execution.\\nEven for simple loops like this, it is important to use the pipeline directive to generate the most\\nefficient result and accurate performance estimates.\\nThe radix sort loop implements the core radix-sorting algorithm. In general, radix sorting algo-\\nrithmssortdatabyconsideringonedigitorgroupofbitsatatime. Thesizeofeachdigitdetermines\\nthe radix of the sort. Our algorithm considers 4 bits at a time of the 32-bit Symbol.frequency vari-\\nable. Hence we'),\n",
       " Document(page_content='Thesizeofeachdigitdetermines\\nthe radix of the sort. Our algorithm considers 4 bits at a time of the 32-bit Symbol.frequency vari-\\nable. Hence we are using radix r = 24 = 16 sort. For each 4-bit digit in the 32-bit number, we\\nperform a counting sort. The radix sort loop performs these 8 counting sort operations, iterating\\nto 32 in steps of 4. Radix-sorting algorithms can also operate from left to right (least significant\\ndigitfirst) orright toleft(most significantdigit first). The algorithmimplementedhereworks from\\nleast significant digit to most significant digit. In the code, the radix can be configured by setting\\nthe RADIX and BITS PER LOOP parameters.\\nWhat would happen if we increased or decreased the radix? How would this effect the\\nnumber of counting sort operations that are performed? How would this change the resource\\nusage, e.g., the size of the arrays?\\nThe code stores the current state of the sort in sorting[] and previous sorting[]. Each iteration of\\nradix sort loop, the'),\n",
       " Document(page_content='How would this change the resource\\nusage, e.g., the size of the arrays?\\nThe code stores the current state of the sort in sorting[] and previous sorting[]. Each iteration of\\nradix sort loop, the current value of sorting[] is copied to previous sorting[] and then the values are\\nsorted as they are copied back into sorting[]. The digit histogram[] and digit location[] arrays are\\nusedinradix sort looptoimplementthecountingsortonaparticulardigit. Thetwoarray partition\\ns declare that these two arrays should be completely partitioned into registers. These arrays are\\nsmall and used frequently, thus this does not use many resources and can provide performance\\nbenefits. Finally, current digit[] stores the digit being sorted for each item in the current iteration\\nof the radix sort.\\nThiscodealsocontainstwoassert()callswhichcheckassumptionsabouttheinputnum symbols.\\nSince this variable determines the number of valid elements in the in array, it must be bounded by\\nthesizeofthatarray.'),\n",
       " Document(page_content='symbols.\\nSince this variable determines the number of valid elements in the in array, it must be bounded by\\nthesizeofthatarray. Suchassertionsaregooddefensiveprogrammingpracticeingeneraltoensure\\nthattheassumptionsofthisfunctionaremet. InVivado(cid:13)R HLStheyserveanadditionalpurposeas\\nwell. Since num symbols determines the number of times that many of the internal loops execute,\\nVivado(cid:13)R HLS can infer the tripcount of the loop based on these assertions. In addition, Vivado(cid:13)R\\nHLS also uses these assertions to minimize the bitwidth of the variables used in the implemented\\ncircuit.\\n203HUFFMAN ENCODING\\n#include ”huffman.h”\\n#include ”assert.h”\\nconst unsigned int RADIX = 16;\\nconst unsigned int BITS PER LOOP = 4; // should be log2(RADIX)\\ntypedef ap uint<BITS PER LOOP> Digit;\\nvoid sort(\\n/∗ input ∗/ Symbol in[INPUT SYMBOL SIZE],\\n/∗ input ∗/ int num symbols,\\n/∗ output ∗/ Symbol out[INPUT SYMBOL SIZE]) {\\nSymbol previous sorting[INPUT SYMBOL SIZE], sorting[INPUT SYMBOL SIZE];\\nap'),\n",
       " Document(page_content='sort(\\n/∗ input ∗/ Symbol in[INPUT SYMBOL SIZE],\\n/∗ input ∗/ int num symbols,\\n/∗ output ∗/ Symbol out[INPUT SYMBOL SIZE]) {\\nSymbol previous sorting[INPUT SYMBOL SIZE], sorting[INPUT SYMBOL SIZE];\\nap uint<SYMBOL BITS> digit histogram[RADIX], digit location[RADIX];\\n#pragma HLS ARRAY PARTITION variable=digit location complete dim=1\\n#pragma HLS ARRAY PARTITION variable=digit histogram complete dim=1\\nDigit current digit[INPUT SYMBOL SIZE];\\nassert(num symbols >= 0);\\nassert(num symbols <= INPUT SYMBOL SIZE);\\ncopy in to sorting:\\nfor(int j = 0; j < num symbols; j++) {\\n#pragma HLS PIPELINE II=1\\nsorting[j] = in[j];\\n}\\nradix sort:\\nfor(int shift = 0; shift < 32; shift += BITS PER LOOP) {\\ninit histogram:\\nfor(int i = 0; i < RADIX; i++) {\\n#pragma HLS pipeline II=1\\ndigit histogram[i] = 0;\\n}\\ncompute histogram:\\nfor(int j = 0; j < num symbols; j++) {\\n#pragma HLS PIPELINE II=1\\nDigit digit = (sorting[j].frequency >> shift) & (RADIX − 1); // Extrract a digit\\ncurrent digit[j] = digit; // Store the current digit'),\n",
       " Document(page_content='j = 0; j < num symbols; j++) {\\n#pragma HLS PIPELINE II=1\\nDigit digit = (sorting[j].frequency >> shift) & (RADIX − 1); // Extrract a digit\\ncurrent digit[j] = digit; // Store the current digit for each symbol\\ndigit histogram[digit]++;\\nprevious sorting[j] = sorting[j]; // Save the current sorted order of symbols\\n204HUFFMAN ENCODING\\n}\\ndigit location[0] = 0;\\nfind digit location:\\nfor(int i = 1; i < RADIX; i++)\\n#pragma HLS PIPELINE II=1\\ndigit location[i] = digit location[i−1] + digit histogram[i−1];\\nre sort:\\nfor(int j = 0; j < num symbols; j++) {\\n#pragma HLS PIPELINE II=1\\nDigit digit = current digit[j];\\nsorting[digit location[digit]] = previous sorting[j]; // Move symbol to new sorted location\\nout[digit location[digit]] = previous sorting[j]; // Also copy to output\\ndigit location[digit]++; // Update digit location\\n}\\n}\\n}\\nFigure11.6: Thesortfunctionemploysaradixsortontheinputsymbolsbasedupontheirfrequency\\nvalues.\\nPreviously we’ve seen the loop tripcount directive used to give Vivado(cid:13)R'),\n",
       " Document(page_content='Update digit location\\n}\\n}\\n}\\nFigure11.6: Thesortfunctionemploysaradixsortontheinputsymbolsbasedupontheirfrequency\\nvalues.\\nPreviously we’ve seen the loop tripcount directive used to give Vivado(cid:13)R HLS information\\nabout the tripcount of loops. Using assert() statements serves many of the same purposes,\\nwith some advantages and disadvantages. One advantage of using assert() statements is that\\nthey are checked during simulation and this information can be used to further optimize the\\ncircuit. In contrast, the loop tripcount directive only affects performance analysis and is not\\nused for optimization. On the other hand, assert() statements can only be used to give bounds\\non variable values, but can’t be used to set expected or average values, which can only be\\ndone through the loop tripcount directive. In most cases, it is recommended to first provide\\nworst case bounds through assert() statements, and then if necessary also add loop tripcount\\ndirectives.\\nThe body of the radix sort loop'),\n",
       " Document(page_content='directive. In most cases, it is recommended to first provide\\nworst case bounds through assert() statements, and then if necessary also add loop tripcount\\ndirectives.\\nThe body of the radix sort loop is divided into four subloops, labeled init histogram,\\ncompute histogram, find digit location, and re sort. init histogram and compute histogram loops\\ncombine to compute the histogram of the input, based on the current digit being considered.\\nThis produces a count of the number of each times each digit occurs in digit histogram[]. The\\ncompute histogram loop also stores the current digit being sorted for each symbol in current digit[].\\nNext, the find digit location loop computes a prefix sum of the resulting histogram values, placing\\nthe result in digit location[]. In the context of the counting sort, digit location[] contains the\\nlocation of the first symbol with each digit in the newly sorted array. Lastly, the re sort loop\\nreorders the symbols based upon these results, placing each element'),\n",
       " Document(page_content='sort, digit location[] contains the\\nlocation of the first symbol with each digit in the newly sorted array. Lastly, the re sort loop\\nreorders the symbols based upon these results, placing each element in its correct place in the\\nnewly sorted array. It uses the key stored in current digit[] to select the right location from\\ndigit location[]. This location is incremeted each time through the re sort loop to place the next\\nelement with the same digit in the next location in the sorted array. Overall, each iteration\\n205HUFFMAN ENCODING\\nthrough the radix sort loop implements a counting sort on one digit. The counting sort is a stable\\nsort, so that elements with the same digit remain in the same order. After stable-sorting based on\\neach digit, the array is returned in the correct final order.\\nWe have previous discussed the histogram and prefix sum algorithms in Chapter 8.2 and 8.1.\\nIn this case, with simple code and complete partitioning of digit histogram[] and digit location[], we\\ncan'),\n",
       " Document(page_content='have previous discussed the histogram and prefix sum algorithms in Chapter 8.2 and 8.1.\\nIn this case, with simple code and complete partitioning of digit histogram[] and digit location[], we\\ncan achieve a loop II of 1 to compute the histogram and prefix sum, since the number of bins is\\nrelativelysmall. Theoptimizationofthere sortloopissimilar. Sincetheonlyrecurrenceisthrough\\nthe relatively small digit location[] array, achieving a loop II of 1 is also straightforward. Note\\nthat this approach works primarily because we’ve configured RADIX to be relatively small. With\\nlarger values of RADIX, it would be preferable to implement digit histogram[] and digit location[] as\\nmemories, which might require additional optimization to achieve a loop II of 1.\\nAnother alternative that may make sense in the context of this code is to combine complete\\npartitioning of digit histogram[] and digit location[] with complete unrolling of the init histogram\\nand find digit location loops. These loops access'),\n",
       " Document(page_content='the context of this code is to combine complete\\npartitioning of digit histogram[] and digit location[] with complete unrolling of the init histogram\\nand find digit location loops. These loops access each location in these small arrays and perform\\noperations with a minimal amount of logic. In this case, although unrolling loops would likely\\nresult in replicating the circuit for each loop body, fewer resources would be required to implement\\nthis circuit since the array accesses would be at constant indexes. However, for larger values of the\\nBITS PER LOOP parameter this change becomes prohibitive, since each additional bit doubles the\\nRADIX parameter, doubling the cost of these unrolled loops. This is a somewhat common situation\\nwith parameterized code where different optimizations make sense with different parameter values.\\nWhat happens to the performance and utilization results when you perform the optimiza-\\ntions on the prefix sum and histogram loops as specified in Chapter 8.2 and'),\n",
       " Document(page_content='with different parameter values.\\nWhat happens to the performance and utilization results when you perform the optimiza-\\ntions on the prefix sum and histogram loops as specified in Chapter 8.2 and 8.1? Are these\\noptimizations necessary in this case?\\nIs the re sort for loop able to achieve the specified initiation interval of one cycle? Why or\\nwhy not?\\nFor a large dataset (n > 256), what is the approximate latency, in terms of n, of the code\\nin Figure 11.6. What portions of the code dominate the number of cycles? How would this\\nchange as the RADIX parameter changes?\\nNote that the re sort loop not only stores the sorted arrray in sorting[] but also stores the sorted\\narray in out[]. While this may seem redundant, we need to ensure that out[] is only written to\\nin order to obey the requirements of the toplevel dataflow directive. In this case, out[] will be\\noverwritten multiple times with partially sorted results, but only the final result will be passed on\\nthe following'),\n",
       " Document(page_content='the requirements of the toplevel dataflow directive. In this case, out[] will be\\noverwritten multiple times with partially sorted results, but only the final result will be passed on\\nthe following function.\\nThedataflowdirectivehasseveralrequirementsinordertoperformthetasklevelpipelining\\noptimization. One of them is the need for single producer and consumer of data between the\\ntasks. Since we would like to perform task level pipelining for the Huffman encoding process as\\nshown in Figure 11.4, we must insure that each of these tasks follow this requirement. In the\\ncase of this sort function, which is one of the tasks, it must only consume (read from but not\\n206HUFFMAN ENCODING\\nwrite to) the input argument data and only produce (write to but not read from) the output\\nargument data. In order to met this requirement, we create the internal array sorting, which is\\nread from and written to throughout the function. We copy the input data from the argument\\nin at the beginning of the function'),\n",
       " Document(page_content='to met this requirement, we create the internal array sorting, which is\\nread from and written to throughout the function. We copy the input data from the argument\\nin at the beginning of the function and write the final results to the output argument out at\\nthe end of the function. This insures that we follow the producer/consumer requirements for\\nthe dataflow directive.\\n11.2.3 Create Tree\\nThe next function in the Huffman encoding process forms the binary tree representing the Huff-\\nman code. This is implemented in the create tree function shown in Figure 11.8. in[] contains\\nnum symbols Symbol elements, sorted from lowest to highest frequency. The function creates a\\nbinary tree of those symbols which is stored into three output arrays named parent, left, and right.\\nThe left and right arrays represent the left and right children of each intermediate node in the tree.\\nIf the child is a leaf node, then the corresponding element of the left or right array will contain the\\nsymbol value of'),\n",
       " Document(page_content='represent the left and right children of each intermediate node in the tree.\\nIf the child is a leaf node, then the corresponding element of the left or right array will contain the\\nsymbol value of the child, otherwise it contains the special flag INTERNAL NODE. Similarly, the\\nparent array holds the index of the parent node of each intermediate node. The parent of the root\\nnode of the tree is defined to be index zero. The tree is also ordered, in the sense that a parent\\nalways has a higher index than its children. As a result, we can efficiently implement bottom-up\\nand top-down traversals of the tree.\\nFigure 11.7 shows an example of these data structures. Six symbols sorted by their frequencies\\nare stored in the in array. The resulting Huffman tree is stored in three arrays parent, left, and\\nright. In addition, the frequency of each intermediate node is stored in the frequency array. We\\ndirectly denote the node numbers for the left and right arrays (e.g., n0, n1, etc.) for the sake'),\n",
       " Document(page_content='and\\nright. In addition, the frequency of each intermediate node is stored in the frequency array. We\\ndirectly denote the node numbers for the left and right arrays (e.g., n0, n1, etc.) for the sake of\\nillustration. These will hold a special internal node value in reality.\\nWhile it may be odd to think of storing a complex data structure in a tree like this, it is\\nactually very common in embedded programming where data allocation is not allowed[53]. In\\nfact, the C library implementations of malloc() and free() often implement low-level memory\\nmanagement in this way to enable small allocations to be created from larger memory alloca-\\ntions, usually called pages, returned from the operating system. This enables the operating\\nsystem to efficiently manage large allocations of memory efficiently and to coordinate virtual\\nmemory using the processor page table and disk storage which usually handle large blocks of\\ndata. 4 Kilo-bytes is a typical size for these pages. For more ideas about'),\n",
       " Document(page_content='and to coordinate virtual\\nmemory using the processor page table and disk storage which usually handle large blocks of\\ndata. 4 Kilo-bytes is a typical size for these pages. For more ideas about implementing data\\nstructures using arrays, see [58].\\nIn the Huffman tree, each symbol is associated with a leaf node in the tree. Intermediate nodes\\nin the tree are created by grouping the two symbols with the smallest frequency and using them as\\nthe left and right nodes of a new intermediate node. That intermediate node has a frequency which\\nis the sum of the frequencies of each child node. This process continues by iteratively creating\\nintermediate nodes from the two nodes with the smallest frequencies, which may include other\\nintermediate nodes or leaf nodes. The tree building process completes when all of the intermediate\\nnodes have been incorporated into the binary tree.\\nThere are many ways to represent this process in code. For instance, we might explicitly create\\nan array representing'),\n",
       " Document(page_content='when all of the intermediate\\nnodes have been incorporated into the binary tree.\\nThere are many ways to represent this process in code. For instance, we might explicitly create\\nan array representing every node in the tree which is sorted by frequency. In this case selecting\\n207HUFFMAN ENCODING\\nn4 Intermediate: 2 4 7 10 17\\nIn: S F 17\\n0 1\\nF 1 n2 n3 Left: B C A D n2\\n7 10\\n0 1 0 1\\nB 1\\nn1\\nA 4 D E Right:\\nF n0 n1 E n3\\nC 2 0 1\\nn0\\nA 3 C 2\\n0 1\\nParent:\\n1 2 4 4 0\\nD 5\\nB F\\nE 5\\nFigure 11.7: The Symbol array in is used to create the Huffman tree. The tree is shown graphically\\nalong with the corresponding values for the four arrays used to represent the tree (intermediate,\\nleft, right, and parent).\\nnodes to add to the tree is simple, since they will always be in the same locations of the sorted\\narray. On the other hand, inserting a newly created node into the list is relatively complex because\\nthe array must again be sorted, which would require moving elements around. Alternatively, we\\nmight add'),\n",
       " Document(page_content='On the other hand, inserting a newly created node into the list is relatively complex because\\nthe array must again be sorted, which would require moving elements around. Alternatively, we\\nmight add pointer-like array indexes to the data structure in our array to enable the data to be\\nlogically sorted without actually moving the data around. This would reduce data copying, but\\nwould increase the cost of accessing each element and require extra storage. Many of the normal\\nalgorithmic tradeoffs in the design of data structures apply in the context of HLS just as well as\\nthey apply to processors.\\nIn this case, however, we can make some additional simplifying observations. The most im-\\nportant observation is that new intermediate nodes are always created in order of frequency. We\\nmight create an intermediate node with a frequency that is less than the frequency of some leaf\\nnode, but we will never create an intermediate node with a frequency less than an already created\\nintermediatenode.'),\n",
       " Document(page_content='an intermediate node with a frequency that is less than the frequency of some leaf\\nnode, but we will never create an intermediate node with a frequency less than an already created\\nintermediatenode. Thissuggeststhatwecanmaintainasorteddatastructurebystoringthenodes\\nin two separate arrays: a sorted array of symbols and a sorted array of intermediate nodes. As we\\n‘use’ the lowest frequency elements of each list, we only need to append to the end of the list of\\nintermediate nodes. There is a small extra complexity because we might need to remove zero, one,\\nor two elements from either array, but this turns out to be much less complex than resorting the\\nnode array.\\nConceptually this algorithm is very similar to the mergesort algorithm discussed in Section\\n10.3. The key difference is what operation is done as elements are removed from the sorted\\narrays. In mergesort, the least element is simply inserted at the appropriate position in the\\narray. In this case, the two least elements are'),\n",
       " Document(page_content='operation is done as elements are removed from the sorted\\narrays. In mergesort, the least element is simply inserted at the appropriate position in the\\narray. In this case, the two least elements are identified and then merged into a new tree node.\\nThis code to implement the create tree function is shown in Figure 11.8. The first block of code\\ndefines the local variables that we use in the function. frequency[] stores the frequencies for each\\nintermediate node as it is created. in count tracks which symbols have been given a parent node in\\nthe tree, while tree count tracks which newly created intermediate nodes have been given a parent\\n208HUFFMAN ENCODING\\n#include ”huffman.h”\\n#include ”assert.h”\\nvoid create tree (\\n/∗ input ∗/ Symbol in[INPUT SYMBOL SIZE],\\n/∗ input ∗/ int num symbols,\\n/∗ output ∗/ ap uint<SYMBOL BITS> parent[INPUT SYMBOL SIZE−1],\\n/∗ output ∗/ ap uint<SYMBOL BITS> left[INPUT SYMBOL SIZE−1],\\n/∗ output ∗/ ap uint<SYMBOL BITS> right[INPUT SYMBOL SIZE−1]) {\\nFrequency'),\n",
       " Document(page_content='output ∗/ ap uint<SYMBOL BITS> parent[INPUT SYMBOL SIZE−1],\\n/∗ output ∗/ ap uint<SYMBOL BITS> left[INPUT SYMBOL SIZE−1],\\n/∗ output ∗/ ap uint<SYMBOL BITS> right[INPUT SYMBOL SIZE−1]) {\\nFrequency frequency[INPUT SYMBOL SIZE−1];\\nap uint<SYMBOL BITS> tree count = 0; // Number of intermediate nodes assigned a parent.\\nap uint<SYMBOL BITS> in count = 0; // Number of inputs consumed.\\nassert(num symbols > 0);\\nassert(num symbols <= INPUT SYMBOL SIZE);\\nfor(int i = 0; i < (num symbols−1); i++) {\\n#pragma HLS PIPELINE II=5\\nFrequency node freq = 0;\\n// There are two cases.\\n// Case 1: remove a Symbol from in[]\\n// Case 2: remove an element from intermediate[]\\n// We do this twice, once for the left and once for the right of the new intermediate node.\\nassert(in count < num symbols || tree count < i);\\nFrequency intermediate freq = frequency[tree count];\\nSymbol s = in[in count];\\nif((in count < num symbols && s.frequency <= intermediate freq) || tree count == i) {\\n// Pick symbol from in[].\\nleft[i] ='),\n",
       " Document(page_content='intermediate freq = frequency[tree count];\\nSymbol s = in[in count];\\nif((in count < num symbols && s.frequency <= intermediate freq) || tree count == i) {\\n// Pick symbol from in[].\\nleft[i] = s.value; // Set input symbol as left node\\nnode freq = s.frequency; // Add symbol frequency to total node frequency\\nin count++; // Move to the next input symbol\\n} else {\\n// Pick internal node without a parent.\\nleft[i] = INTERNAL NODE; // Set symbol to indicate an internal node\\nnode freq = frequency[tree count]; // Add child node frequency\\nparent[tree count] = i; // Set this node as child’s parent\\ntree count++; // Go to next parentless internal node\\n}\\nassert(in count < num symbols || tree count < i);\\nintermediate freq = frequency[tree count];\\ns = in[in count];\\nif((in count < num symbols && s.frequency <= intermediate freq) || tree count == i) {\\n209HUFFMAN ENCODING\\n// Pick symbol from in[].\\nright[i] = s.value;\\nfrequency[i] = node freq + s.frequency;\\nin count++;\\n} else {\\n// Pick internal node without a'),\n",
       " Document(page_content='intermediate freq) || tree count == i) {\\n209HUFFMAN ENCODING\\n// Pick symbol from in[].\\nright[i] = s.value;\\nfrequency[i] = node freq + s.frequency;\\nin count++;\\n} else {\\n// Pick internal node without a parent.\\nright[i] = INTERNAL NODE;\\nfrequency[i] = node freq + intermediate freq;\\nparent[tree count] = i;\\ntree count++;\\n}\\n// Verify that nodes in the tree are sorted by frequency\\nassert(i == 0 || frequency[i] >= frequency[i−1]);\\n}\\nparent[tree count] = 0; //Set parent of last node (root) to 0\\n}\\nFigure 11.8: The complete code for Huffman tree creation. The code takes as input the sorted\\nSymbol array in, the number of elements in that array n, and outputs the Huffman tree in the three\\narrays left, right, and parent.\\nnode. Each iteration through the main loop creates a new intermediate node without a parent, so\\nall of the intermediate nodes between tree count and i have not yet been assigned a parent in the\\ntree.\\nThe main loop contains two similar blocks of code. Each block compares the'),\n",
       " Document(page_content='without a parent, so\\nall of the intermediate nodes between tree count and i have not yet been assigned a parent in the\\ntree.\\nThe main loop contains two similar blocks of code. Each block compares the frequency of the\\nnext available symbol in[in count].frequency with the frequency of the next available intermediate\\nnode frequency[tree count]. It then selects the lowest frequency of the two to be incorporated as the\\nleaf of a new intermediate node. The first block does this for the left child of the new node, storing\\nin left[i], while the second block selects the right child of the new node, storing in right[i]. In both\\ncases, we need to be careful to ensure that the comparison is meaningful. In the first iteration of\\nthe loop, tree count == 0 and i == 0, so there is no valid intermediate node to be considered and\\nwe must always select an input symbol. During the final iterations of the loop, it is likely that all\\nof the input symbols will have been consumed, so in count == num symbols'),\n",
       " Document(page_content='node to be considered and\\nwe must always select an input symbol. During the final iterations of the loop, it is likely that all\\nof the input symbols will have been consumed, so in count == num symbols and we must always\\nconsume an intermediate node.\\nThe number of iterations of the loop depends on the input num symbols in an interesting way.\\nSince each input symbol becomes a leaf node in the binary tree, we know that there will be exactly\\nnum symbols−1 intermediate nodes to be created, since this is a basic property of a binary tree. At\\nthe end of the loop we will have created num symbols−1 new nodes, each of which has two children.\\nnum symbolsofthesechildrenwillbeinputsymbolsandnum symbols−2willbeintermediatenodes.\\nThere will be one intermediate node remaining as the root of the tree without a parent. This last\\nnode is artificially assigned a parent index of zero in the last line of code. This completes the\\nbuilding of the Huffman tree.\\nIn the tree, the children of an intermediate node'),\n",
       " Document(page_content='a parent. This last\\nnode is artificially assigned a parent index of zero in the last line of code. This completes the\\nbuilding of the Huffman tree.\\nIn the tree, the children of an intermediate node can be either a symbol or a intermediate node.\\nIn creating the huffman tree, this information isn’t very important, although it will be important\\n210HUFFMAN ENCODING\\nlater when we traverse the tree later. To store this difference a special value INTERNAL NODE\\nis stored in left[] and right[] if the corresponding child is an internal node. Note that this storage\\nessentially requires one more bit to represent in the array. As a result, the left[] and right[] arrays\\nare one bit larger than you might expect.\\nFor a large dataset (n > 256), what is the approximate latency, in terms of n, of the code\\nin Figure 11.8? What portions of the code dominate the number of cycles?\\n11.2.4 Compute Bit Length\\nThe compute bit length function determines the depth in the tree for each symbol. The depth'),\n",
       " Document(page_content='the code\\nin Figure 11.8? What portions of the code dominate the number of cycles?\\n11.2.4 Compute Bit Length\\nThe compute bit length function determines the depth in the tree for each symbol. The depth is\\nimportant because it determines the number of bits used to encode each symbol. Computing the\\ndepth of each node in the tree is done using the recurrence:\\ndepth(root) = 0\\n∀n! = root, depth(n) = depth(parent(n)+1) (11.1)\\n∀n, child depth(n) = depth(n)+1\\nThis recurrence can be computed by traversing the tree starting at the root node and exploring\\neach internal node in order. As we traverse each internal node, we can compute the depth of the\\nnode and the corresponding depth (incremented by one) of any child nodes. It turns out that we\\ndon’t actually care about the depth of the internal nodes, only about the depth of the child nodes.\\nAs a result, the code actually computes the recurrence:\\nchild depth(root) = 1\\n(11.2)\\n∀n! = root, child depth(n) = child depth(parent(n)+1)\\nThe code for this'),\n",
       " Document(page_content='only about the depth of the child nodes.\\nAs a result, the code actually computes the recurrence:\\nchild depth(root) = 1\\n(11.2)\\n∀n! = root, child depth(n) = child depth(parent(n)+1)\\nThe code for this function is shown in Figure 11.9. The input arguments to the function\\nrepresent a Huffman tree in parent[], left[], and right[]. num symbols contains the number of input\\nsymbols, which is one more than the number of intermediate nodes in the tree. The output\\nlength histogram[]. Each element of that array stores the number of symbols with the given depth.\\nThus, if there are five symbols with depth three, then length histogram[3] = 5.\\nchild depth[] stores the depth of each internal node while the tree is being traversed. After the\\ndepth of each internal node is determined in the traverse tree loop, length histogram[] is updated.\\ninternal length histogram[] is used to ensure that our function adheres the requirements for the\\ndataflow directive, where the output array length histogram[] is never'),\n",
       " Document(page_content='length histogram[] is updated.\\ninternal length histogram[] is used to ensure that our function adheres the requirements for the\\ndataflow directive, where the output array length histogram[] is never read. The init histogram loop\\ninitializes these two arrays.\\nThe init histogram loop has a pipeline directive with II = 1. Is it possible to meet this II?\\nWhat happens if we increase the II to something larger? What happens if we do not apply this\\ndirective?\\nInternal nodes in the tree are traversed from the root node, which has the largest index, down\\nto index zero. Since the array of nodes were created in bottom-up order, this reverse order results\\nin a top-down traversal of the tree enabling the computation of the recurrence for each node in\\na single pass through the nodes. For each node, we determine the depth of its children. Then if\\nthe node actually does have any children which are symbols, we figure out how many children and\\n211HUFFMAN ENCODING\\n#include ”huffman.h”\\n#include'),\n",
       " Document(page_content='node, we determine the depth of its children. Then if\\nthe node actually does have any children which are symbols, we figure out how many children and\\n211HUFFMAN ENCODING\\n#include ”huffman.h”\\n#include ”assert.h”\\nvoid compute bit length (\\n/∗ input ∗/ ap uint<SYMBOL BITS> parent[INPUT SYMBOL SIZE−1],\\n/∗ input ∗/ ap uint<SYMBOL BITS> left[INPUT SYMBOL SIZE−1],\\n/∗ input ∗/ ap uint<SYMBOL BITS> right[INPUT SYMBOL SIZE−1],\\n/∗ input ∗/ int num symbols,\\n/∗ output ∗/ ap uint<SYMBOL BITS> length histogram[TREE DEPTH]) {\\nassert(num symbols > 0);\\nassert(num symbols <= INPUT SYMBOL SIZE);\\nap uint<TREE DEPTH BITS> child depth[INPUT SYMBOL SIZE−1];\\nap uint<SYMBOL BITS> internal length histogram[TREE DEPTH];\\ninit histogram:\\nfor(int i = 0; i < TREE DEPTH; i++) {\\n#pragma HLS pipeline II=1\\ninternal length histogram[i] = 0;\\n}\\nchild depth[num symbols−2] = 1; // Depth of the root node is 1.\\ntraverse tree:\\nfor(int i = num symbols−3; i >= 0; i−−) {\\n#pragma HLS pipeline II=3\\nap uint<TREE DEPTH BITS> length ='),\n",
       " Document(page_content='histogram[i] = 0;\\n}\\nchild depth[num symbols−2] = 1; // Depth of the root node is 1.\\ntraverse tree:\\nfor(int i = num symbols−3; i >= 0; i−−) {\\n#pragma HLS pipeline II=3\\nap uint<TREE DEPTH BITS> length = child depth[parent[i]] + 1;\\nchild depth[i] = length;\\nif(left[i] != INTERNAL NODE || right[i] != INTERNAL NODE){\\nint children;\\nif(left[i] != INTERNAL NODE && right[i] != INTERNAL NODE) {\\n// Both the children of the original node were symbols\\nchildren = 2;\\n} else {\\n// One child of the original node was a symbol\\nchildren = 1;\\n}\\nap uint<SYMBOL BITS> count = internal length histogram[length];\\ncount += children;\\ninternal length histogram[length] = count;\\nlength histogram[length] = count;\\n}\\n}\\n}\\nFigure 11.9: The complete code for determining the number of symbols at each bit length.\\n212HUFFMAN ENCODING\\nupdate the histogram accordingly. Child nodes which are internal nodes are represented by the\\nspecial value INTERNAL NODE.\\nFor a large dataset (n > 256), what is the approximate latency, in terms'),\n",
       " Document(page_content='the histogram accordingly. Child nodes which are internal nodes are represented by the\\nspecial value INTERNAL NODE.\\nFor a large dataset (n > 256), what is the approximate latency, in terms of n, of the code\\nin Figure 11.9? What portions of the code dominate the number of cycles?\\nThis code has several recurrences. For example, one recurrence occurs because of the\\nhistogram computation. In this case, the loop is synthesized with an II of 3. What happens\\nif you target a lower II in the pipeline directive? Can you rewrite the code to eliminate the\\nrecurrences and achieve a lower II?\\n11.2.5 Truncate Tree\\nThe next part of the Huffman encoding process reorganizes nodes with a depth that is larger than\\nthat specified in MAX CODEWORD LENGTH. This is done by finding any symbols with a greater\\ndepth, and moving them to a level that is smaller than that specified target. Interestingly, this\\ncan be done entirely by manipulating the histogram of symbol depths, as long as the histogram is\\nmodified in'),\n",
       " Document(page_content='and moving them to a level that is smaller than that specified target. Interestingly, this\\ncan be done entirely by manipulating the histogram of symbol depths, as long as the histogram is\\nmodified in a way that is consistent with the same modifications on the original tree.\\nThe input histogram is contained in input length histogram, which was derived by the\\ncompute bit length() function described in the previous section. There are two identical output\\narrays truncated length histogram1 and truncated length histogram2. These arrays are passed to\\ntwo separate functions later in the process (canonize tree and create codewords), and thus we must\\nhave two arrays to adhere to the single producer, single consumer constraint of the dataflow\\ndirective.\\nThe code is shown in Figure 11.10. The copy input loop copies the data from the input array\\ninput length histogram. The move nodes loop contains the bulk of the processing to modify the\\nhistogram. Lastly, the input length histogram function'),\n",
       " Document(page_content='input loop copies the data from the input array\\ninput length histogram. The move nodes loop contains the bulk of the processing to modify the\\nhistogram. Lastly, the input length histogram function copies the internal result to other output at\\nthe end of the function.\\nThe copy in for loop is not optimized. What happens to the latency and initiation interval\\nof the truncate tree function if we use a pipeline or unroll directive on this loop. What happens\\nto the overall latency and initiation interval of the design (i.e., the huffman encoding function)?\\nThe function continues in the second move nodes for loop, which performs the bulk of the\\ncomputation. This for loop starts by iterating through the truncated length histogram array from\\nthe largest index (TREE DEPTH - the specified maximum depth for a tree). This continues down\\nthrough the array until there is a non-zero element or i reaches the MAX CODEWORD LENGTH.\\nIf we do not find a non-zero element, that means the initial input Huffman'),\n",
       " Document(page_content='for a tree). This continues down\\nthrough the array until there is a non-zero element or i reaches the MAX CODEWORD LENGTH.\\nIf we do not find a non-zero element, that means the initial input Huffman tree does not have any\\nnodes with a depth larger than the target depth. In other words, we can exit this function without\\nperforming any truncation. If there is a value larger than the target depth, then the function\\ncontinues by reorganizing the tree so that all of the nodes have depth smaller than the target\\ndepth. This is done by the operations in the reorder while loop. When there are nodes to move,\\nthe move nodes for loop goes through them from those with the largest depth, and continues to\\nsmaller depths until all nodes are rearranged with a depth smaller than the target. Each iteration\\nof this move nodes for loops works on moving nodes from one depth at a time.\\n213HUFFMAN ENCODING\\n#include ”huffman.h”\\n#include ”assert.h”\\nvoid truncate tree(\\n/∗ input ∗/ ap uint<SYMBOL BITS> input'),\n",
       " Document(page_content='this move nodes for loops works on moving nodes from one depth at a time.\\n213HUFFMAN ENCODING\\n#include ”huffman.h”\\n#include ”assert.h”\\nvoid truncate tree(\\n/∗ input ∗/ ap uint<SYMBOL BITS> input length histogram[TREE DEPTH],\\n/∗ output ∗/ ap uint<SYMBOL BITS> output length histogram1[TREE DEPTH],\\n/∗ output ∗/ ap uint<SYMBOL BITS> output length histogram2[TREE DEPTH]\\n) {\\n// Copy into temporary storage to maintain dataflow properties\\ncopy input:\\nfor(int i = 0; i < TREE DEPTH; i++) {\\noutput length histogram1[i] = input length histogram[i];\\n}\\nap uint<SYMBOL BITS> j = MAX CODEWORD LENGTH;\\nmove nodes:\\nfor(int i = TREE DEPTH − 1; i > MAX CODEWORD LENGTH; i−−) {\\n// Look to see if there is any nodes at lengths greater than target depth\\nreorder:\\nwhile(output length histogram1[i] != 0) {\\n#pragma HLS LOOP TRIPCOUNT min=3 max=3 avg=3\\nif (j == MAX CODEWORD LENGTH) {\\n// Find deepest leaf with codeword length < target depth\\ndo {\\n#pragma HLS LOOP TRIPCOUNT min=1 max=1 avg=1\\nj−−;\\n} while(output length'),\n",
       " Document(page_content='LOOP TRIPCOUNT min=3 max=3 avg=3\\nif (j == MAX CODEWORD LENGTH) {\\n// Find deepest leaf with codeword length < target depth\\ndo {\\n#pragma HLS LOOP TRIPCOUNT min=1 max=1 avg=1\\nj−−;\\n} while(output length histogram1[j] == 0);\\n}\\n// Move leaf with depth i to depth j+1.\\noutput length histogram1[j] −= 1; // The node at level j is no longer a leaf.\\noutput length histogram1[j+1] += 2; // Two new leaf nodes are attached at level j+1.\\noutput length histogram1[i−1] += 1; // The leaf node at level i+1 gets attached here.\\noutput length histogram1[i] −= 2; // Two leaf nodes have been lost from level i.\\n// now deepest leaf with codeword length < target length\\n// is at level (j+1) unless j+1 == target length\\nj++;\\n}\\n}\\n214HUFFMAN ENCODING\\n// Copy the output to meet dataflow requirements and check the validity\\nunsigned int limit = 1;\\ncopy output:\\nfor(int i = 0; i < TREE DEPTH; i++) {\\noutput length histogram2[i] = output length histogram1[i];\\nassert(output length histogram1[i] >= 0);\\nassert(output length'),\n",
       " Document(page_content='int limit = 1;\\ncopy output:\\nfor(int i = 0; i < TREE DEPTH; i++) {\\noutput length histogram2[i] = output length histogram1[i];\\nassert(output length histogram1[i] >= 0);\\nassert(output length histogram1[i] <= limit);\\nlimit ∗= 2;\\n}\\n}\\nFigure 11.10: The complete code for rearranging the Huffman tree such that the depth of any node\\nis under the target specified by the parameter MAX CODEWORD LENGTH.\\nThe reorder while loop moves one node in each iteration. The first if statement is used to find\\nthe leaf node with the largest depth. We will then alter this node by making it an intermediate\\nnode, and adding it and the leaf node with a depth larger than than target as children. This if\\nclause has a do/while loop that iterates downward from the target looking for a non-zero entry\\nin the truncated length histogram array. It works in a similar manner as the beginning of the\\nmove nodes for loop. When it has found the deepest leaf node less than the target depth, it stops.\\nThe depth of this node is'),\n",
       " Document(page_content='histogram array. It works in a similar manner as the beginning of the\\nmove nodes for loop. When it has found the deepest leaf node less than the target depth, it stops.\\nThe depth of this node is stored in j.\\nNowwehaveanodewithadepthilargerthanthetarget, andanodewithadepthsmallerthan\\nthe target stored in j. We move the node from depth i and a node from j into child nodes at depth\\nj + 1. Therefore, we add two symbols to truncated length histogram[j+1]. We are making a new\\nintermediate node a depth j thus, we subtract a symbol from that level. We move the other leaf\\nnode from depth i to depth i − 1. And we subtract two from truncated length histogram[i] since one\\nof the nodes went to level j + 1 and the other when to level i − 1. These operations are performed\\nin the four statements on the array truncated length histogram. Since we added a symbol to level\\nj + 1, we update j, which holds the highest level under the target level, and then we repeat the\\nprocedure. This is done until there'),\n",
       " Document(page_content='array truncated length histogram. Since we added a symbol to level\\nj + 1, we update j, which holds the highest level under the target level, and then we repeat the\\nprocedure. This is done until there are no additional symbols with a depth larger than the target.\\nThe function completes by creating an additional copy of the new bit lengths. This is\\ndone by storing the updated bit lengths in the array truncated length histogram1 into the array\\ntruncated length histogram2. We will pass these two arrays to the final two functions in the\\nhuffman encoding top function; we need two arrays to insure that the constraints of the dataflow\\ndirective are met.\\n11.2.6 Canonize Tree\\nThe next step in the encoding process is to determine the number of bits for each of the symbols.\\nWedothisinthecanonize treefunctionshowninFigure11.11. Thefunctiontakesasinputanarray\\nof symbols in sorted order, the total number of symbols (num symbols), and a histogram of lengths\\ndescribing the Huffman tree. The output'),\n",
       " Document(page_content='treefunctionshowninFigure11.11. Thefunctiontakesasinputanarray\\nof symbols in sorted order, the total number of symbols (num symbols), and a histogram of lengths\\ndescribing the Huffman tree. The output symbol bits[] contains the number of encoded bits used\\nfor each symbol. Thus, if the symbol with value 0x0A is encoded in 4 bits, then symbol bits[10] = 4.\\nThecanonizationprocessconsistsoftwoloops,labeledinit bitsandprocess symbols. Theinit bits\\nloop executes first, initializing symbol bits[] array to 0. The process symbols loop then processes the\\nsymbols in sorted order from smallest frequency to largest frequency. Naturally, the least frequent\\n215HUFFMAN ENCODING\\n#include ”huffman.h”\\n#include ”assert.h”\\nvoid canonize tree(\\n/∗ input ∗/ Symbol sorted[INPUT SYMBOL SIZE],\\n/∗ input ∗/ ap uint<SYMBOL BITS> num symbols,\\n/∗ input ∗/ ap uint<SYMBOL BITS> codeword length histogram[TREE DEPTH],\\n/∗ output ∗/ CodewordLength symbol bits[INPUT SYMBOL SIZE] ) {\\nassert(num symbols <= INPUT SYMBOL'),\n",
       " Document(page_content='uint<SYMBOL BITS> num symbols,\\n/∗ input ∗/ ap uint<SYMBOL BITS> codeword length histogram[TREE DEPTH],\\n/∗ output ∗/ CodewordLength symbol bits[INPUT SYMBOL SIZE] ) {\\nassert(num symbols <= INPUT SYMBOL SIZE);\\ninit bits:\\nfor(int i = 0; i < INPUT SYMBOL SIZE; i++) {\\nsymbol bits[i] = 0;\\n}\\nap uint<SYMBOL BITS> length = TREE DEPTH;\\nap uint<SYMBOL BITS> count = 0;\\n// Iterate across the symbols from lowest frequency to highest\\n// Assign them largest bit length to smallest\\nprocess symbols:\\nfor(int k = 0; k < num symbols; k++) {\\nif (count == 0) {\\n//find the next non−zero bit length\\ndo {\\n#pragma HLS LOOP TRIPCOUNT min=1 avg=1 max=2\\nlength−−;\\n// n is the number of symbols with encoded length i\\ncount = codeword length histogram[length];\\n}\\nwhile (count == 0);\\n}\\nsymbol bits[sorted[k].value] = length; //assign symbol k to have length bits\\ncount−−; //keep assigning i bits until we have counted off n symbols\\n}\\n}\\nFigure 11.11: The complete code for canonizing the Huffman tree, which determins the number'),\n",
       " Document(page_content='symbol k to have length bits\\ncount−−; //keep assigning i bits until we have counted off n symbols\\n}\\n}\\nFigure 11.11: The complete code for canonizing the Huffman tree, which determins the number of\\nbits for each symbol.\\n216HUFFMAN ENCODING\\nint k = 0;\\nprocess symbols:\\nfor(length = TREE DEPTH; length >= 0; length−−) {\\ncount = codeword length histogram[length];\\nfor(i = 0; i < count; i++) {\\n#pragma HLS pipeline II=1\\nsymbol bits[sorted[k++].value] = length;\\n}\\n}\\nFigure 11.12: Alternate loop structure for the process symbols loop in Figure 11.11.\\nsymbols are assigned the longest codes while the most frequent symbols are assigned the shortest\\ncode. Each time through the process symbols loop, we assign the length of one symbol. The length\\nof the symbol is determined by the the inner do/while loop, which steps through the histogram\\nof lengths. This loop finds the largest bit length that has not yet had codewords assigned and\\nstores the number of codewords in that length in count. Each time'),\n",
       " Document(page_content='loop, which steps through the histogram\\nof lengths. This loop finds the largest bit length that has not yet had codewords assigned and\\nstores the number of codewords in that length in count. Each time through the outer loop, count\\nis decremented until we run out of codewords. When count becomes zero, the inner do/while loop\\nexecutes again to find a length with codewords to assign.\\nNote that the process symbols loop cannot be pipelined because the inner do/while loop cannot\\nbeunrolled. Thisissomewhatawkwardastheinnerloopwillusuallyexecuteexactlyonce,stepping\\ntothenextlengthinthehistogram. Onlyinsomewhatrarecaseswilltheinnerloopneedtoexecute\\nmore than once if we happen to get to a length which does not have any codewords assigned. In\\nthis case, there’s not too much of a loss since all the operations in the loop are simple operations\\nthat are unlikely to be pipelined, with the exception of the memory operations. There are other\\nways to structure this loop, however, which can be pipelined.'),\n",
       " Document(page_content='in the loop are simple operations\\nthat are unlikely to be pipelined, with the exception of the memory operations. There are other\\nways to structure this loop, however, which can be pipelined. One possibility is to use an outer for\\nloop to iterate over codeword length histogram[] and an inner loop to count each symbol, as shown\\nin Figure 11.12.\\nImplementthecodeinFigure11.11andthealternatecodestructureinFigure11.12. Which\\nresults in higher performance? Which coding style is more natural to you?\\n11.2.7 Create Codeword\\nThe final step in the encoding process is to create the codeword for each symbol. This process\\nsimply assigns each symbol in order according to the properties of a Canonical Huffman code. The\\nfirst property is that longer length codes have a higher numeric value than the same length prefix\\nof shorter codes. The second property is that codes with the same length increase by one as the\\nsymbol value increases. In order to achieve these properties while keeping the code simple,'),\n",
       " Document(page_content='prefix\\nof shorter codes. The second property is that codes with the same length increase by one as the\\nsymbol value increases. In order to achieve these properties while keeping the code simple, it is\\nuseful to determine the first codeword of each length. If we know the number of codewords of each\\nlength given by codeword length histogram, then this can be found using the following recurrence:\\nfirst codeword(1) = 0\\n∀i > 1, first codeword(i) = (first codeword(i−1)+codeword length histogram(i−1)) << 1\\n(11.3)\\n217HUFFMAN ENCODING\\nEssentially,ratherthanactuallyassigningthecodewordsoneafteranother,thisrecurrenceallocates\\nall the codewords first. This allows us to actually assign the codewords in order of symbol value\\nwithout being concerned about also ordering them by length or frequency.\\nIn addition to assigning codewords to symbols, we also need to format the codewords so that\\nthey can be easily used for encoding and decoding. Systems that use Huffman encoding often'),\n",
       " Document(page_content='or frequency.\\nIn addition to assigning codewords to symbols, we also need to format the codewords so that\\nthey can be easily used for encoding and decoding. Systems that use Huffman encoding often store\\ncodewordsinbit-reversedorder. Thiscanmakethedecodingprocesseasiersincethebitsarestored\\nin the same order that the tree is traversed during decoding, from root node to leaf node.\\nThe code implementing the create codewords function is shown in Figure 11.13. symbol bits[]\\ncontains the length of the codeword for each symbol and codeword length histogram[] contains the\\nnumber of codewords with each length. The output encoding[] represents the encoding for each\\nsymbol. Each element consists of the actual codeword and the length of each codeword packed\\ntogether. The maximum length of a codeword is given by the MAX CODEWORD LENGTH param-\\neter. In turn, this determines the number of bits required to hold the codeword, which is given by\\nCODEWORD LENGTH BITS. The CODEWORD LENGTH BITS least'),\n",
       " Document(page_content='is given by the MAX CODEWORD LENGTH param-\\neter. In turn, this determines the number of bits required to hold the codeword, which is given by\\nCODEWORD LENGTH BITS. The CODEWORD LENGTH BITS least significant bits of each ele-\\nment in the encoding array contains the same value received from the input array symbol bits. The\\nhighorderMAX CODEWORD LENGTHbitsofeachencodingelementcontainstheactualcodeword.\\nUsing 27 bits for MAX CODEWORD LENGTH resulting in CODEWORD LENGTH BITS of 5 is a\\nparticularly useful combination, since each element of encoding[] fits in a single 32-bit word.\\nThe code consists primarily of two loops, labeled first codewords and assign codewords. The\\nfirst codewords loop finds the first codeword with each length, implementing the recurrence in\\nEquation 11.3. The assign codewords loop finally associates each symbol with a codeword. The\\ncodeword is found using the length of each codeword and indexing into the correct element of\\nfirst codeword[]. The main complexity of this'),\n",
       " Document(page_content='loop finally associates each symbol with a codeword. The\\ncodeword is found using the length of each codeword and indexing into the correct element of\\nfirst codeword[]. The main complexity of this code is in the bit reversal process, which is based on\\nthe bit reverse32 function. We have talked about this function previously in the FFT chapter (see\\nChapter 5.3), so we will not discuss it here again. After reversing the bits in the codeword, the\\nnext statement removes the least significant ’0’ bits leaving only the bit-reversed codeword. The\\nbit-reversed codeword is then packed in the high-order bits together with the length of the symbol\\nin the low-order bits and stored in encoding[]. Lastly, the value in first codeword[] is incremented.\\nIn the code in Figure 11.13, the inputs actually contain some redundant information.\\nIn particular, we could compute the number of symbols for each bit length stored in\\ncodeword length histogram[] from the length of each codeword symbol bits[] using a'),\n",
       " Document(page_content='some redundant information.\\nIn particular, we could compute the number of symbols for each bit length stored in\\ncodeword length histogram[] from the length of each codeword symbol bits[] using a histogram\\ncomputation. Instead, in this code we’ve chosen to reuse the histogram originally computed in\\nthe truncate tree() function. Instead we could save the storage by recomputing the histogram.\\nDo you think this is a good tradeoff? How many resources are required to compute the his-\\ntogram in this function? How many resources are required to communicate the histogram\\nthrough the pipeline?\\nEstimate the latency of the code in Figure 11.13\\nLet us now go through our running example and show how this is used to derive the initial\\ncodewords. In the example, the symbols A, D, and E have two bits for their encoding; symbol C has\\n218HUFFMAN ENCODING\\n#include ”huffman.h”\\n#include ”assert.h”\\n#include <iostream>\\nvoid create codeword(\\n/∗ input ∗/ CodewordLength symbol bits[INPUT SYMBOL SIZE],\\n/∗ input'),\n",
       " Document(page_content='their encoding; symbol C has\\n218HUFFMAN ENCODING\\n#include ”huffman.h”\\n#include ”assert.h”\\n#include <iostream>\\nvoid create codeword(\\n/∗ input ∗/ CodewordLength symbol bits[INPUT SYMBOL SIZE],\\n/∗ input ∗/ ap uint<SYMBOL BITS> codeword length histogram[TREE DEPTH],\\n/∗ output ∗/ PackedCodewordAndLength encoding[INPUT SYMBOL SIZE]\\n) {\\nCodeword first codeword[MAX CODEWORD LENGTH];\\n// Computes the initial codeword value for a symbol with bit length i\\nfirst codeword[0] = 0;\\nfirst codewords:\\nfor(int i = 1; i < MAX CODEWORD LENGTH; i++) {\\n#pragma HLS PIPELINE II=1\\nfirst codeword[i] = (first codeword[i−1] + codeword length histogram[i−1]) << 1;\\nCodeword c = first codeword[i];\\n// std::cout << c.to string(2) << ” with length ” << i << ”\\\\n”;\\n}\\nassign codewords:\\nfor (int i = 0; i < INPUT SYMBOL SIZE; ++i) {\\n#pragma HLS PIPELINE II=5\\nCodewordLength length = symbol bits[i];\\n//if symbol has 0 bits, it doesn’t need to be encoded\\nmake codeword:\\nif(length != 0) {\\n// std::cout << first codeword[length].to'),\n",
       " Document(page_content='{\\n#pragma HLS PIPELINE II=5\\nCodewordLength length = symbol bits[i];\\n//if symbol has 0 bits, it doesn’t need to be encoded\\nmake codeword:\\nif(length != 0) {\\n// std::cout << first codeword[length].to string(2) << ”\\\\n”;\\nCodeword out reversed = first codeword[length];\\nout reversed.reverse();\\nout reversed = out reversed >> (MAX CODEWORD LENGTH − length);\\n// std::cout << out reversed.to string(2) << ”\\\\n”;\\nencoding[i] = (out reversed << CODEWORD LENGTH BITS) + length;\\nfirst codeword[length]++;\\n} else {\\nencoding[i] = 0;\\n}\\n}\\n}\\nFigure 11.13: The complete code for generating the canonical Huffman codewords for each of the\\nsymbols. The codewords can be computed with knowledge of number of bits that each sym-\\nbol uses (stored in the input array symbol bits[]). Additionally, we have another input array\\ncodeword length histogram[] which stores at each entry the number of symbols with codewords\\nat that bit length. The output is the code word for each symbol stored in the encoding[] array.\\n219HUFFMAN'),\n",
       " Document(page_content='length histogram[] which stores at each entry the number of symbols with codewords\\nat that bit length. The output is the code word for each symbol stored in the encoding[] array.\\n219HUFFMAN ENCODING\\nthree bits; and symbols B and F have four bits. Thus, we have:\\nbit length(1) = 0\\nbit length(2) = 3\\n(11.4)\\nbit length(3) = 1\\nbit length(4) = 2\\nUsing Equation 11.3 to calculate the values of first codeword, we determine:\\nfirst codeword(1) = 0 = 0b0\\nfirst codeword(2) = (0+0) << 1 = 0b00\\n(11.5)\\nfirst codeword(3) = (0+3) << 1 = 6 = 0b110\\nfirst codeword(4) = (6+1) << 1 = 14 = 0b1110\\nOnce we have determined these values, then consider each symbol in order from smallest to\\nlargest. For each symbol, we determine the length of its codeword and assign the next codeword\\nof the appropriate length. In the running example, we consider symbols A, B, C, D, E, and F\\nin alphabetical order. The symbol A has two bits for its encoding. We perform a lookup into\\nfirst codeword[2] = 0. Thus we assign the codeword'),\n",
       " Document(page_content='running example, we consider symbols A, B, C, D, E, and F\\nin alphabetical order. The symbol A has two bits for its encoding. We perform a lookup into\\nfirst codeword[2] = 0. Thus we assign the codeword for A to 0b00. We increment the value at\\nfirst codeword[2] to 1. The symbol B has four bits. Since first codeword[4] = 14 = 0b1110, it gets\\nassignedthecodeword0b1110. SymbolChasthreebits. Thevalueoffirst codeword[3] = 6 = 0b110,\\nthus it gets the codeword 110. Symbol D has two bits so it gets first codeword[2] = 1 = 0b01;\\nremember that we incremented this value after we assigned the codeword to symbol A. Symbol E\\nhas two bits so it gets the codeword 0b01 + 1 = 0b10. And F has four bits so it gets the codeword\\n0b1110 + 1 = 0b1111.\\nThe final codewords for all of the symbols are:\\nA → 00\\nB → 1110\\nC → 110\\n(11.6)\\nD → 01\\nE → 10\\nF → 1111\\n11.2.8 Testbench\\nThe final part of the code is the testbench. This is shown in Figure 11.14. The general structure is\\nto read the input frequency values from a'),\n",
       " Document(page_content='→ 110\\n(11.6)\\nD → 01\\nE → 10\\nF → 1111\\n11.2.8 Testbench\\nThe final part of the code is the testbench. This is shown in Figure 11.14. The general structure is\\nto read the input frequency values from a file, process them using the huffman encoding function,\\nand compare the resulting codewords with an existing golden reference that is stored in a file.\\nThe main() function starts by setting up the variables required to read the frequencies from a\\nfile (in this case the file is huffman.random256.txt) and puts them into in[]. This is done in the\\nfile to array function, which takes as input the filename for the input data and the length of the\\ndata (array length), and stores the entries in that file into array[] variable. This file contains the\\nfrequency of each symbol. Frequency values are stored in symbol order, thus the first value of the\\nfile represents the frequency of symbol ’0’, and so on.\\nThemain()functioncontinuesbyinitializingin[]usingthefrequenciesfromthefile. Itthencalls\\nthe top'),\n",
       " Document(page_content='in symbol order, thus the first value of the\\nfile represents the frequency of symbol ’0’, and so on.\\nThemain()functioncontinuesbyinitializingin[]usingthefrequenciesfromthefile. Itthencalls\\nthe top huffman encoding function. The function returns the encoded symbol values in encoding[].\\nSince the result of processing should be a prefix code, we check that the properties of a prefix\\ncode are actually satisfied. The result is then compared to the codewords stored in a golden\\nreference, which is stored in the file huffman.random256.gold. We do this by writing the result\\n220HUFFMAN ENCODING\\n#include ”huffman.h”\\n#include <stdio.h>\\n#include <stdlib.h>\\nvoid file to array(const char ∗filename, ap uint<16> ∗&array, int array length) {\\nprintf(”Start reading file [%s]\\\\n”, filename);\\nFILE ∗file = fopen(filename, ”r”);\\nif(file == NULL) {\\nprintf(”Cannot find the input file\\\\n”);\\nexit(1);\\n}\\nint file value = 0;\\nint count = 0;\\narray = (ap uint<16> ∗) malloc(array length∗sizeof(ap uint<16>));\\nwhile(1)'),\n",
       " Document(page_content='”r”);\\nif(file == NULL) {\\nprintf(”Cannot find the input file\\\\n”);\\nexit(1);\\n}\\nint file value = 0;\\nint count = 0;\\narray = (ap uint<16> ∗) malloc(array length∗sizeof(ap uint<16>));\\nwhile(1) {\\nint eof check = fscanf(file, ”%x”, &file value);\\nif(eof check == EOF) break;\\nelse {\\narray[count++] = (ap uint<16>) file value ;\\n}\\n}\\nfclose(file);\\nif(count != array length) exit(1);\\n}\\nint main() {\\nprintf(”Starting canonical Huffman encoding testbench\\\\n”);\\nFILE ∗output file;\\nint return val = 0;\\nap uint<16> ∗frequencies = NULL;\\nfile to array(”huffman.random256.txt”, frequencies, INPUT SYMBOL SIZE);\\nSymbol in[INPUT SYMBOL SIZE];\\nfor (int i = 0 ; i < INPUT SYMBOL SIZE; i++) {\\nin[i].frequency = frequencies[i];\\nin[i].value = i;\\n}\\nint num nonzero symbols;\\n221HUFFMAN ENCODING\\nPackedCodewordAndLength encoding[INPUT SYMBOL SIZE];\\nhuffman encoding(in, encoding, &num nonzero symbols);\\noutput file = fopen(”huffman.random256.out”, ”w”);\\nfor(int i = 0; i < INPUT SYMBOL SIZE; i++)\\nfprintf(output file, ”%d, %x\\\\n”, i,'),\n",
       " Document(page_content='SYMBOL SIZE];\\nhuffman encoding(in, encoding, &num nonzero symbols);\\noutput file = fopen(”huffman.random256.out”, ”w”);\\nfor(int i = 0; i < INPUT SYMBOL SIZE; i++)\\nfprintf(output file, ”%d, %x\\\\n”, i, (unsigned int) encoding[i]);\\nfclose(output file);\\nprintf (”\\\\n∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗Comparing against output data∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗ \\\\n\\\\n”);\\nif (system(”diff huffman.random256.out huffman.random256.golden”)) {\\nfprintf(stdout, ”∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗\\\\n”);\\nfprintf(stdout, ”FAIL: Output DOES NOT match the golden output\\\\n”);\\nfprintf(stdout, ”∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗\\\\n”);\\nreturn val = 1;\\n} else {\\nfprintf(stdout, ”∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗\\\\n”);\\nfprintf(stdout, ” PASS: The output matches the golden output\\\\n”);\\nfprintf(stdout, ”∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗\\\\n”);\\nreturn val = 0;\\n}\\nprintf(”Ending canonical Huffman encoding testbench\\\\n”);\\nreturn return val;\\n}\\nFigure11.14: ThecompletecodeforthecanonicalHuffmanencodingtestbench.'),\n",
       " Document(page_content='val = 0;\\n}\\nprintf(”Ending canonical Huffman encoding testbench\\\\n”);\\nreturn return val;\\n}\\nFigure11.14: ThecompletecodeforthecanonicalHuffmanencodingtestbench. Thecodeinitializes\\nthe in array with data from an input file. It passes that into the top huffman encoding function.\\nThen it stores the resulting codewords into a file, and compares that with another golden reference\\nfile. It prints out the results of the comparison, and returns the appropriate value.\\n222HUFFMAN ENCODING\\nto a file named random256.out and performing a file comparison using the diff tool. The diff\\ntool returns ’0’ if the files are identical and non-zero if the files are different. Thus, the if condition\\noccurs when the files are different, and the else condition is executed when the files are the same.\\nIn both cases, we print out a message and set the return val to the appropriate value. This return\\nvalue is used by the Vivado(cid:13)R HLS tool during cosimulation to check the correctness of results. The\\nreturn'),\n",
       " Document(page_content='we print out a message and set the return val to the appropriate value. This return\\nvalue is used by the Vivado(cid:13)R HLS tool during cosimulation to check the correctness of results. The\\nreturn value should be ’0’ if it passes, and non-zero if it does not pass.\\n11.3 Conclusion\\nHuffman Coding is a common type of data compression used in many applications. While encoding\\nand decoding using a Huffman code are relatively simple operations, generating the Huffman code\\nitself can be a computationally challenging problem. In many systems it is advantageous to have\\nrelatively small blocks of data, implying that new Huffman codes must be created often, making it\\nworthwhile to accelerate.\\nCompared to other algorithms we’ve studied in this book, creating a Huffman code contains a\\nnumber of steps with radically different code structures. Some are relatively easy to parallelize,\\nwhile others are more challenging. Some portions of the algorithm naturally have higher O(n)\\ncomplexity, meaning that'),\n",
       " Document(page_content='with radically different code structures. Some are relatively easy to parallelize,\\nwhile others are more challenging. Some portions of the algorithm naturally have higher O(n)\\ncomplexity, meaning that they must be more heavily parallelized to achieve a balanced pipeline.\\nHowever, using the dataflow directive in Vivado(cid:13)R HLS, these different code structures can be\\nlinked together relatively easily\\n223HUFFMAN ENCODING\\n224Bibliography\\n[1] AmeerM.S.AbdelhadiandGuyG.F.Lemieux. Modularmulti-portedSRAM-basedmemories.\\nIn Proceedings of the International Symposium on Field Programmable Gate Arrays (FPGA),\\npages 35–44. ACM, 2014. ISBN 978-1-4503-2671-1. doi: 10.1145/2554688.2554773. URL\\nhttp://doi.acm.org/10.1145/2554688.2554773.\\n[2] SystemC. Accellera, 2.3.2 edition, October 2017. URL http://www.accellera.org/\\ndownloads/standards/systemc.\\n[3] Hassan M. Ahmed, Jean-Marc Delosme, and Martin Morf. Highly concurrent computing\\nstructures for matrix arithmetic and signal processing. IEEE'),\n",
       " Document(page_content='http://www.accellera.org/\\ndownloads/standards/systemc.\\n[3] Hassan M. Ahmed, Jean-Marc Delosme, and Martin Morf. Highly concurrent computing\\nstructures for matrix arithmetic and signal processing. IEEE Computer, 15(1):65–82, 1982.\\n[4] Raymond J. Andraka. Building a high performance bit-serial processor in an FPGA. In\\nProceedings of Design SuperCon, volume 96, pages 1–5, 1996.\\n[5] Oriol Arcas-Abella et al. An empirical evaluation of high-level synthesis languages and tools\\nfor database acceleration. In Proceedings of the International Field Programmable Logic and\\nApplications Conference (FPL), pages 1–8. IEEE, 2014.\\n[6] AMBA AXI and ACE Protocol Specification. ARM Limited, v1.0 edition, 2013. URL http://\\ninfocenter.arm.com/help/index.jsp?topic=/com.arm.doc.ihi0022e/index.html. ARM\\nIHI 0022E.\\n[7] Bryce E. Bayer. Color imaging array, July 1976. US 3971065.\\n[8] Samuel Bayliss and George A. Constantinides. Optimizing SDRAM bandwidth for cus-\\ntom FPGA loop accelerators. In Proceedings of the'),\n",
       " Document(page_content='Bryce E. Bayer. Color imaging array, July 1976. US 3971065.\\n[8] Samuel Bayliss and George A. Constantinides. Optimizing SDRAM bandwidth for cus-\\ntom FPGA loop accelerators. In Proceedings of the International Symposium on Field Pro-\\ngrammable Gate Arrays (FPGA), pages 195–204. ACM, February 2012.\\n[9] Marcus Bednara et al. Tradeoff analysis and architecture design of a hybrid hardware/soft-\\nware sorter. In Proceedings of the International Conference on Application-specific Systems,\\nArchitectures and Processors (ASAP), pages 299–308. IEEE, 2000.\\n[10] Vaughn Betz and Jonathan Rose. VPR: A new packing, placement and routing tool for\\nFPGAresearch. InProceedingsoftheInternationalFieldProgrammableLogicandApplications\\nConference (FPL), pages 213–222. Springer, 1997.\\n[11] GuyE.Blelloch. Prefixsumsandtheirapplications. TechnicalReportCMU-CS-90-190,School\\nof Computer Science, Carnegie Mellon University, November 1990.\\n[12] Stephen Brown and Jonathan Rose. FPGA and CPLD architectures: A tutorial.'),\n",
       " Document(page_content='TechnicalReportCMU-CS-90-190,School\\nof Computer Science, Carnegie Mellon University, November 1990.\\n[12] Stephen Brown and Jonathan Rose. FPGA and CPLD architectures: A tutorial. IEEE Design\\nand Test of Computers, 13(2):42–57, 1996.\\n225BIBLIOGRAPHY BIBLIOGRAPHY\\n[13] Andrew Canis, Jongsok Choi, Mark Aldham, Victor Zhang, Ahmed Kammoona, Jason H\\nAnderson, Stephen Brown, and Tomasz Czajkowski. LegUp: high-level synthesis for FPGA-\\nbased processor/accelerator systems. In Proceedings of the International Symposium on Field\\nProgrammable Gate Arrays (FPGA), pages 33–36. ACM, 2011.\\n[14] JianwenChen,JasonCong,MingYan,andYiZou. FPGA-accelerated3Dreconstructionusing\\ncompressive sensing. In Proceedings of the International Symposium on Field Programmable\\nGate Arrays (FPGA), pages 163–166. ACM, 2012.\\n[15] Jason Cong, Bin Liu, Stephen Neuendorffer, Juanjo Noguera, Kees Vissers, and Zhiru Zhang.\\nHigh-level synthesis for fpgas: From prototyping to deployment. IEEE Transactions on\\nComputer-aided Design'),\n",
       " Document(page_content='Jason Cong, Bin Liu, Stephen Neuendorffer, Juanjo Noguera, Kees Vissers, and Zhiru Zhang.\\nHigh-level synthesis for fpgas: From prototyping to deployment. IEEE Transactions on\\nComputer-aided Design of Integrated Circuits and Systems (TCAD), 30(4):473–491, 2011.\\n[16] James W. Cooley and John W. Tukey. An algorithm for the machine calculation of complex\\nfourier series. Mathematics of Computation, 19(90):297–301, 1965. ISSN 00255718. URL\\nhttp://www.jstor.org/stable/2003354.\\n[17] ThomasH.Cormen,CharlesE.Leiserson,RonaldL.Rivest,andCliffordStein. Introduction to\\nAlgorithms, Third Edition. MIT Press, 3rd edition, 2009. ISBN 0262033844, 9780262033848.\\n[18] Philippe Coussy and Adam Morawiec. High-level synthesis, volume 1. Springer, 2010.\\n[19] Steve Dai, Ritchie Zhao, Gai Liu, Shreesha Srinath, Udit Gupta, Christopher Batten, and\\nZhiru Zhang. Dynamic hazard resolution for pipelining irregular loops in high-level synthesis.\\nIn Proceedings of the International Symposium on Field Programmable Gate'),\n",
       " Document(page_content='Gupta, Christopher Batten, and\\nZhiru Zhang. Dynamic hazard resolution for pipelining irregular loops in high-level synthesis.\\nIn Proceedings of the International Symposium on Field Programmable Gate Arrays (FPGA),\\npages 189–194, 2017. ISBN 978-1-4503-4354-1. doi: 10.1145/3020078.3021754. URL http:\\n//doi.acm.org/10.1145/3020078.3021754.\\n[20] Jeffrey Dean and Sanjay Ghemawat. MapReduce: Simplified data processing on large clusters.\\nCommunications of the ACM, 51(1):107–113, January 2008. ISSN 0001-0782. doi: 10.1145/\\n1327452.1327492. URL http://doi.acm.org/10.1145/1327452.1327492.\\n[21] AlvinM.Despain. FouriertransformcomputersusingCORDICiterations. IEEE Transactions\\non Computers, 100(10):993–1001, 1974.\\n[22] J. Detrey and F. de Dinechin. Floating-point trigonometric functions for FPGAs. In Proceed-\\nings of the International Field Programmable Logic and Applications Conference (FPL), pages\\n29–34, August 2007. doi: 10.1109/FPL.2007.4380621.\\n[23] L Peter Deutsch. DEFLATE compressed data'),\n",
       " Document(page_content='In Proceed-\\nings of the International Field Programmable Logic and Applications Conference (FPL), pages\\n29–34, August 2007. doi: 10.1109/FPL.2007.4380621.\\n[23] L Peter Deutsch. DEFLATE compressed data format specification version 1.3. 1996.\\n[24] Jean Duprat and Jean-Michel Muller. The CORDIC algorithm: new results for fast VLSI\\nimplementation. IEEE Transactions on Computers, 42(2):168–178, 1993.\\n[25] Brian P. Flannery, William H. Press, Saul A. Teukolsky, and William Vetterling. Numerical\\nrecipes in C. Press Syndicate of the University of Cambridge, New York, 1992.\\n[26] Daniel D. Gajski, Nikil D. Dutt, Allen C.H. Wu, and Steve Y.L. Lin. High-Level Synthesis:\\nIntroduction to Chip and System Design. Springer Science & Business Media, 2012.\\n[27] W Morven Gentleman and Gordon Sande. Fast fourier transforms: for fun and profit. In\\nProceedings of the November 7-10, 1966, fall joint computer conference, pages 563–578. ACM,\\n1966.\\n226BIBLIOGRAPHY BIBLIOGRAPHY\\n[28] Nivia George, HyoukJoong Lee,'),\n",
       " Document(page_content='transforms: for fun and profit. In\\nProceedings of the November 7-10, 1966, fall joint computer conference, pages 563–578. ACM,\\n1966.\\n226BIBLIOGRAPHY BIBLIOGRAPHY\\n[28] Nivia George, HyoukJoong Lee, David Novo, Tiark Rompf, Kevin J. Brown, Arvind K. Su-\\njeeth, Martin Odersky, Kunle Olukotun, and Paolo Ienne. Hardware system synthesis from\\ndomain-specific languages. In Proceedings of the International Field Programmable Logic and\\nApplications Conference (FPL), pages 1–8. IEEE, 2014.\\n[29] Sumit Gupta, Rajesh Gupta, Nikil Dutt, and Alexandru Nicolau. SPARK: A Parallelizing\\nApproach to the High-level Synthesis of Digital Circuits. Kluwer, 2004. ISBN 1-4020-7837-4.\\n[30] Scott Hauck and Andre DeHon. Reconfigurable computing: the theory and practice of FPGA-\\nbased computation, volume 1. Morgan Kaufmann, 2010.\\n[31] James Hegarty, Ross Daly, Zachary DeVito, Jonathan Ragan-Kelley, Mark Horowitz, and\\nPat Hanrahan. Rigel: Flexible multi-rate image processing hardware. ACM Trans.'),\n",
       " Document(page_content='volume 1. Morgan Kaufmann, 2010.\\n[31] James Hegarty, Ross Daly, Zachary DeVito, Jonathan Ragan-Kelley, Mark Horowitz, and\\nPat Hanrahan. Rigel: Flexible multi-rate image processing hardware. ACM Trans. Graph.,\\n35(4):85:1–85:11, July 2016. ISSN 0730-0301. doi: 10.1145/2897824.2925892. URL http:\\n//doi.acm.org/10.1145/2897824.2925892.\\n[32] M. Heideman, D. Johnson, and C. Burrus. Gauss and the history of the fast fourier transform.\\nASSP Magazine, IEEE, 1(4):14–21, October 1984. ISSN 0740-7467. doi: 10.1109/MASSP.\\n1984.1162257.\\n[33] DavidA.Huffman. Amethodfortheconstructionofminimum-redundancycodes. Proceedings\\nof the IRE, 40(9):1098–1101, 1952.\\n[34] Ryan Kastner, Anup Hosangadi, and Farzan Fallah. Arithmetic optimization techniques for\\nhardware and software design. Cambridge University Press, 2010.\\n[35] David Knapp. Behavioral Synthesis: Digital System Design using the Synopsys Behavioral\\nCompiler. Prentice-Hall, 1996. ISBN 0-13-569252-0.\\n[36] Donald Ervin Knuth. The art of computer'),\n",
       " Document(page_content='Press, 2010.\\n[35] David Knapp. Behavioral Synthesis: Digital System Design using the Synopsys Behavioral\\nCompiler. Prentice-Hall, 1996. ISBN 0-13-569252-0.\\n[36] Donald Ervin Knuth. The art of computer programming: sorting and searching, volume 3.\\nPearson Education, 1998.\\n[37] Charles Eric Laforest, Zimo Li, Tristan O’rourke, Ming G. Liu, and J. Gregory Steffan.\\nComposing multi-ported memories on FPGAs. ACM Transactions on Reconfigurable Tech-\\nnology and Systems (TRETS), 7(3):16:1–16:23, September 2014. ISSN 1936-7406. doi:\\n10.1145/2629629. URL http://doi.acm.org/10.1145/2629629.\\n[38] Glen G. Langdon Jr, Joan L. Mitchell, William B. Pennebaker, and Jorma J. Rissanen. Arith-\\nmetic coding encoder and decoder system, February 27 1990. US Patent 4,905,297.\\n[39] Dajung Lee, Janarbek Matai, Brad Weals, and Ryan Kastner. High throughput channel\\ntracking for JTRS wireless channel emulation. In Proceedings of the International Field Pro-\\ngrammable Logic and Applications Conference (FPL). IEEE,'),\n",
       " Document(page_content='Weals, and Ryan Kastner. High throughput channel\\ntracking for JTRS wireless channel emulation. In Proceedings of the International Field Pro-\\ngrammable Logic and Applications Conference (FPL). IEEE, 2014.\\n[40] Edward A. Lee and David G. Messerschmitt. Pipeline interleaved programmable DSPs: Ar-\\nchitecture. IEEE Transactions on Acoustics, Speech, and Signal Processing (TASSP), 35(9):\\n1320–1333, September 1987.\\n[41] Edward A. Lee and Pravin Varaiya. Structure and Interpretation of Signals and Systems,\\nSecond Edition. 2011. ISBN 0578077191. URL LeeVaraiya.org.\\n[42] Edward Ashford Lee. Plato and the Nerd: The Creative Partnership of Humans and Technol-\\nogy. MIT Press, 2017. ISBN 978-0262036481.\\n227BIBLIOGRAPHY BIBLIOGRAPHY\\n[43] C. Leiserson, F. Rose, and J. Saxe. Optimizing synchronous circuitry by retiming. In Third\\nCaltech Conference On VLSI, 1993.\\n[44] G. Liu, M. Tan, S. Dai, R. Zhao, and Z. Zhang. Architecture and synthesis for area-efficient\\npipelining of irregular loop nests. IEEE'),\n",
       " Document(page_content='by retiming. In Third\\nCaltech Conference On VLSI, 1993.\\n[44] G. Liu, M. Tan, S. Dai, R. Zhao, and Z. Zhang. Architecture and synthesis for area-efficient\\npipelining of irregular loop nests. IEEE Transactions on Computer-aided Design of Integrated\\nCircuits and Systems (TCAD), 36(11):1817–1830, November 2017. ISSN 0278-0070. doi:\\n10.1109/TCAD.2017.2664067.\\n[45] RuiMarcelinoetal. SortingunitsforFPGA-basedembeddedsystems. InDistributedEmbedded\\nSystems: Design, Middleware and Resources, pages 11–22. Springer, 2008.\\n[46] Janarbek Matai, Pingfan Meng, Lingjuan Wu, Brad T Weals, and Ryan Kastner. Designing a\\nhardwareintheloopwirelessdigitalchannelemulatorforsoftwaredefinedradio. InProceedings\\nof the International Conference on Field-Programmable Technology (FPT). IEEE, 2012.\\n[47] Janarbek Matai, Joo-Young Kim, and Ryan Kastner. Energy efficient canonical huffman\\nencoding. In Proceedings of the International Conference on Application-specific Systems,\\nArchitectures and Processors (ASAP). IEEE,'),\n",
       " Document(page_content='Joo-Young Kim, and Ryan Kastner. Energy efficient canonical huffman\\nencoding. In Proceedings of the International Conference on Application-specific Systems,\\nArchitectures and Processors (ASAP). IEEE, 2014.\\n[48] Janarbek Matai, Dustin Richmond, Dajung Lee, and Ryan Kastner. Enabling FPGAs for the\\nmasses. arXiv preprint arXiv:1408.5870, 2014.\\n[49] Janarbek Matai, Dustin Richmond, Dajung Lee, Zac Blair, Qiongzhi Wu, Amin Abazari, and\\nRyan Kastner. Resolve: Generation of high-performance sorting architectures from high-level\\nsynthesis. In Proceedings of the International Symposium on Field Programmable Gate Arrays\\n(FPGA),pages195–204.ACM,2016. ISBN978-1-4503-3856-1. doi: 10.1145/2847263.2847268.\\nURL http://doi.acm.org/10.1145/2847263.2847268.\\n[50] Carver Mead and Lynn Conway. Introduction to VLSI systems, volume 1080. Addison-Wesley\\nReading, MA, 1980.\\n[51] Giovanni De Micheli. Synthesis and optimization of digital circuits. McGraw-Hill Higher\\nEducation, 1994.\\n[52] Shahnam Mirzaei, Anup'),\n",
       " Document(page_content='to VLSI systems, volume 1080. Addison-Wesley\\nReading, MA, 1980.\\n[51] Giovanni De Micheli. Synthesis and optimization of digital circuits. McGraw-Hill Higher\\nEducation, 1994.\\n[52] Shahnam Mirzaei, Anup Hosangadi, and Ryan Kastner. FPGA implementation of high speed\\nfir filters using add and shift method. In Computer Design, 2006. ICCD 2006. International\\nConference on, pages 308–313. IEEE, 2007.\\n[53] MISRA. Guidelines for the Use of the C Language in Critical Systems. March 2013. ISBN\\n978-1-906400-10-1. URL https://www.misra.org.uk.\\n[54] Rene Mueller et al. Sorting networks on FPGAs. The VLDB Journal—The International\\nJournal on Very Large Data Bases, 21(1):1–23, 2012.\\n[55] Jorge Ortiz et al. A streaming high-throughput linear sorter system with contention buffering.\\nInternational Journal of Reconfigurable Computing, 2011.\\n[56] Marios C. Papaefthymiou. Understanding retiming through maximum average-weight cycles.\\nIn SPAA ’91: Proceedings of the third annual ACM symposium on Parallel'),\n",
       " Document(page_content='of Reconfigurable Computing, 2011.\\n[56] Marios C. Papaefthymiou. Understanding retiming through maximum average-weight cycles.\\nIn SPAA ’91: Proceedings of the third annual ACM symposium on Parallel algorithms and\\narchitectures, pages 338–348, 1991.\\n[57] William B. Pennebaker. JPEG: Still image data compression standard. Springer, 1992.\\n228BIBLIOGRAPHY BIBLIOGRAPHY\\n[58] Robert Sedgewick. Algorithms in C. Addison-Wesley, 2001. ISBN 978-0201756081.\\n[59] Bhaskar Sherigar and Valmiki K. Ramanujan. Huffman decoder used for decoding both ad-\\nvanced audio coding (AAC) and MP3 audio, June 29 2004. US Patent App. 10/880,695.\\n[60] F. Winterstein, S. Bayliss, and G. A. Constantinides. High-level synthesis of dynamic data\\nstructures: A case study using Vivado HLS. In Proceedings of the International Symposium\\non Field Programmable Gate Arrays (FPGA), pages 362–365, December 2013. doi: 10.1109/\\nFPT.2013.6718388.\\n[61] IanH.Witten, RadfordM.Neal, andJohnG.Cleary.'),\n",
       " Document(page_content='In Proceedings of the International Symposium\\non Field Programmable Gate Arrays (FPGA), pages 362–365, December 2013. doi: 10.1109/\\nFPT.2013.6718388.\\n[61] IanH.Witten, RadfordM.Neal, andJohnG.Cleary. Arithmeticcodingfordatacompression.\\nCommunications of the ACM, 30(6):520–540, 1987.\\n[62] UltraScale Architecture Configurable Logic Block (UG574). Xilinx, v1.5 edition,\\nFebruary 2017. URL https://www.xilinx.com/support/documentation/user_guides/\\nug574-ultrascale-clb.pdf.\\n[63] Vivado Design Suite User Guide: High-Level Synthesis (UG902). Xilinx,v2017.1edition,April\\n2017. URL https://www.xilinx.com/support/documentation/sw_manuals/xilinx2017_\\n1/ug902-vivado-high-level-synthesis.pdf.\\n[64] UltraScale Architecture Configuration (UG570). Xilinx, v1.7 edition, March\\n2017. URL https://www.xilinx.com/support/documentation/user_guides/\\nug570-ultrascale-configuration.pdf.\\n229BIBLIOGRAPHY BIBLIOGRAPHY\\n230Glossary\\narray partitioning Dividing a single logical array into multiple physical memories.. 87,'),\n",
       " Document(page_content='BIBLIOGRAPHY\\n230Glossary\\narray partitioning Dividing a single logical array into multiple physical memories.. 87, 152, 177\\nbitstream The configuration data used to program the functionality of an FPGA. 13\\nBRAM A block RAM is a configurable random access memory that is embedded throughout an\\nFPGA for data storage and communication.. 17, 131, 150, 164, 189, 232\\nC/RTL cosimulation TheprocessofverifyinganRTLdesigngeneratedbyHLSusingtestvectors\\ncaptured from the C testbench.. 128\\nCompressed Row Storage Compressed Row Storage is a technique for representing a sparse\\nmatrix. It allows large matrices with a small number of elements to be stored and operated\\non efficiently.. 117\\ndata rate The frequency at which a task can process the input data. This is often expressed in\\nbits/second and thus also depends on the size of the input data. 20\\nDiscrete Fourier Transform An transformation that takes a discrete signal and converts it to\\na freqeuncy-domain representation.. 4, 77, 99, 231\\nEDA Electronic'),\n",
       " Document(page_content='depends on the size of the input data. 20\\nDiscrete Fourier Transform An transformation that takes a discrete signal and converts it to\\na freqeuncy-domain representation.. 4, 77, 99, 231\\nEDA Electronic design automation (EDA) are a set of software tools used to aid the hardware\\ndesign process. . 11\\nFast Fourier Transform An optimized version of the Discrete Fourier Transform (DFT) which\\nrequires fewer operations.. 77, 99\\nFF A flip-flop (FF) is a circuit that can store information. We typically think of it as storing one\\nbit of data and are a fundamental building block for creating memories in digital circuits..\\n14, 150, 164, 232\\nfinite impulse response A common digital signal processing task that performs a convolution\\non the input signal with a fixed signal that is defined by its coefficients. The FIR is often\\nperformed in hardware and can be efficiently implemented. 22\\nFPGA A field-programmable gate array (FPGA) is an integrated circuit that can be customized\\nor programmed after it is'),\n",
       " Document(page_content='The FIR is often\\nperformed in hardware and can be efficiently implemented. 22\\nFPGA A field-programmable gate array (FPGA) is an integrated circuit that can be customized\\nor programmed after it is manufactured (“in the field”). . 11, 232\\nHLS High-level synthesis is a hardware design process that translates an algorithmic description\\n(which is decoupled from the cycle to cycle behavior) into a register transfer level (RTL)\\nhardware description language which specifies the exact behavior of the circuit on a cycle-by-\\ncycle basis. 11, 188\\n231Glossary Glossary\\nI/O block An I/O block provides the interface between the FPGA fabric and the remainder of\\nthe system. I/O blocks can talk to memories (e.g., on-chip caches and off-chip DRAM,\\nmicroprocessors (using AXI or other protocols), sensors, actuators, etc.. . 16, 233\\nIP core AnRTL-levelcomponentwithwell-definedinterfacesenablingittobeincorporatedintoa\\nlargerdesign. Oftenusedasawayofhidingthe‘intellectualproperty’fromanothercompany,\\nhence the'),\n",
       " Document(page_content='etc.. . 16, 233\\nIP core AnRTL-levelcomponentwithwell-definedinterfacesenablingittobeincorporatedintoa\\nlargerdesign. Oftenusedasawayofhidingthe‘intellectualproperty’fromanothercompany,\\nhence the name.. 18\\nlogic synthesis The process of converting an glsrtl design into a netlist of device-level primitives..\\n13\\nloop interchange A code transformation that changes the order of loop operations. This trans-\\nformation is often a useful approach to addressing recurrences in code.. 94\\nloop pipelining Enabling multiple iterations of a loop to run concurrently sharing the same func-\\ntional units.. 84\\nLUT A lookup table (LUT) is a memory where the address signal are the inputs and the corre-\\nsponding outputs are contained in the memory entries. It is a key computational component\\nof modern field-programmable gate array (FPGA)s.. 13, 232\\nnetlist An intermediate design artifact consisting of device-level primitive elements and the con-\\nnections between them. In FPGA designs, the primitive elements'),\n",
       " Document(page_content='gate array (FPGA)s.. 13, 232\\nnetlist An intermediate design artifact consisting of device-level primitive elements and the con-\\nnections between them. In FPGA designs, the primitive elements include lookup table\\n(LUT)s,flip-flop (FF)s, and block RAM (BRAM)s.. 13\\npartial loop unrolling A transformation where the body of a loop is replicated multiple times.\\nThis is often used in processor systems to reduce loop condition overhead or to provide\\nopportunities for vectorization. In HLS, it can have a similar effect, enabling more operations\\nfrom the same loop nest to be considered in scheduling. This can improve the performance\\nof a design.. 90, 126\\nplace and route The process of converting a netlist of device-level primitives into the configura-\\ntion of a particular device.. 13\\nprocess An individual component in a dataflow architecture. 114\\nprocessing element A coarse-grained concurrently executing component in a design. In HLS,\\nthis is often used in the context of a dataflow design..'),\n",
       " Document(page_content='individual component in a dataflow architecture. 114\\nprocessing element A coarse-grained concurrently executing component in a design. In HLS,\\nthis is often used in the context of a dataflow design.. 153\\nrecurrence A code structure that results in a feedback loop when implemented in a circuit. Re-\\ncurrences limit the throughput of the circuit.. 26, 94\\nROM A Read-only Memory is a memory which is initialized to a particular value and then read\\nbut never written. In many cases the storage for ROMs can be highly optimized because\\ntheir value never changes.. 96\\nrouting channel A routing channel provides a flexible set of connections between the FPGA\\nprogrammable logic elements. . 15, 16, 15, 16, 233\\nRTL Register-transfer level (RTL) is a hardware design abstraction which models a synchronous\\ndigital circuit using logical operations that occur between between hardware registers. It is\\ncommon design entry for modern digital design. 11, 123, 202\\n232Glossary Glossary\\nslice A (typically small)'),\n",
       " Document(page_content='circuit using logical operations that occur between between hardware registers. It is\\ncommon design entry for modern digital design. 11, 123, 202\\n232Glossary Glossary\\nslice A (typically small) set of LUTs, FFs and multiplexors. These are often reported in FPGA\\nresource utilization reports. . 14, 15, 14, 15, 16\\nsorting cell Ansimplestatefulcomponentthatformspartofalargersortingnetworkoralgorithm.\\nCommonly a cell performs a compare-and-swap operation between two elements.. 180\\nstable sort A sorting algorithm that keeps different elements with the same sorting key in their\\noriginal sequence after sorting.. 174, 205\\nStatic Single Assignment StaticSingleAssignmentisanintermediaterepresentationincompil-\\ners where each variable is assigned only once. This form makes many common optimizations\\nsimpler to write.. 180\\nswitchbox A switchbox connects routing channels to provide a flesible routing structure for data\\nrouted between the programmable logic and I/O block. . 15, 16, 15, 16\\nsystolic array'),\n",
       " Document(page_content='to write.. 180\\nswitchbox A switchbox connects routing channels to provide a flesible routing structure for data\\nrouted between the programmable logic and I/O block. . 15, 16, 15, 16\\nsystolic array An array of processing elements that coordinate to perform a more complex al-\\ngorithm. Systolic arrays are usually designed so that each processing element encapsulates\\nsome local information and only communicates with its local neighbors. This often enables\\nsystolic arrays to easily scale to large problem sizes by increasing the size of the array.. 180,\\n194\\ntask A fundamental atomic unit of behavior or high-level synthesis computation; this corresponds\\nto a function invocation in high-level synthesis. 20\\ntask pipelining Being able to execute more than one task concurrently on the same accelerator\\nin a pipelined fashion.. 113, 114, 154\\ntask interval Thetimebetweenwhenonetaskstartsandthenextstartsorthedifferencebetween\\nthe start times of two consecutive tasks. 20\\ntask latency The time between'),\n",
       " Document(page_content='a pipelined fashion.. 113, 114, 154\\ntask interval Thetimebetweenwhenonetaskstartsandthenextstartsorthedifferencebetween\\nthe start times of two consecutive tasks. 20\\ntask latency The time between when a task starts and when it finishes. 20\\n233Glossary Glossary\\n234Acronyms\\nASIC application-specific integrated circuit. Glossary: ASIC\\nBRAM block RAM. Glossary: BRAM, 17, 18, 131, 150, 152, 164, 189, 232\\nCRS compressed row storage. Glossary: Compressed Row Storage, 117, 118, 119, 120\\nDFT Discrete Fourier Transform. Glossary: Discrete Fourier Transform, 4, 77, 79, 80, 81, 82, 83,\\n92, 94, 92, 96, 97, 99, 100, 102, 116, 231\\nEDA electronic design automation. Glossary: EDA, 11\\nFF flip-flop. Glossary: FF, 14, 15, 18, 17, 18, 150, 164, 232\\nFFT Fast Fourier Transform. Glossary: Fast Fourier Transform, 77, 97, 99, 100, 102, 103, 104,\\n105, 107, 108, 109, 111, 113, 114, 113, 114, 116\\nFPGA field-programmable gate array. Glossary: FPGA, 11, 13, 14, 15, 16, 17, 232\\nHLS high-level synthesis. Glossary: HLS,'),\n",
       " Document(page_content='97, 99, 100, 102, 103, 104,\\n105, 107, 108, 109, 111, 113, 114, 113, 114, 116\\nFPGA field-programmable gate array. Glossary: FPGA, 11, 13, 14, 15, 16, 17, 232\\nHLS high-level synthesis. Glossary: HLS, 11, 12, 13, 15, 18, 188\\nLUT lookup table. Glossary: LUT, 13, 14, 13, 14, 15, 232\\nPAR place and route. Glossary: place and route, 13\\nPE Processing Element. Glossary: processing element, 153, 154\\nROM Read-only Memory. Glossary: ROM, 96\\nRTL register-transfer level. Glossary: RTL, 11, 13, 123, 128, 202, 231\\nSSA static single assigment. Glossary: Static Single Assignment, 180\\n235')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"/home/ubuntu/Home_User/HHT_Documents/Index-source/book.txt\") as f:\n",
    "    state_of_the_union = f.read()\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=' ',\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "texts = text_splitter.create_documents([state_of_the_union])\n",
    "len(texts)\n",
    "text_splitter.split_text(state_of_the_union)[0]\n",
    "texts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m OpenAIEmbeddings()\n\u001b[0;32m----> 3\u001b[0m db1 \u001b[38;5;241m=\u001b[39m \u001b[43mFAISS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Home_User/HHT_Documents/fermihedral-main/venv/lib/python3.11/site-packages/langchain_core/vectorstores.py:635\u001b[0m, in \u001b[0;36mVectorStore.from_documents\u001b[0;34m(cls, documents, embedding, **kwargs)\u001b[0m\n\u001b[1;32m    633\u001b[0m texts \u001b[38;5;241m=\u001b[39m [d\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m    634\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [d\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m--> 635\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Home_User/HHT_Documents/fermihedral-main/venv/lib/python3.11/site-packages/langchain_community/vectorstores/faiss.py:930\u001b[0m, in \u001b[0;36mFAISS.from_texts\u001b[0;34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    904\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_texts\u001b[39m(\n\u001b[1;32m    905\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    911\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m FAISS:\n\u001b[1;32m    912\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Construct FAISS wrapper from raw documents.\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \n\u001b[1;32m    914\u001b[0m \u001b[38;5;124;03m    This is a user friendly interface that:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;124;03m            faiss = FAISS.from_texts(texts, embeddings)\u001b[39;00m\n\u001b[1;32m    929\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 930\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[43membedding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    931\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__from(\n\u001b[1;32m    932\u001b[0m         texts,\n\u001b[1;32m    933\u001b[0m         embeddings,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    937\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    938\u001b[0m     )\n",
      "File \u001b[0;32m~/Home_User/HHT_Documents/fermihedral-main/venv/lib/python3.11/site-packages/langchain_openai/embeddings/base.py:535\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.embed_documents\u001b[0;34m(self, texts, chunk_size)\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;66;03m# NOTE: to keep things simple, we assume the list may contain texts longer\u001b[39;00m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;66;03m#       than the maximum context and use length-safe embedding function.\u001b[39;00m\n\u001b[1;32m    534\u001b[0m engine \u001b[38;5;241m=\u001b[39m cast(\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeployment)\n\u001b[0;32m--> 535\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_len_safe_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Home_User/HHT_Documents/fermihedral-main/venv/lib/python3.11/site-packages/langchain_openai/embeddings/base.py:430\u001b[0m, in \u001b[0;36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[0;34m(self, texts, engine, chunk_size)\u001b[0m\n\u001b[1;32m    428\u001b[0m batched_embeddings: List[List[\u001b[38;5;28mfloat\u001b[39m]] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m _iter:\n\u001b[0;32m--> 430\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m_chunk_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_invocation_params\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    434\u001b[0m         response \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mmodel_dump()\n",
      "File \u001b[0;32m~/Home_User/HHT_Documents/fermihedral-main/venv/lib/python3.11/site-packages/openai/resources/embeddings.py:114\u001b[0m, in \u001b[0;36mEmbeddings.create\u001b[0;34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    108\u001b[0m         embedding\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfrombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[1;32m    109\u001b[0m             base64\u001b[38;5;241m.\u001b[39mb64decode(data), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    110\u001b[0m         )\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/embeddings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEmbeddingCreateParams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCreateEmbeddingResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Home_User/HHT_Documents/fermihedral-main/venv/lib/python3.11/site-packages/openai/_base_client.py:1240\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1226\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1227\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1228\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1235\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1236\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1237\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1238\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1239\u001b[0m     )\n\u001b[0;32m-> 1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Home_User/HHT_Documents/fermihedral-main/venv/lib/python3.11/site-packages/openai/_base_client.py:921\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    913\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    914\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    919\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    920\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Home_User/HHT_Documents/fermihedral-main/venv/lib/python3.11/site-packages/openai/_base_client.py:961\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    958\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered httpx.TimeoutException\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    960\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 961\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    970\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRaising timeout error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    971\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m APITimeoutError(request\u001b[38;5;241m=\u001b[39mrequest) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n",
      "File \u001b[0;32m~/Home_User/HHT_Documents/fermihedral-main/venv/lib/python3.11/site-packages/openai/_base_client.py:1053\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1050\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1054\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1055\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1056\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1057\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1058\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1059\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Home_User/HHT_Documents/fermihedral-main/venv/lib/python3.11/site-packages/openai/_base_client.py:961\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    958\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered httpx.TimeoutException\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    960\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 961\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    970\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRaising timeout error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    971\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m APITimeoutError(request\u001b[38;5;241m=\u001b[39mrequest) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n",
      "File \u001b[0;32m~/Home_User/HHT_Documents/fermihedral-main/venv/lib/python3.11/site-packages/openai/_base_client.py:1053\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1050\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1054\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1055\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1056\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1057\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1058\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1059\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Home_User/HHT_Documents/fermihedral-main/venv/lib/python3.11/site-packages/openai/_base_client.py:952\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    949\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSending HTTP Request: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, request\u001b[38;5;241m.\u001b[39mmethod, request\u001b[38;5;241m.\u001b[39murl)\n\u001b[1;32m    951\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 952\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    958\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered httpx.TimeoutException\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Home_User/HHT_Documents/fermihedral-main/venv/lib/python3.11/site-packages/httpx/_client.py:914\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    906\u001b[0m follow_redirects \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    907\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfollow_redirects\n\u001b[1;32m    908\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(follow_redirects, UseClientDefault)\n\u001b[1;32m    909\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m follow_redirects\n\u001b[1;32m    910\u001b[0m )\n\u001b[1;32m    912\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 914\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m~/Home_User/HHT_Documents/fermihedral-main/venv/lib/python3.11/site-packages/httpx/_client.py:942\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    939\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 942\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    947\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    948\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Home_User/HHT_Documents/fermihedral-main/venv/lib/python3.11/site-packages/httpx/_client.py:979\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    977\u001b[0m     hook(request)\n\u001b[0;32m--> 979\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    981\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/Home_User/HHT_Documents/fermihedral-main/venv/lib/python3.11/site-packages/httpx/_client.py:1015\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1010\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1011\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1012\u001b[0m     )\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1015\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1019\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[0;32m~/Home_User/HHT_Documents/fermihedral-main/venv/lib/python3.11/site-packages/httpx/_transports/default.py:233\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    220\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    221\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    222\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    230\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    231\u001b[0m )\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 233\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    238\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    239\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    240\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[1;32m    241\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    242\u001b[0m )\n",
      "File \u001b[0;32m~/Home_User/HHT_Documents/fermihedral-main/venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:216\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    213\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, Iterable)\n",
      "File \u001b[0;32m~/Home_User/HHT_Documents/fermihedral-main/venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:196\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    192\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[0;32m~/Home_User/HHT_Documents/fermihedral-main/venv/lib/python3.11/site-packages/httpcore/_sync/connection.py:99\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 99\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mhandle_request(request)\n",
      "File \u001b[0;32m~/Home_User/HHT_Documents/fermihedral-main/venv/lib/python3.11/site-packages/httpcore/_sync/connection.py:76\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request_lock:\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m         stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m         ssl_object \u001b[38;5;241m=\u001b[39m stream\u001b[38;5;241m.\u001b[39mget_extra_info(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mssl_object\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     79\u001b[0m         http2_negotiated \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     80\u001b[0m             ssl_object \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     81\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m ssl_object\u001b[38;5;241m.\u001b[39mselected_alpn_protocol() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     82\u001b[0m         )\n",
      "File \u001b[0;32m~/Home_User/HHT_Documents/fermihedral-main/venv/lib/python3.11/site-packages/httpcore/_sync/connection.py:122\u001b[0m, in \u001b[0;36mHTTPConnection._connect\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    114\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhost\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_origin\u001b[38;5;241m.\u001b[39mhost\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mascii\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    116\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mport\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_origin\u001b[38;5;241m.\u001b[39mport,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msocket_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_socket_options,\n\u001b[1;32m    120\u001b[0m     }\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconnect_tcp\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[0;32m--> 122\u001b[0m         stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_tcp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m         trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m stream\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Home_User/HHT_Documents/fermihedral-main/venv/lib/python3.11/site-packages/httpcore/_backends/sync.py:206\u001b[0m, in \u001b[0;36mSyncBackend.connect_tcp\u001b[0;34m(self, host, port, timeout, local_address, socket_options)\u001b[0m\n\u001b[1;32m    200\u001b[0m exc_map: ExceptionMapping \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    201\u001b[0m     socket\u001b[38;5;241m.\u001b[39mtimeout: ConnectTimeout,\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;167;01mOSError\u001b[39;00m: ConnectError,\n\u001b[1;32m    203\u001b[0m }\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[0;32m--> 206\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m option \u001b[38;5;129;01min\u001b[39;00m socket_options:\n\u001b[1;32m    212\u001b[0m         sock\u001b[38;5;241m.\u001b[39msetsockopt(\u001b[38;5;241m*\u001b[39moption)  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/socket.py:836\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, all_errors)\u001b[0m\n\u001b[1;32m    834\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source_address:\n\u001b[1;32m    835\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[0;32m--> 836\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n\u001b[1;32m    838\u001b[0m exceptions\u001b[38;5;241m.\u001b[39mclear()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "db1 = FAISS.from_documents(texts[:200], embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db2 = FAISS.from_documents(texts[200:400], embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db3 = FAISS.from_documents(texts[400:500], embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db4 = FAISS.from_documents(texts[500:], embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db1.merge_from(db2)\n",
    "db1.merge_from(db3)\n",
    "db1.merge_from(db4)\n",
    "p= '/home/ubuntu/Home_User/HHT_Documents/Index-source/book_index'\n",
    "db1.save_local(p)\n",
    "new_db = FAISS.load_local(p, embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
